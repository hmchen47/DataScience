# Reading Notes for Data Science

## General Topics


### Visualization

+ K. He & G.Meeden, [Selecting the Number of Bins in a Histogram: A Decision Theoretic Approach](../AppliedDS-UMich/2-InfoVis/p01-HistBins.md)


## Statistics

### Study Design

+ Song JW, Chung KC. [Observational studies: cohort and case-control studies](../Notes/p02-Observational.md). Plast Reconstr Surg. 2010;126(6):2234–2242



### Bayesian Approaches

+ D. Spiegelhalter, K. Abrams, J. Myles, [An Overview of the Bayesian Approach](../Notes/p01-Bayesian.md), Chapter 3 in Bayesian Approaches to Clinical Trials and Health-Care Evaluation, 2004+ S. Ghosh, [Basics of Bayesian Methods](../Notes/p03-BayesianBasics.md), in "Methods in molecular biology" (Clifton, N.J.) 620:155-78, 2010



## Artificial Intelligence




## Machine Learning

### General Topics for ML

+ Reashikaa Verma, [24 Best (and Free) Books To Understand Machine Learning](../Notes/a07=MLBools.md)
+ Pedro Domingos, [A Few Useful Things to Know about Machine Learning](../AppliedDS-UMich/3-AML/p0-UsefulThings.md)
+ Ron Kohavi, Randal M. Henne, and Dan Sommerfield, [Practical Guide to Controlled Experiments on the Web: Listen to Your Customers not to the HiPPO](../AppliedDS-UMich/3-AML/p1-ControlledExp.md)
+ S. Kaufman, S. Rosset, & C. Perlich, [Leakage in Data Mining: Formulation, Detection, and Avoidance](../AppliedDS-UMich/3-AML/p3-Leakage.md)
+ Martin Zinkevich, [Rules of Machine Learning: Best Practices for ML Engineering](../AppliedDS-UMich/3-AML/p4-MLRules.md)
+ Maytal Saar-Tsechansky and Foster Provost, [Handling Missing Values when Applying Classification Models](../AppliedDS-UMich/3-AML/p5-Missing.md)
+ [Graphs](../AppliedDS-UMich/5-SocialNet/p2-Graphs.md)


### Machine Learning Models




### Application - Social Networks

+ David Easley and Jon Kleinberg, [Power Laws and Rich-Get-Richer Phenomena](../AppliedDS-UMich/5-SocialNet/p1-PowerLaw.md), Chapter 18 in [Networks, Crowds, and Markets: Reasoning About a Highly Connected World](http://www.cs.cornell.edu/home/kleinber/networks-book/)
+ David Easley and Jon Kleinberg, [The Small-World Phenomenon](../AppliedDS-UMich/5-SocialNet/The Small-World Phenomenon), Chapter 20 in [Networks, Crowds, and Markets: Reasoning About a Highly Connected World](http://www.cs.cornell.edu/home/kleinber/networks-book/)



## Neural Networks

### General Topics for Neural Networks

+ Matthew Stewart, [Introduction to Neural Networks](../ML/MLNN-Hinton/a01-IntroNN.md)
+ Matthew Stewart, [Intermediate Topics in Neural networks](../ML/MLNN-Hinton/a02-IntermediateNN.md)
+ Matthew Stewart, [Neural Network Optimization](../ML/MLNN-Hinton/a03-Optimization.md)
+ Matthew Stewart, [Simple Guide to Hyperparameter Tuning in Neural Networks](../ML/MLNN-Hinton/a04-Hyperparameter.md)
+ Matthew Stewart, [Neural Style Transfer and Visualization of Convolutional Networks](../ML/MLNN-Hinton/a05-VisualCNN.md)
+ Random Nerd, [Delta Learning Rule & Gradient Descent | Neural Networks](../ML/MLNN-Hinton/a06-DeltaRule.md)
+ Drew Rollins, [Delta Function](../ML/MLNN-Hinton/a07-DeltaFunc.md)


### Activation Functions

+ Chris McCormick, [Deep Learning Tutorial - Softmax Regression](../ML/MLNN-Hinton/a08-SoftmaxReg.md)
+ [Softmax Classifier](../ML/MLNN-Hinton/a09-SoftmaxClass.md) in CS231n Convolutional Neural Networks for Visual Recognition, Stanford University



### Convolution Neural Networks

+ Adit Deshpande, [A Beginner's Guide To Understanding Convolutional Neural Networks](../ML/MLNN-Hinton/a10-CNNsGuide.md)



### Deep Learning

+ Adit Deshpande, [The 9 Deep Learning Papers You Need to Know About](../ML/MLNN-Hinton/a11-9Papers.md)





## DataBase for Data Science




## Python Implementation




## Reading List

+ [Bayesian Inference](http://www.stat.cmu.edu/~larry/=sml/Bayes.pdf), chapter 12
+ A. Julien-Laferriere, [Hopfield network](https://bit.ly/2UH5h2X)
+ [Hopfield Model of Neural Network](https://bit.ly/2xQGikM), Chapter 2,
+ R. Rojas, [The Hopfield Model](https://bit.ly/2wTiP2A) in Neural Networks, Springer, 1996
+ J. J. Hopfield, "[Neural networks and physical systems with emergent collective computational abilities](https://bit.ly/34bVbdR)", Proceedings of the National Academy of Sciences of the USA, vol. 79 no. 8 pp. 2554–2558, April 1982
+ J. Hopfield, D. Feinstein and R. Palmer, [‘Unlearning’ has a stabilizing effect in collective memories](https://bit.ly/3aLt07R), Nature 304(5922):158-9 · July 1983
+ G. Hinton and T. Sejnowski, [Optimal perceptual inference](https://bit.ly/2V4G7u3), Proceedings of the IEEE conference on Computer Vision and Pattern Recognition
+ L. Saul, T. Jaakkola, M. Jordan, [Mean field theory for sigmoid belief networks](https://bit.ly/3aL71xI), Journal of artificial intelligence research, 1996
+ G. Hinton and T. Sejnowski, [Learning and relearning in Boltzmann machines](https://bit.ly/2yzujbS), In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, MIT Press, Cambridge, MA., 1986
+ G. Hinton, R. Salakhutdinov, [A Better Way to Pretrain Deep Boltzmann Machines](https://bit.ly/349yQgV), Advances in Neural Information Processing Systems 25 (NIPS 2012)
+ H. Yu, [A gentle tutorial on Restricted Boltzmann Machine and Contrastive Divergence](https://bit.ly/2RcJVIF), 2017
+ A. Fischer & C. Igel, [An Introduction to Restricted Boltzmann Machines](https://bit.ly/3aHqlMo). In Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications: 17th Iberoamerican Congress, CIARP 2012, Buenos Aires, Argentina, September 3-6, 2012. Proceedings (pp.14-36
+ Wythoff, BJ, 1993. [Backpropagation neural networks. A tutorial](https://bit.ly/2UGj6yQ), Chemometrics and Intelligent Laboratory Systems, 18: 115-155
+ A. Kurenkov, [A 'Brief' History of Neural Nets and Deep Learning](https://bit.ly/2Xbbk1I), 2015
+ Judea Pearl, [Belief networks revisited](https://ftp.cs.ucla.edu/pub/stat_ser/R175.pdf), Artificial Intelligence, 1993
+ Judea Pearl and Stuart Russell, [Bayesian Networks](https://ftp.cs.ucla.edu/pub/stat_ser/r277.pdf), Technical Report, R-277, 2000
+ Judea Pearl, [A Personal Journey into Bayesian Networks](https://ftp.cs.ucla.edu/pub/stat_ser/r476.pdf), Technical Report, R-476, 2018
+ I. Ben‐Gal, [Bayesian Networks](https://bit.ly/2V2P4UP), Encyclopedia of Statistics in Quality and Reliability 2008
+ M. Wellman and M. Henrion, [Explaining 'explaining away'](https://bit.ly/2UHuM4f), IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(3):287-292, April 1993
+ G. Hinton, P. Dayan, B. Frey, and R. Neal, [The wake-sleep algorithm for unsupervised neural networks](https://bit.ly/39FC5h1), Science, Vol. 268, Issue 5214, pp. 1158-1161, 1995
+ A. Ng and M. Jordan, [On discriminative vs. generative classifiers: a comparison of logistic regression and naive bayes](https://bit.ly/39K05Qn), Adv. Neural Inf. Proc. Syst. 14, 841 (2002)
+ C. Bishop and J. Lasserre, [Generative or Discriminative? Getting the Best of Both Worlds](https://bit.ly/2yuricF), BAYESIAN STATISTICS 8, pp. 3–24, 2007
+ R. Tibshirani, [Modeling Basics: Assessment, Selection, and Complexity](https://bit.ly/3dXnSiO), Statistical Machine Learning, Spring 2015
+ G. Hinton, S. Osindero,, and Y.-W. Teh, [A Fast Learning Algorithm for Deep Belief Nets](https://bit.ly/2wQSqCk), Neural Computation, July 2006
+ G. E. Hinton*, R. R. Salakhutdinov, [Reducing the Dimensionality of Data with Neural Networks](https://bit.ly/2xbMHXZ), Science, 28 Jul 2006
+ P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol, [Extracting and composing robust features with denoising autoencoders](https://bit.ly/2USqMy4), ICML '08: Proceedings of the 25th international conference on Machine learning, 2008
+ S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio, [Contractive Auto-Encoders: Explicit Invariance During Feature Extraction](https://bit.ly/2K2WXVr), ICML 2011

