# Bayesian Approach got Neural Networks

## Overview

+ [The Bayesian framework](../ML/MLNN-Hinton/09-Overfitting.md#94-introduction-to-the-bayesian-approach)
  + instead of looking for the most likely setting of the parameters of a model, consider all possible settings of the parameters
  + trying to figure out for each of these possible settings have probabilities given the data we observed
  + assumption: always have a prior distribution for everything
    + the prior may be very vague
    + with given data, combine the prior distribution w/ a likelihood term to get a posterior distribution
  + likelihood term: how probable the observed data is given the parameters of the model
    + flavor parameter settings that make the data likely
    + fight the prior
    + always win w/ enough data: even w/ the wrong prior but end up w/ the right hypothesis if awful of data

+ [Maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (MLE)
  + a method of estimating the parameters of a probability distribution by maximizing a likelihood function
  + the assumed statistical model: the observed data is most probable
  + maximum likelihood estimate: the point in the parameter space that maximizes the likelihood function
  + Bayesian inference: a special case of maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters
  + frequentist inference: a special case of an extremum estimator, with the objective function being the likelihood

+ [Bayes Theorem](../ML/MLNN-Hinton/09-Overfitting.md#94-introduction-to-the-bayesian-approach)
  + equivalent expression for the join probability
  
    \[ p(D)p(W|D) = p(D, W) = p(W)p(D|W) \]

    + $p(D, W)$: join probability with a set of parameters $W$ and some data $D$
    + for supervised learning, the data is going to consist of the target values
    + $p(W|D), p(D|W)$: conditional probability

  + Bayes theorem

    \[ p(W|D) = \frac{p(W) p(D|W)}{p(D)} \]

    + $p(W|D)$: posterior probability of weight vector $W$ given training data $D$
    + $p(W)$: prior probability of weight vector $W$
    + $p(D|W)$: probability of observed data given $W$
    + $p(D) = \int_W p(W)p(D|W)$


## Weight Decay

+ [Bayesian approaches](../ML/MLNN-Hinton/09-Overfitting.md#95-the-bayesian-interpretation-of-weight-decay)
  + full model: trying to compute the posterior probability of every possible setting of the parameters of a model
  + reduced form: looking for single set of parameters that is best compromise between
    + fitting my prior beliefs about what the parametres should be like
    + fitting the data observed

+ [Supervised Maximum Likelihood Learning](../ML/MLNN-Hinton/09-Overfitting.md#95-the-bayesian-interpretation-of-weight-decay)
  + finding a weight vector that minimizes the squared residuals is equivalent to finding a weight vector that minimizes the log probability density of the correct answer
  + assume the answer is generated by adding Gaussian noise to the output of the neural network
  + max likelihood learning

    <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
      <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture9/lec9.pptx" ismap target="_blank">
        <img src="../ML/MLNN-Hinton/img/m09-10.png" style="margin: 0.1em;" alt="Supervised maximum likelihood" title="Supervised maximum likelihood" width=250>
      </a>
    </div>

  + probability density of the target value given the net's output plus Gaussian noise

    \[ p(t_c | y_c) = \underbrace{\frac{1}{\sqrt{2\pi} \sigma} \exp \left(-\frac{(t_c - y_c)^2}{2\sigma^2}\right)}_{\text{Gaussian distribution}\\ \text{centered at the net's output}} \]

  + cost: minimizing squared error is the same as maximizing log prob under a Gaussian

    \[ -\log p(t_c | y_c) = k + \frac{(t_c - y_c)^2}{2 \sigma^2} \]

+ [MAP: Maximum a Posterior](../ML/MLNN-Hinton/09-Overfitting.md#95-the-bayesian-interpretation-of-weight-decay)
  + proper Bayesian approach: Bayesian have all sort of clever tricks for approximating this horrendous distribution, including Monte Carlo methods
  + simple way to find the most probable weight vector
  + easier to work in the log domain than the probability domain: minimize a cost $\implies$ using negative log probabilities
  + math representation

    \[\begin{array}{rcccccc}
      & p(W|D) & = & p(W) & p(D|W) & / & p(D) \\
        & \downarrow & & \downarrow & \downarrow & & \downarrow\\
      Cost = & -\log p(W|D) &=& - \log p(W) & -\log p(D|W) & + & \log p(D)
    \end{array}\]

    + $\log p(W)$: log probability of $W$ under the prior
    + $\log p(D|W)$: log probability of target values given $W$
    + $\log p(D)$: an integral over all possible weight vectors so it does not depend on $W$

+ [The Bayesian interpretation](../ML/MLNN-Hinton/09-Overfitting.md#95-the-bayesian-interpretation-of-weight-decay)

  \[\begin{array}{cccc}
    -\log p(W|D) & = & -\log p(D|W) & -\log p(W) & + \underbrace{\log p(D)}_{\text{constant}} \\
    \downarrow & & \downarrow & \downarrow & \\
    C^* & = & \frac{1}{2\sigma_D^2} \sum_c (y_c - t_c)^2 & + \frac{1}{2\sigma_W^2} \sum_i w_i^2
  \end{array}\]

  + Aggregating all cases for cost function

+ [MacKay's method](../ML/MLNN-Hinton/09-Overfitting.md#96-mackays-quick-and-dirty-method-of-fixing-weight-costs)
  1. start w/ guesses for both the noise variance and the weight prior variance
  2. while not yet bored
    + learning using the ratio of the variances as the weight penalty coefficient
    + reset the noise variance to be the variance of the residual errors
    + reset the weight prior variance to be the variance of the distribution of the actual learned weights
  3. repeat step 1 & 2



