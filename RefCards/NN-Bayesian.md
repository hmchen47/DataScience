# Bayesian Approach for Neural Networks

## Overview

+ [The Bayesian framework](../ML/MLNN-Hinton/09-Overfitting.md#94-introduction-to-the-bayesian-approach)
  + instead of looking for the most likely setting of the parameters of a model, consider all possible settings of the parameters
  + trying to figure out for each of these possible settings have probabilities given the data we observed
  + assumption: always have a prior distribution for everything
    + the prior may be very vague
    + with given data, combine the prior distribution w/ a likelihood term to get a posterior distribution
  + likelihood term: how probable the observed data is given the parameters of the model
    + flavor parameter settings that make the data likely
    + fight the prior
    + always win w/ enough data: even w/ the wrong prior but end up w/ the right hypothesis if awful of data

+ [Maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (MLE)
  + a method of estimating the parameters of a probability distribution by maximizing a likelihood function
  + the assumed statistical model: the observed data is most probable
  + maximum likelihood estimate: the point in the parameter space that maximizes the likelihood function
  + Bayesian inference: a special case of maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters
  + frequentist inference: a special case of an extremum estimator, with the objective function being the likelihood

+ [Bayes Theorem](../ML/MLNN-Hinton/09-Overfitting.md#94-introduction-to-the-bayesian-approach)
  + equivalent expression for the join probability
  
    \[ p(D)p(W|D) = p(D, W) = p(W)p(D|W) \]

    + $p(D, W)$: join probability with a set of parameters $W$ and some data $D$
    + for supervised learning, the data is going to consist of the target values
    + $p(W|D), p(D|W)$: conditional probability

  + Bayes theorem

    \[ p(W|D) = \frac{p(W) p(D|W)}{p(D)} \]

    + $p(W|D)$: posterior probability of weight vector $W$ given training data $D$
    + $p(W)$: prior probability of weight vector $W$
    + $p(D|W)$: probability of observed data given $W$
    + $p(D) = \int_W p(W)p(D|W)$


## Weight Decay

+ [Bayesian approaches](../ML/MLNN-Hinton/09-Overfitting.md#95-the-bayesian-interpretation-of-weight-decay)
  + full model: trying to compute the posterior probability of every possible setting of the parameters of a model
  + reduced form: looking for single set of parameters that is best compromise between
    + fitting my prior beliefs about what the parametres should be like
    + fitting the data observed

+ [Supervised Maximum Likelihood Learning](../ML/MLNN-Hinton/09-Overfitting.md#95-the-bayesian-interpretation-of-weight-decay)
  + finding a weight vector that minimizes the squared residuals is equivalent to finding a weight vector that minimizes the log probability density of the correct answer
  + assume the answer is generated by adding Gaussian noise to the output of the neural network
  + max likelihood learning

    <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
      <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture9/lec9.pptx" ismap target="_blank">
        <img src="../ML/MLNN-Hinton/img/m09-10.png" style="margin: 0.1em;" alt="Supervised maximum likelihood" title="Supervised maximum likelihood" width=250>
      </a>
    </div>

  + probability density of the target value given the net's output plus Gaussian noise

    \[ p(t_c | y_c) = \underbrace{\frac{1}{\sqrt{2\pi} \sigma} \exp \left(-\frac{(t_c - y_c)^2}{2\sigma^2}\right)}_{\text{Gaussian distribution}\\ \text{centered at the net's output}} \]

  + cost: minimizing squared error is the same as maximizing log prob under a Gaussian

    \[ -\log p(t_c | y_c) = k + \frac{(t_c - y_c)^2}{2 \sigma^2} \]

+ [MAP: Maximum a Posterior](../ML/MLNN-Hinton/09-Overfitting.md#95-the-bayesian-interpretation-of-weight-decay)
  + proper Bayesian approach: Bayesian have all sort of clever tricks for approximating this horrendous distribution, including Monte Carlo methods
  + simple way to find the most probable weight vector
  + easier to work in the log domain than the probability domain: minimize a cost $\implies$ using negative log probabilities
  + math representation

    \[\begin{array}{rcccccc}
      & p(W|D) & = & p(W) & p(D|W) & / & p(D) \\
        & \downarrow & & \downarrow & \downarrow & & \downarrow\\
      Cost = & -\log p(W|D) &=& - \log p(W) & -\log p(D|W) & + & \log p(D)
    \end{array}\]

    + $\log p(W)$: log probability of $W$ under the prior
    + $\log p(D|W)$: log probability of target values given $W$
    + $\log p(D)$: an integral over all possible weight vectors so it does not depend on $W$

+ [The Bayesian interpretation](../ML/MLNN-Hinton/09-Overfitting.md#95-the-bayesian-interpretation-of-weight-decay)

  \[\begin{array}{cccc}
    -\log p(W|D) & = & -\log p(D|W) & -\log p(W) & + \underbrace{\log p(D)}_{\text{constant}} \\
    \downarrow & & \downarrow & \downarrow & \\
    C^* & = & \frac{1}{2\sigma_D^2} \sum_c (y_c - t_c)^2 & + \frac{1}{2\sigma_W^2} \sum_i w_i^2
  \end{array}\]

  + Aggregating all cases for cost function

+ [MacKay's method](../ML/MLNN-Hinton/09-Overfitting.md#96-mackays-quick-and-dirty-method-of-fixing-weight-costs)
  1. start w/ guesses for both the noise variance and the weight prior variance
  2. while not yet bored
    + learning using the ratio of the variances as the weight penalty coefficient
    + reset the noise variance to be the variance of the residual errors
    + reset the weight prior variance to be the variance of the distribution of the actual learned weights
  3. repeat step 1 & 2


## Full Bayesian Learning

+ [Full Bayesian learning](../ML/MLNN-Hinton/09-Overfitting.md#91-overview-of-ways-to-improve-generalization)
  + compute the full posterior distribution over all possible parameter settings
  + making prediction
    + prediction based on each different setting of the parameters
    + combine all these predictions by weighting each of them w/ the posterior probability of that setting of the parameters
    + very computationally intensive
  + advantage: allowing to use complicated models even w/o much data

+ [Approximating full Bayesian learning in a neural network](../ML/MLNN-Hinton/09-Overfitting.md#91-overview-of-ways-to-improve-generalization)
  + neural network w/ a few parameters
    + put a grid over the parameters space
    + evaluate $p(W|D)$ at each grid-point
    + each parameter only allow a few alternative values
    + taking the cross-product of all these values for all the parameters
    + evaluate at each point how well the model predict the data as if supervised learning hwo well the model predicts the targets
    + the posterior probability of that group point
    + characteristics: expensive; not involving any gradient descent; no local optimum issues
  + evaluating all predictions made w/ grid points on test data
    + expensive
    + much better than ML learning when the posterior is vague or multimodal (happened when data is scarce)

  \[ p(t_{test} | input_{test}) = \sum_{g \in grid} p(W_g | D) p(t_{test} | input_{test}, W_g) \]

+ [Dealing w/ too many parameters for a grid](../ML/MLNN-Hinton/10-CombineDropout.md#103-the-idea-of-full-bayesian-learning)
  + the numbder of grid-point: exponential growth as the number of parameters increase
  + enough data
    + only a tiny fraction of the grid points makes a significant contribution to the predictions
    + focus on evaluating this tiny fraction
  + idea: good enough to just sample weight vectors according to their posterior probabilities

    \[ p(y_{test} | input_{test}, D) = \sum_i \underbrace{p(W_i | D)}_{\text{sample weight vectors}\\ \text{with this probability}} p(y_{text} | input_{test}, W_i) \]

+ [Vector space and Gaussian noise](../ML/MLNN-Hinton/10-CombineDropout.md#103-the-idea-of-full-bayesian-learning)
  + standard backpropagation
  + situations of the weights (left diagram)
    + settle into a local minimum
    + get stuck on a plateau
    + just move so slowly that we run out of patience
  + adding Gaussian noise after each update (right diagram)
    + weight vector never settle down
    + keep wandering around but tend to prefer low cost regions of the weight space
    + how often visiting each possible setting of the weights?
    + red dots:
      + samples take of the weights as wandering around the space and then save the weights after every 10000 steps
      + a few of them in high-cost regions $\impliedby$ big regions
      + the depest minimum w/ the most red dots while other minima w/ red dots as well
      + the dots not right at bottom or minimum  $\impliedby$ noisy samples

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture10/lec10.pptx" ismap target="_blank">
      <img src="../ML/MLNN-Hinton/img/m10-11.png" style="margin: 0.1em;" alt="Situations of weight vector in weight space" title="Situations of weight vector in weight space" height=200>
      <img src="../ML/MLNN-Hinton/img/m10-12.png" style="margin: 0.1em;" alt="Weight vector w/ Gaussian noise" title="Weight vector w/ Gaussian noise" height=200>
    </a>
  </div>

+ [Markov Chain Monte Carlo (MCMC)](../ML/MLNN-Hinton/10-CombineDropout.md#103-the-idea-of-full-bayesian-learning)
  + property of Markov Chain Monte Carlo method
    + use just the right amount noise
    + let the weight vector wander around for long enough taking a sample
    + get an unbiased sample from the true posterior over weight vectors
    + make it feasible to use full Bayesian learning w/ thousands of parameters

+ [Full Bayesian learning w/ mini-batches](../ML/MLNN-Hinton/10-CombineDropout.md#103-the-idea-of-full-bayesian-learning)
  + computing the gradient of the cost function on a random mini-batch
  + proposal of Ahn, Korattikara & Welling
    + how to do this efficiently
    + possible w/ lots of parameters


