# Fast Learning Algorithms

[Chapter 8](http://page.mi.fu-berlin.de/rojas/neural/chapter/K8.pdf) in [Neural Networks - A Systematic Introduction](http://page.mi.fu-berlin.de/rojas/neural/), 1996


## 8.1 Introduction - Classical backpropagation



### 8.1.1 Backpropagation with momentum



### 8.1.2 The fractal geometry of backpropagation



## 8.2 Some simple improvements to backpropagation



### 8.2.1 Initial weight selection



### 8.2.2 Clipped derivatives and offset term



### 8.2.3 Reducing the number of floating-point operations



### 8.2.4 Data decorrelation



## 8.3 Adaptive step algorithms



### 8.3.1 Silva and AlmeidaÂ´s algorithm



### 8.3.2 Delta-bar-delta



### 8.3.3 RPROP



### 8.3.4 The Dynamic Adaption Algorithm



## 8.4 Second-order algorithms



### 8.4.1 Quickprop



### 8.4.2 Second-order backpropagation



## 8.5 Relaxation methods



### 8.5.1 Weight and node perturbation



### 8.5.2 Symmetric and asymmetric relaxation



### 8.5.3 A final thought on taxonomy



### 8.6 Historical and bibliographical remarks






