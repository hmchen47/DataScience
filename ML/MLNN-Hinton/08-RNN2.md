# 8. Recurrent Neural Networks II
  
## 8.1 A brief overview of Hessian-free optimization

### Lecture Notes

+ How much can we reduce the error by moving in a given direction?
  + If choosing a direction to move in and keep going in that direction, how much does the error decrease before it starts rising again?
    + assumption: the curvature is constant; i.e. a quadratic error surface
    + assumption: the magnitude of the gradient decreases as moving down the gradient, i.e. the error surface is convex upward
  + maximum error reduction
    + depending on the ratio of the gradient to the curvature
    + good direction to move in: the one w/ a high ratio of gradient to curvature, even if the gradient itself is small
  + Example of a direction to move in (see diagram)
    + vertical axis: error
    + horizontal axis: weights in the direction moving
    + blue arrow: the reduction we get if the start at the red point
    + upper diagram w/ a gentle gradient
      + a better gracious gradient to the curvature
      + a bigger reduction in the arrow by the time to get minimum
    + how can we find the directions w/ the upper diagram?
    + directions in which even though the gradient may be small, the curvature is even smaller
  + how to find such directions?

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
      <img src="img/m08-01.png" style="margin: 0.1em;" alt="Direction and how much error to reduce" title="Direction and how much error to reduce" width=250>
    </a>
  </div>

+ Newton's method
  + the basic problem
    + assumption: the steepest descent on a quadratic error surface
    + not the direction where the gradient would go in
    + error surface w/ circular cross-section: the gradient is a good direction which points to the minimum
    + applying a linear transformation to turn ellipses into circles; going downhill in a circular error surface
  + Newton's method multiplies the gradient vector by the inverse of the curvature matrix $H$

    \[ \Delta \mathbf{w} = -\varepsilon H(\mathbf{w})^{-1} \frac{d E}{d\mathbf{w}} \]

    + $H(\mathbf{w})$: the Hessian transformation, a function of weights
    + real quadratic surface: jump to the minimum in one step if $\varepsilon$ chosen correctly
    + infeasible to invert the matrix w/ many parameters, e.g., a million weights $\to$ trillion terms of the curvature matrix

+ Curvature Matrix
  + elements of curvature matrix
    + each weight $w_i$ or $w_j$ telling how the gradient in one direction changes as you change in another direction
    + ie, if $w_i$ changed, how does the gradient of the error w.r.t. $w_j$ change? $\to$ typical off diagonal terms
    + for diagonal entry, how the gradient of the error changes in direction of the weight as you change that weight
    + specifying how the gradient in one direction changes as moving into some other direction
    + off-diagonal terms correspond to twists in the error surface
    + twist: when you travel in one direction, the gradient in another direction changes
    + nice circular bulb: all those off diagonal terms are zero, ie, the gradient in other directions not changed
  + reason about wrong direction w/ steepest descent
    + the gradient for one weight messed up by the simultaneous changes to all the other weights
    + ie, updating other weights while changing one weight
    + curvature matrix determines the sizes of these interactions

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
      <img src="img/m08-02.png" style="margin: 0.1em;" alt="Curvature matrix" title="Curvature matrix" width=250>
    </a>
  </div>

+ How to avoid inverting a huge matrix
  + curvature matrix w/ too many terms in a big network
    + Le Cun: just using the terms along the leading diagonal of the curvature matrix
      + making the step size depends on the leading diagonal
      + get different step size w/ different weights
    + only a tiny fraction of the interactions (self-interactions)
      + ignoring most of the terms in the curvature matrix
  + approximated in many different ways
    + approximation w/ much lower rank matrix which captures the main aspects of the curvature matrix
    + including, Hessian-free method, LBFGS, and many other methods trying to do an approximate second-order method for minimizing the error
  + Hessian-free (HF) method
    + making an approximation to the curvature matrix
    + assuming the approximation correct
    + the curvature known and the error surface really quadratic
    + using conjugate gradient, an efficient technique to minimize the error $\to$ close to a minimum on this approximation to the curvature
    + making another approximation and reaching the minimum w/ conjugate gradient again
    + RNN
      + adding a penalty for changing any of the hidden activities too much
      + preventing from changing a weight early on that causes huge effects later on in the sequence
      + putting quadratic penalty on those changes then combining it w/ the rest of the Hessian method

+ Conjugate gradient
  + an alternative to going to the minimum in one step by multiplying by the inverse of the curvature matrix
    + start by taking the direction of steepest descent and go to the minimum in that direction
    + might involve re-evaluating the gradient or re-evaluating the error a few times to find the minimum in that direction
  + using a sequence of steps each of which finds the minimum along one direction
  + ensuring moving in the conjugate direction
    + conjugate direction: a direction conjugate to the previous directions
    + not messing up the minimization already done
    + conjugate: as you go in the new direction, you do not change the __gradients__ in the previous directions
    + opposite: twist
  + a picture of conjugate gradient (see diagram)
    + red line: major axis of the ellipse
    + black arrow: start off one step of steepest descent and all the way to the minimum in that direction
    + the minimum not always lying on the red line
    + gradient = 0 $\to$ black arrow perpendicular to the red line
    + the actual black arrow (the gradient) not perpendicular to the red line
    + making a little progress by making a small step at right angle to the red line and a small step along the red line
    + the gradient in the direction of the first step is zero at all points on the green line
      + the gradient in the direction of black arrow is zero w.r.t the green line
    + if moving along the green line, don't mess up the minimization already did in the first direction
    + searching on the green line to find how far we should go to minimize the error along the green line
    + repeating the procedure in high dimensional error surface $\to$ reaching the minimum eventually
    + all directions considered $\to$ global minimum

    <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
      <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
        <img src="img/m08-03.png" style="margin: 0.1em;" alt="Conjugate gradient" title="Conjugate gradient" width=200>
      </a>
    </div>

+ Goal of conjugate 
  + Objective: global minimum of an $N$-dim quadratic surface
  + in $N$ steps, conjugate gradient guaranteed to find the minimum of an $N$-dim <span style="color: red;">quadratic</span> surface
    + managing to get thr gradient = 0 in N directions
    + not orthogonal directions but independent of one another $\to$ global minimum
    + usually many cases w/ less than $N$ steps, it has typically got the error very close to the minimum value
    + not full $N$ step executed $\to$ expensive as inverting the whole matrix
  + non-linear conjugate gradient
    + able to apply directly to a non-quadratic error surface, such as the error surface for a multilayer non-linear neural network
    + usually working well
    + essentially a batch method and able to apply it to large mini batches
    + done by many steps of conjugate gradient on the same large mini batch and then move on to the next item in your batch
  + HF optimizer:
    + using conjugate gradient for minimization on a genuinely quadratic surface where it excels
    + genuinely quadratic surface: the quadratic approximation to the true surface made by Hessian-free matrix


### Lecture Video

<video src="https://youtu.be/K2X0eBd-0lc?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>

 
## 8.2 Modeling character strings with multiplicative connections

### Lecture Notes

+ Modeling text: advantages of working with characters
  + web page composed of character strings
  + powerful learning method
    + by understanding the world by reading the web
     + to learn which strings make words
  + big hassle: pre-processing text to get words
    + what about morphemes (<span style="color: red;">pre</span>fixes, suffix<span style="color: red;">es</span> etc)
      + morpheme: the smallest meaningful unit in a language
    + what about subtle effects line "sn" words?
      + "sn": very high chance of meaning somthing about the lips or nose, particularly he upper lip or nose
      + e.g., snarl, sneexe, snot, snog, snort
      + how about snow?  a good word for cocaine
    + what about New York?
      + treated as on elxical item
      + "new york Minster roof": treat new and york to separate lexical items
    + what about Finnish?
      + put lots of morphemes to make a great big words
      + (<span style="color: red;">ymm&auml;rt&auml;m&auml;ttomyydell&auml;ns&auml;k&auml;&auml;n</span>): combine fiev words

+ An obvious recurrent neural network
  + purpose: modeling character strings
  + architecture and process:
    + 1500 hidden states
    + inputs: characters and hiddent state dynamics
    + hidden state dynamics: the hidden state at time $t$ providing input to determine the hidden state at time $t+1$
    + predict the next character when retaining the new hidden state (new character + previous hidden state dynamics)
    + single softmax over the 86 characters
    + try to find high probability to the correct next character and low probability to the ortehrs
    + train the whole system by back propagating from that softmax the log probability of getting the correct character
      + back propagat the log probability through the two output connections back through the hidden two character connections
      + then through the hideent-to-hidden connections
      + all the way back till the beginning of the string
  + a lot easier to predict 86 characters than 100,000 words
  + easy to use the softmax at output
  + no problem w/ a great big sofmax

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
      <img src="img/m08-04.png" style="margin: 0.1em;" alt="Recurrent neural net for character strings" title="Recurrent neural net for character strings" width=350>
    </a>
  </div>

+ Tree structure for character string
  + reeason for not using RNN but instead a different kind of network
  + modeling string w/ tree
    + ranging all possible character strings int a tree w/ a branching ratio of 86 here
    + a tiny little subtree of that great big tree: occuring many times bu w/ different things represented by that dots (...) before the fix
    + representing many characters followed by `...fix`
      + left branch: trailed by 'i'
      + right branch: trailed by 'e'
    + every new character moving one step down in the the tree to a new node
  + exponentially many nodes in the tree of all character strings of length $N$
    + might be too many nodes to store them all
    + proabaility on each connection: the probability of producing that letter given the context of the node
  + RNN:
    + enormous tree
    + a hidden state vector to represent each node
    + next character must transform to a new node
      + the node w/ the hidden state vector represnting a whole string of characters followed by 'fix'
      + operating on the hidden state vector to produce the appropriate new hidden state vector
  + node implemented as hidden states in an RNN
    + different node able to share structure
      + most likely a verb as 'fix' shown
      + the character 'i' is more likely beacuse of the ending 'ing'
      + the knowledge of verb w/ 'fix' sharing w/ lots of nodes
    + operting on the part of the state represnting a verb able to share with all the verbs
    + using distributed representation
  + next hidden representation
    + conjunction of the current state at and the character to determine which branch to take
      + fllowed 'fix' not saying that 'i' tends to expect an 'n' next
      + but supposed that the string is a verb, an 'i' should expect an 'n' next
      + conjunction the fact that a verb gets into this state labeled 'fixi' expecting to see an 'n'
    + depending on the conjunction of the current character and the current hidden representation

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
      <img src="img/m08-05.png" style="margin: 0.1em;" alt="Sub-tree in the tree of all character strings" title="Sub-tree in the tree of all character strings" width=250>
    </a>
  </div>

+ Multiplicative connections
  + inputs of recurrent net
    + traditional: using the character inputs to the recurrent net to provide extra additive input to the hidden states
    + capturing inputs by using multiplicative connections
      + use those characters to swap in a whole hidden-to-hidden weight matrix
      + the character determined the transistion matrix
      + using the current input character to choose the whole hidden-to-hidden weight matrix
    + naive method: 86x1500x1500 parameters $\to$ too many parameters $\to$ making the net overfit
  + multiplicative interaction w/ fewer parameters
    + different transition matrix for each of the 86 characters
    + characters w/ common characteristics
    + making these 86 character-specific weight matrices to share parameters
    + e.g., all digits are similar to each other in the way they amkd the hidden state evolve
    + e.g., the character 9 and 8 should have similar matrices

+ Using factors to implement multiplicative interactions
  + factor
    + the triangle w/ $f$ (see diagram)
    + group $a$ and group $b$ interact multiplicatively to provide input to group $c$
  + get groups $a$ and $b$ to interact multiplicatively by using "factors"
    1. each factor first computes a weighted sum for each of its input groups
      + taking the vector state of group $a$
      + multiplying the vector by the weight on the connection comming into the factor
      + i.e., taking the scalar product of the vector $a$ and the weight vector $\mathbf{u}$
      + obtaining a number at the left-hand vertex of that triangle
      + similarly, taking vector state of group $b$ and mutiplying by the weight vector $\mathbf{w}$ to get the bottomvertex of that triangle
    2. the send the product of the weighted sums to its output group
      + multiply those two number obtained in step 1
      + obtain a scalar number
      + use the scalar to scal ethe outgoing weight $\mathbf{v}$ to provide input of group $c$
      + the input of group $c$: product of the two numbers from the two vertices of the triangle times the outgoing weight vector $\mathbf{v}$
    + mathematical representation

    \[ \mathbf{c}_f = \left( \mathbf{b}^T \mathbf{w}_f \right) \left( \mathbf{a}^T \mathbf{u}_f \right) \mathbf{v}_f \]

    + $\mathbf{c}_f$: vector of inputs to group $c$
    + $(\mathbf{b}^T \mathbf{w}_f)$: scalar input to $f$ from group $b$
    + $(\mathbf{a}^T \mathbf{u}_f)$: scalar input to $f$ from group $a$

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
      <img src="img/m08-06.png" style="margin: 0.1em;" alt="Factors for multiplicative interactions" title="Factors for multiplicative interactions" width=200>
    </a>
  </div>

+ Using factors to implement a set of basis matrices
  + each factor defines a rank 1 transition matrix (the product of two vectors) from $a$ to $c$
  + treat a factor as computing two scallar products multiplying them
  + using the product as a weight on the outgoing vector $\mathbf{v}$
  + mathematical representation

    \[\begin{align*}
      \mathbf{c}_f &= \left( \mathbf{b}^T \mathbf{w}_f \right) \left( \mathbf{a}^T \mathbf{u}_f \right) \mathbf{v}_f \\
        &= \left( \mathbf{b}^T \mathbf{w}_f \right) \left( \mathbf{u}^T \mathbf{v}^T_f \right) \mathbf{a} \\
      \mathbf{c} &= \left( \sum_f \left( \mathbf{b}^T \mathbf{w}_f \right) \left( \mathbf{u}_f \mathbf{V}_f^T \right) \right) \mathbf{a} \tag{1}
    \end{align*}\]

    + $( \mathbf{b}^T \mathbf{w}_f )$: scalar coefficient
    + $( \mathbf{u}^T \mathbf{v}^T_f )$: outer product transition matrix with rank 1
    + $\mathbf{c}$: the sum of all factors

  + the sum of left-hand side of Eq. (1): a great big transistion matrix
  + multiply the transition matrix by the current hiddent state to produce a new hidden state
  + synthesized the transition matrix out of these rank 1 matrices provided by each factor
  + the current character in group $b$: determined the weight on each of the rank 1 matrices

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
      <img src="img/m08-07.png" style="margin: 0.1em;" alt="Factors for a set of basis matricess" title="Factors for a set of basis matricess" width=200>
    </a>
  </div>

+ Using a 3-way factors to allow a character to create a whole transition matrix
  + each factor, $f$, defines a rank one matrix, $\mathbf{U}_f \mathbf{V}_f^T$
  + each character, $k$, determines a <span style="color: red;">gain</span> $W_{kf}$ for each of these matrices
  + a number of factors (~1500)
  + the character input is different in only one of those active $\to$ only one relvant weight at a time
  + $\mathbf{w}_kf$: the weight from the current character $k$

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
      <img src="img/m08-08.png" style="margin: 0.1em;" alt="3-way factors to create a whole transition matrix" title="3-way factors to create a whole transition matrix" width=450>
    </a>
  </div>


### Lecture Video

<video src="https://youtu.be/qhFF6-K-5kM?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


## 8.3 Learning to predict the next character using HF

### Lecture Notes

+ Training the character model
  + Ilya Sutskever proposal: [Generating Text with Recurrent Neural Networks](https://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf)
  + Ilya Sutskever [thesis](https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)
  + used 5 million string of 100 characters taken from Wikipedia
  + objective: predict the 11th character of each string
  + using HF optimizer: a moth on a GPU board to get a really good model
  + Ilya's current best RNN: might be th ebest signgle model for character prediction
    + combinations of many models do better
  + working in a very different way from the best other models
    + able to balance quotes and brackets over long distances
    + relying on matching previous contexts not able to

+ How to generate character strings from the model
  + Procedure
    + starting the model w/ its default hidden state
    + give it a "burn-in" sequence of characters and lt it update its hidden state after each character
    + then look at the probability distribution it predicts for the next character
    + pick a character randomly from the distribution and tell the net that this was the characcter that actually occurred
      + i.e., tell it that its guess was correct, whatever it guessed
    + continue to let it pick character until bored
    + look at the character string it produces to see what it "knows"
  + Example

    > He was elected President during the Revolutionary War and forgave Opus Paul at Rome. The regime of his crew of England, is now Arab women's icons in  and the demons that use something between the characters‘ sisters in lower coil trains were always operated on the line of the <span style="color: red;">ephemerable</span> street, respectively, the graphic or other facility for deformation of a given proportion of large segments at RTUS<span style="color: red;">)</span>. The B every chord was a "strongly cold internal palette pour even the white blade.”

  + some completions produced by the model
    + Shelia thrunge<span style="color: red;">s</span> &nbsp;&nbsp;&nbsp;&nbsp;<span style="color: blue;">(most frequent)</span>
    + People thruge &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: blue;">(most frequent next character is space)</span>
    + Shiela, Thrunge<span style="color: red;">lini del Rey</span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: blue;">(first tru)</span>
    + The meaning of the life is <span style="color: red;">literary regognition.</span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: blue;">(6th try)</span>
    + The meaning of leife is <span style="color: red;">the tradition of the ancient human reproductio: it is less favorable tot he good boy for when to remove her biggeer.</span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: blue;">(one fo the first 10 tries for a model trained for longer.)</span>

+ What does it know?
  + a huge number of words and a lot about proper names, dates, and numbers
  + good at balancing quotes and brackets
    + count brackets: none, one, many
  + a lot about syntax but very hard to pin down exactly what form this knowledge has
    + syntactic knowledge not modular
  + a lot of weak semantic associations
    + e.g., know Plato associated w/ Wittgenstein; cabbage associated w/ vegetable

+ RNNs for predicting the next word
  + Tomas Mikolov et. al. proposal
    + [Recurrent neural network based language model](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf), 2010
    + [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), 2013
    + [On the difficulty of training Recurrent Neural Networks](https://arxiv.org/pdf/1211.5063.pdf), 2013
    + [Learning Longer Memory in Recurrent Neural Networks](https://arxiv.org/pdf/1412.7753), 2015
    + train large RNNS on quite large training sets using BPTT
      + better than feed-forward neural nets
      + better than the best other modes
      + even betetr when averaged w/ other models
    + RNNs:
      + require much less training data to reach the sam elevel of performance as other models
      + improve faster than other methods as the dataset getting bigger


### Lecture Video

<video src="url" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


## 8.4 Echo state networks

### Lecture Notes



### Lecture Video

<video src="url" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


