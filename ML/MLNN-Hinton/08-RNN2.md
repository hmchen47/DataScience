# 8. Recurrent Neural Networks II
  
## 8.1 A brief overview of Hessian-free optimization

### Lecture Notes

+ How much can we reduce the error by moving in a given direction?
  + If choosing a direction to move in and keep going in that direction, how much does the error decrease before it starts rising again?
    + assumption: the curvature is constant; i.e. a quadratic error surface
    + assumption: the magnitude of the gradient decreases as moving down the gradient, i.e. the error surface is convex upward
  + maximum error reduction
    + depending on the ratio of the gradient to the curvature
    + good direction to move in: the one w/ a high ratio of gradient to curvature, even if the gradient itself is small
  + Example of a direction to move in (see diagram)
    + vertical axis: error
    + horizontal axis: weights in the direction moving
    + blue arrow: the reduction we get if the start at the red point
    + upper diagram w/ a gentle gradient
      + a better gracious gradient to the curvature
      + a bigger reduction in the arrow by the time to get minimum
    + how can we find the directions w/ the upper diagram?
    + directions in which even though the gradient may be small, the curvature is even smaller
  + how to find such directions?

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
      <img src="img/m08-01.png" style="margin: 0.1em;" alt="Direction and how much error to reduce" title="Direction and how much error to reduce" width=250>
    </a>
  </div>

+ Newton's method
  + the basic problem
    + assumption: the steepest descent on a quadratic error surface
    + not the direction where the gradient would go in
    + error surface w/ circular cross-section: the gradient is a good direction which points to the minimum
    + applying a linear transformation to turn ellipses into circles; going downhill in a circular error surface
  + Newton's method multiplies the gradient vector by the inverse of the curvature matrix $H$

    \[ \Delta \mathbf{w} = -\varepsilon H(\mathbf{w})^{-1} \frac{d E}{d\mathbf{w}} \]

    + $H(\mathbf{w})$: the Hessian transformation, a function of weights
    + real quadratic surface: jump to the minimum in one step if $\varepsilon$ chosen correctly
    + infeasible to invert the matrix w/ many parameters, e.g., a million weights $\to$ trillion terms of the curvature matrix

+ Curvature Matrix
  + elements of curvature matrix
    + each weight $w_i$ or $w_j$ telling how the gradient in one direction changes as you change in another direction
    + ie, if $w_i$ changed, how does the gradient of the error w.r.t. $w_j$ change? $\to$ typical off diagonal terms
    + for diagonal entry, how the gradient of the error changes in direction of the weight as you change that weight
    + specifying how the gradient in one direction changes as moving into some other direction
    + off-diagonal terms correspond to twists in the error surface
    + twist: when you travel in one direction, the gradient in another direction changes
    + nice circular bulb: all those off diagonal terms are zero, ie, the gradient in other directions not changed
  + reason about wrong direction w/ steepest descent
    + the gradient for one weight messed up by the simultaneous changes to all the other weights
    + ie, updating other weights while changing one weight
    + curvature matrix determines the sizes of these interactions

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
      <img src="img/m08-02.png" style="margin: 0.1em;" alt="Curvature matrix" title="Curvature matrix" width=250>
    </a>
  </div>

+ How to avoid inverting a huge matrix
  + curvature matrix w/ too many terms in a big network
    + Le Cun: just using the terms along the leading diagonal of the curvature matrix
      + making the step size depends on the leading diagonal
      + get different step size w/ different weights
    + only a tiny fraction of the interactions (self-interactions)
      + ignoring most of the terms in the curvature matrix
  + approximated in many different ways
    + approximation w/ much lower rank matrix which captures the main aspects of the curvature matrix
    + including, Hessian-free method, LBFGS, and many other methods trying to do an approximate second-order method for minimizing the error
  + Hessian-free (HF) method
    + making an approximation to the curvature matrix
    + assuming the approximation correct
    + the curvature known and the error surface really quadratic
    + using conjugate gradient, an efficient technique to minimize the error $\to$ close to a minimum on this approximation to the curvature
    + making another approximation and reaching the minimum w/ conjugate gradient again
    + RNN
      + adding a penalty for changing any of the hidden activities too much
      + preventing from changing a weight early on that causes huge effects later on in the sequence
      + putting quadratic penalty on those changes then combining it w/ the rest of the Hessian method

+ Conjugate gradient
  + an alternative to going to the minimum in one step by multiplying by the inverse of the curvature matrix
    + start by taking the direction of steepest descent and go to the minimum in that direction
    + might involve re-evaluating the gradient or re-evaluating the error a few times to find the minimum in that direction
  + using a sequence of steps each of which finds the minimum along one direction
  + ensuring moving in the conjugate direction
    + conjugate direction: a direction conjugate to the previous directions
    + not messing up the minimization already done
    + conjugate: as you go in the new direction, you do not change the __gradients__ in the previous directions
    + opposite: twist
  + a picture of conjugate gradient (see diagram)
    + red line: major axis of the ellipse
    + black arrow: start off one step of steepest descent and all the way to the minimum in that direction
    + the minimum not always lying on the red line
    + gradient = 0 $\to$ black arrow perpendicular to the red line
    + the actual black arrow (the gradient) not perpendicular to the red line
    + making a little progress by making a small step at right angle to the red line and a small step along the red line
    + the gradient in the direction of the first step is zero at all points on the green line
      + the gradient in the direction of black arrow is zero w.r.t the green line
    + if moving along the green line, don't mess up the minimization already did in the first direction
    + searching on the green line to find how far we should go to minimize the error along the green line
    + repeating the procedure in high dimensional error surface $\to$ reaching the minimum eventually
    + all directions considered $\to$ global minimum

    <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
      <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
        <img src="img/m08-03.png" style="margin: 0.1em;" alt="Conjugate gradient" title="Conjugate gradient" width=200>
      </a>
    </div>

+ Goal of conjugate 
  + Objective: global minimum of an $N$-dim quadratic surface
  + in $N$ steps, conjugate gradient guaranteed to find the minimum of an $N$-dim <span style="color: red;">quadratic</span> surface
    + managing to get thr gradient = 0 in N directions
    + not orthogonal directions but independent of one another $\to$ global minimum
    + usually many cases w/ less than $N$ steps, it has typically got the error very close to the minimum value
    + not full $N$ step executed $\to$ expensive as inverting the whole matrix
  + non-linear conjugate gradient
    + able to apply directly to a non-quadratic error surface, such as the error surface for a multilayer non-linear neural network
    + usually working well
    + essentially a batch method and able to apply it to large mini batches
    + done by many steps of conjugate gradient on the same large mini batch and then move on to the next item in your batch
  + HF optimizer:
    + using conjugate gradient for minimization on a genuinely quadratic surface where it excels
    + genuinely quadratic surface: the quadratic approximation to the true surface made by Hessian-free matrix


### Lecture Video

<video src="https://youtu.be/K2X0eBd-0lc?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


## 8.2 Modeling character strings with multiplicative connections

### Lecture Notes



### Lecture Video

<video src="url" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


## 8.3 Learning to predict the next character using HF

### Lecture Notes



### Lecture Video

<video src="url" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


## 8.4 Echo state networks

### Lecture Notes



### Lecture Video

<video src="url" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


