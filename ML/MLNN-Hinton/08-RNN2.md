# 8. Recurrent Neural Networks II
  
## 8.1 A brief overview of Hessian-free optimization

### Lecture Notes

+ How much can we reduce the error by moving in a given direction?
  + If choosing a direction to move in and keep going in that direction, how much does the error decrease before it starts rising again?
    + assumption: the curvature is constant; i.e. a quadratic error surface
    + assumption: the magnitude of the gradient decreases as moving down the gradient, i.e. the error surface is convex upward
  + maximum error reduction
    + depending on the ratio of the gradient to the curvature
    + good direction to move in: the one w/ a high ratio of gradient to curvature, even if the gradient itself is small
  + how to find such directions?

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
      <img src="img/m08-01.png" style="margin: 0.1em;" alt="Direction and how much error to reduce" title="Direction and how much error to reduce" width=250>
    </a>
  </div>

+ Newton's method
  + the basic problem
    + the steepest descent on a quadratic error surface
    + not the direction where the gradient points
    + error surface w/ circular cross-section: the gradient is fine
    + applying a linear transformation to turn ellipses into circles
  + Newton's method multiplies the gradient vector by the inverse of the curvature matrix $H$

    \[ \Delta \mathbf{w} = -\varepsilon H(\mathbf{w})^{-1} \frac{d E}{d\mathbf{w}} \]

    + real quadratic surface: jump to the minimum in one step
    + infeasible to invert the matrix w/ many parameters, e.g., a million weights $\to$ trillion terms of the curvature matrix

+ Curvature Matrix
  + elements of curvature matrix
    + specifying how the gradient in one direction changes as moving into some other direction
    + off-diagonal terms correspond to twists in the error surface
  + reason about wrong direction w/ steepest descent
    + the gradient for one weight messed up by the simultaneous changes to all the other weights
    + curvature matrix determines the sizes of these interactions

  <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
    <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
      <img src="img/m08-02.png" style="margin: 0.1em;" alt="Curvature matrix" title="Curvature matrix" width=250>
    </a>
  </div>

+ How to avoid inverting a huge matrix
  + curvature matrix w/ too many terms in a big network
    + Le Cunn: just using the terms along the leading diagonal
    + however, only a tiny fraction of the interactions (self-interactions)
  + approximated in many different ways; including, Hessian-free method, LBFGS, etc.
  + Hessian-free (HF) method
    + making an approximation to the curvature matrix
    + assuming the approximation correct
    + using conjugate gradient, an efficient technique to minimize the error
    + making another approximation and following the same procedure again
    + RNN: add a penalty for changing any of the hidden activities too much

+ Conjugate gradient
  + an alternative to going to the minimum in one step by multiplying by the inverse of the curvature matrix
  + using a sequence of steps each of which finds the minimum along one direction
  + ensuring each new direction is "conjugate" to the previous directions
    + not messing up the minimization already done
     conjugate: as you go in the new direction, you do not change the __gradients__ in the previous directions
  + a picture of conjugate gradient
    + the gradient in the direction of the first step is zero at all points on the green line
    + if moving along the green line, don't mess up the minimization already did in the first direction

    <div style="margin: 0.5em; display: flex; justify-content: center; align-items: center; flex-flow: row wrap;">
      <a href="http://www.cs.toronto.edu/~hinton/coursera/lecture8/lec8.pdf" ismap target="_blank">
        <img src="img/m08-03.png" style="margin: 0.1em;" alt="Conjugate gradient" title="Conjugate gradient" width=200>
      </a>
    </div>

+ Achievement of conjugate gradient
  + after $N$ steps, conjugate gradient guaranteed to find the minimum of an $N$-dim <span style="color: red;">quadratic>/span? surface
    + reason: after many less than $N$ steps, it has typically got the error very close to the minimum value
  + non-linear conjugate gradient: applied directly to a non-quadratic error surface and usually working well
  + HF optimizer:
    + using conjugate gradient for minimization on a genuinely quadratic surface where it excels
    + genuinely quadratic surface: the quadratic approximation to the true surface



### Lecture Video

<video src="https://youtu.be/K2X0eBd-0lc?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


## 8.2 Modeling character strings with multiplicative connections

### Lecture Notes



### Lecture Video

<video src="url" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


## 8.3 Learning to predict the next character using HF

### Lecture Notes



### Lecture Video

<video src="url" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


## 8.4 Echo state networks

### Lecture Notes



### Lecture Video

<video src="url" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width=180>
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


