# Programming Assignment: K-Means Clustering and PCA

## 1 K-means Clustering

+ Implement the K-means algorithm and use it for image compression

+ Start on an example 2D dataset that will help to gain an intuition of how the K-means algorithm works

+ Use the K-means algorithm for image compression by reducing the number of colors that occur in an image to only those that are most common in that image


### 1.1 Implementing K-means

+ K-means algorithm: a method to automatically cluster similar data examples together

+ Given a training set $\{ x^{(1)}, \dots, x^{(m)} \}$ where $x^{(i)} \in \mathbb{R}^n$

+ Group the training data into a few cohesive "clusters"

+ Intuition of K-means algorithm:
  + an iterative procedure
  + start by guessing the initial centroids
  + refine by repeatedly assigning examples to their closest centroids
  + recompute the centroids based on the assignments

+ K-means algorithm

  ```matlab 
  % Initialize centroids
  centroids = kMeansInitCentroids(X, K);
  for iter = 1:iterations
  % Cluster assignment step: Assign each data point to the
  % closest centroid. idx(i) corresponds to cˆ(i), the index
  % of the centroid assigned to example i
  idx = findClosestCentroids(X, centroids);

  % Move centroid step: Compute means based on centroid
  % assignments
  centroids = computeMeans(X, idx, K);
  end
  ```

+ The inner-loop repeatedly runs two steps
  1. assigning each training example $x^{(i)}$ to its cloesest centroid
  2. recompute the mean of each centroid using the point assigned to it

+ The K-Mean algorithm
  + always converge to final set of means for centroids
  + converged solution might not always be idea
  + the final solution depends on the initial setting of centroids
  + usually run few times with different random initialization
  + choose the one with the lowest cost function value (distortion)


#### 1.1.1 Finding closest centroids

+ Assign every training example $x^{(i)}$ to its closest centroid, given the current positions of centroids

+ for every example $i$

  $$x^{(i)} := \quad \text{ that minimize } \parallel x^{(i)} - \mu_j \parallel^2$$

  + $c^{(i)} =\;$ the index of centroid that closest to $x^{(i)}$
  + $\mu_j = \;$ the position (value) of the $j$-th centroid
  + Note that $c^{(i)}$ corresponds to `idx(i)` in the starter code

+ complete the code in `findClosestCentroids.m`
  + take the data matrix $X$ and the locations of all centroids inside `centroids`
  + output a 1-dim array `idx` that holds the index of the closest centroid to every training example
  + index as a value in $\{ 1, \dots, K \}$, where $K$ = total number of centroids

+ implement the function using a loop over every training example and every centroid

+ run `ex7.m` to evaluate the output `[1 3 2]` corresponding to the centroid assignments for the first 3 examples.


### 1.2 K-means on example dataset



### 1.3 Ransom initialization



### 1.4 Image compression with K-means1.5



### 1.5 Use your own image



## 2 Principal Component Analysis

### 2.1 Example Dataset



### 2.2 Implementing PCA



### 2.3 Dimensionality Reduction with PCA



### 2.4 Face image Dataset



### 2.5 PCA for visualization (optional)



## Programming Exercise Tutorial

### [`indClosestCentroids()` tutorial](https://www.coursera.org/learn/machine-learning/discussions/weeks/8/threads/ncYc-ddQEeWaURKFEvfOjQ)

This tutorial gives a method for `findClosestCentroids()` by iterating through the centroids. This runs considerably faster than looping through the training examples.

+ Create a "distance" matrix of size (m x K) and initialize it to all zeros. $m$ is the number of training examples, $K$ is the number of centroids.
+ Use a for-loop over the `1:K` centroids.
+ Inside this loop, create a column vector of the distance from each training example to that centroid, and store it as a column of the distance matrix. One method is to use the `bsxfun`) function and the `sum()` function to calculate the sum of the squares of the differences between each row in the X matrix and a centroid.
+ When the for-loop ends, you'll have a matrix of centroid distances.
+ Then return idx as the vector of the indexes of the locations with the minimum distance. The result is a vector of size (m x 1) with the indexes of the closest centroids.

---------------------------------------------

Additional implementation tips on how to use `bsxfun()`:

`bsxfun()` means "broadcast function". It applies whatever function you specify (such as @minus) between the 1st argument (a matrix) and the second argument (typically a row vector).

Here's an example you can enter in your console:

```matlab
Q = magic(3)
D = bsxfun(@minus, Q, [1 2 3])
```

It returns a value that is the same size as the first argument.

As implemented for this function, each row in the matrix returned by bsxfun() is the differences between that row of X and each of the features of one of the centroids.

Then, for each row, you compute the sum of the squares of those differences. Here is code that does it.

```matlab
sum(D.^2,2);
```

Note that if you leave off the ',2' part, then `sum()` will sum over the rows, and you'll instead get a (1xn) vector. Using ',2' tells `sum()` to work over the columns, so you'll get a (m x 1) result.

We repeat this process for each of the centroids. Once we have the squared distance between each example and each centroid, we then return the index where the `min()` value was found for each example.


### [`computeCentroids()` tutorial](https://www.coursera.org/learn/machine-learning/discussions/weeks/8/threads/WzfDM7LjEeatew7zqUaXxg)

The method in this tutorial iterates over the centroids.

The function computeCentroids is called with parameters "X, "idx" and "K".

"K" is the number of centroids.

"idx" is a vector with one entry for each example in "X", which tells you which centroid each example is assigned to. The values range from 1 to K, so you will need a for-loop over that range.

You can get a selection of all of the indexes for each centroid with:

```matlab
sel = find(idx == i) % where i ranges from 1 to K
```

Now we want to compute the mean of all these selected examples, and assign it as the new centroid value:

```matlab
centroids(i,:) = mean(X(sel,:),1);
```

Note: Using `mean(...,1)` causes the mean to be computed over the rows, in the event that there is a centroid that has only one example.

Note: This method breaks if sel is a null vector (i.e. if no examples are assigned to centroid 'i'. In that case, probably should only update `centroid(i,:)` if sel is a non-null vector.

Repeat this procedure all each centroid. The function returns the computed centroid values.



### [Tutorials for ex7_pca functions - `pca()`, `projectData()`, `recoverData()`](https://www.coursera.org/learn/machine-learning/programming/ZZkM2/k-means-clustering-and-pca/discussions/threads/wp_NfU55EeWxHxIGetKceQ)

These are tutorials for all three functions in the ex7 PCA exercise. All of these functions have a vectorized implementation in one or two lines of code.

--------------------------

`pca()`

Compute the transpose of X times X, scale by 1/m, and use the `svd()` function to return the U, S, and V matrices.

X is size (m x n), so "X transpose X" and U are both size (n x n)

(note: the feature matrix X has already been normalized, see ex7_pca.m)

--------------------------

`projectData()`

__Errata:__

In projectData.m, make the following change in the Instructions section:

```matlab
% projection_k = x' * U(:, 1:k);
```

------------------

Return Z, the product of X and the first 'K' columns of U.

X is size (m x n), and the portion of U is (n x K). Z is size (m x K).

--------------------------

`recoverData()`

Return X_rec, the product of Z and the first 'K' columns of U.

Dimensional analysis:

+ The original data set was size (m x n)
+ Z is size (m x K), where 'K' is the number of features we retained.
+ U is size (n x n), where 'n' is the number of features in the original set.
+ So "U(:,1:K)" is size (n x K).


So to restore an approximation of the original data set using only K features, we multiply (m x K) * (K x n), giving a (m x n) result.


### [Programming Exercise 7:K-Means Clustering and PCA](https://www.coursera.org/learn/machine-learning/resources/ybmEa)

#### Debugging Tip

The submit script, for all the programming assignments, does not report the line number and location of the error when it crashes. The follow method can be used to make it do so which makes debugging easier.

Open ex7/lib/submitWithConfiguration.m and replace line:

```matlab
 fprintf('!! Please try again later.\n');

```

(around 28) with:

```matlab
fprintf('Error from file:%s\nFunction:%s\nOn line:%d\n', e.stack(1,1).file,e.stack(1,1).name, e.stack(1,1).line );
```

That top line says '!! Please try again later' on crash, instead of that, the bottom line will give the location and line number of the error. This change can be applied to all the programming assignments.


#### Workaround for problem in plotting routine

{CTA Note: This problem only effects certain versions of Octave} after completion of the computecentroids.m function, i ran into the following problem:

```matlab
    K-Means iteration 1/10...
    error: __scatter__: A(I): index out of bounds; value 4 out of bound 3
    error: called from:
    error:   /Applications/Octave.app/Contents/Resources/share/octave/3.4.0/m/plot/private/__scatter__.m at line 199, column 13
    error:   /Applications/Octave.app/Contents/Resources/share/octave/3.4.0/m/plot/scatter.m at line 71, column 11
    error:  ?/ex7/mlclass-ex7/plotDataPoints.m at line 12, column 1
    error:  ?/ex7/mlclass-ex7/plotProgresskMeans.m at line 11, column 1
    error:  ?/ex7/mlclass-ex7/runkMeans.m at line 48, column 9
    error:  ?/ex7/mlclass-ex7/ex7.m at line 92, column 19
```

i don't think it is caused by my solution, and found a workaround by modifying the plotDataPoints.m as follows

```matlab
   % use idx directly. It will index into the default color map.
   % scatter(X(:,1), X(:,2), 15, colors);
    scatter(X(:,1), X(:,2), 15, idx);
```

The issue is a bug in the scatter() function in certain versions of Octave.


#### `findClosestCentroids()` issue with regards to the grader

If two centroids have identical distances, the submit grader wants you to select the one with the lowest index value. This situation arises when running ex7_pca.m - some of the image pixels have the same minimum distance to more than one centroid. This restriction is most easily accommodated by using the min() function to find the centroid with the minimum distance. Students have found that using the find() function does not result in the answer the grader prefers.


#### Selecting the initial centroids - an additional consideration

This issue was omitted from the lectures. When the initial centroids are selected, be sure that they are each unique. For example, if using K-Means to compress an image, each of the initial centroids should represent a unique color. If two initial centroids were the exact same color, then you would effectively have K-1 centroids, not K.

Using the `kMeansInitCentroids()` method as given in ex7.pdf, an experiment on the "bird_small.mat" data set shows that approximately 5 tries in 10,000 will result in duplicate centroids. The method given in ex7.pdf only selects unique members of the training set as the centroids - it does not verify that they are not duplicate values.

One method for preventing duplicate centroids would be as follows:
+ Randomly select a set of K training examples as the initial centroids.
+ Use the unique(centroids, 'rows') function to get a matrix of all of the unique centroid values.If the number of unique rows is not equal to K, then re-select a new set of initial centroids.
+ Another method would be to prevent any duplicates at all by using the unique function on the training examples (unique(X, 'rows')) __before__ randomly selecting the initial centroids.


#### Fully vectorizing `findClosestCentroids()`

It is possible to fully vectorize this function by using 3D arrays for the training examples and the centroids.

__Tip 1__: To transform 2D arrays to 3D, you can use [permute](http://www.mathworks.com/help/matlab/ref/permute.html) with an extra dimension index. For example, you can transform a m×n (2D) matrix __A2__ to a m×1×n (3D) array A3 using A3 = permute(A2, [1 3 2]);

__Tip 2: Instead of using [repmat](http://www.mathworks.com/help/matlab/ref/repmat.html) to "expand" a matrix for binary operations, it is usually faster to use [bsxfun](http://www.mathworks.com/help/matlab/ref/bsxfun.html).


#### Errata in projectData.m

Make the following change in the "Instructions" section:

```matlab
%      projection_k = x' * U(:, 1:k);
```

The "1:k" portion was missing the "1:" part.




