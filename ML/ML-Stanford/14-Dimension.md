# Dimensionality Reduction

## Motivation

### Motivation I: Data Compression

#### Lecture Notes




#### Lecture Video

<video src="https://d3c33hcgiwev3.cloudfront.net/15.1-DimensionalityReduction-MotivationIDataCompression.71895140b22b11e4901abd97e8288176/full/360p/index.mp4?Expires=1556236800&Signature=dDT6M63OK6TF0xCCnxSwuRXXOu5y~YYVpXzwE5mT-LUsKgDAhwRvCe~FGCKGEiU4oUbSkLAlqFpsNDme50wYcmKT-LudBX6PpgQx8zRHFjLegYp3Cj6xw6fwlfKrt2a2HnqkZQtkRM8fzfowkGKjJZGtq65LzQqzMyiZs4KYBgg_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width="180">
  <track src="https://www.coursera.org/api/subtitleAssetProxy.v1/OVB2xEXhQ2CQdsRF4YNgTQ?expiry=1556236800000&hmac=l09QI63lD5RMtkiULIiwaQN3dWajnl8X6AVItWtKxBU&fileExtension=vtt" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


### Motivation II: Visualization

#### Lecture Notes




#### Lecture Video

<video src="https://d3c33hcgiwev3.cloudfront.net/15.2-DimensionalityReduction-MotivationIIVisualization.3d2f10b0b22b11e4bb7e93e7536260ed/full/360p/index.mp4?Expires=1556236800&Signature=jdxNI9BlSBmnehgnW6Rd~5p51NYBvqS8Yhb8oO0j25BHzQSjFmUWp7sxVI3XV~41ruU3z2t20UMFgAQoI3BCHHxt0dMjmBJ-Gc8xfh8ihjpqowomXCNr0MPuLS~xsPBxS0D2UdFfwM8RoOdcnpfDo2LXMxA~Uz9C~H9WI5rIpVA_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width="180">
  <track src="https://www.coursera.org/api/subtitleAssetProxy.v1/GzkaggVLQ9K5GoIFS1PS1A?expiry=1556236800000&hmac=mScR2j-r5Tj-14ZxHa69tTy3MNIMhKCs6MGqRv3YlSo&fileExtension=vtt" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


## Principal Component Analysis

### Principal Component Analysis Problem Formulation

#### Lecture Notes





#### Lecture Video

<video src="https://d3c33hcgiwev3.cloudfront.net/15.3-DimensionalityReduction-PrincipalComponentAnalysisProblemFormulation.35ff7730b22b11e4901abd97e8288176/full/360p/index.mp4?Expires=1556323200&Signature=MmbPisVri9nroEOC6UJShCtbsKFUDpBiXsKlm-tIr4AAWKdPWocvr2fCXGCcEp~dgUCekpm8w8zl9c~7GqD3cERTY32auz65QH~WPIteVKMyE2gv1WDnTxfzwkNpYlNjmO6JIKmBxTiUhrCbtsl-sV~LPcC0X0uAl6jDo~Z0bag_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width="180">
  <track src="https://www.coursera.org/api/subtitleAssetProxy.v1/uWcaBB1jRk2nGgQdY8ZNpQ?expiry=1556323200000&hmac=OhFcaSVfRqmUv0AHq6bdh4bbbVAOd0vB40te4u8wzZ0&fileExtension=vtt" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


### Principal Component Analysis Algorithm

#### Lecture Notes




#### Lecture Video

<video src="https://d3c33hcgiwev3.cloudfront.net/15.3-DimensionalityReduction-PrincipalComponentAnalysisProblemFormulation.35ff7730b22b11e4901abd97e8288176/full/360p/index.mp4?Expires=1556236800&Signature=bXfhOKHJnsqIIBG4GstC9VdhJgEHFbjj8cEtc6XfZxtKERkYRk47QFqCnzJ5X6Ov5QiEpGm145TGwzOpgiisOD4kfKe7r0TW~oqHb9TFdqWqjjmmvU4QoTopBjx7eBfRYXJDYWQ0gLHsICDaAC9uDTQLyQksDumsYqilCDyXzZc_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width="180">
  <track src="https://www.coursera.org/api/subtitleAssetProxy.v1/uWcaBB1jRk2nGgQdY8ZNpQ?expiry=1556236800000&hmac=K9N5sq-eJXuzMZtBt5Vebq8WmYG0NXo_HX_pRquw6LE&fileExtension=vtt" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


## Applying PCA

### Reconstruction from Compressed Representation

#### Lecture Notes




#### Lecture Video

<video src="https://d3c33hcgiwev3.cloudfront.net/15.6-DimensionalityReduction-ReconstructionFromCompressedRepresentation.c55ea6d0b22b11e4aca907c8d9623f2b/full/360p/index.mp4?Expires=1556409600&Signature=E3SsmeZ80renIC1ysphu45RRfLTnPZB7Afmp9fuw8nlkNREuydv5ekeYunMadMD~wyVTlYlCObmF5cWmgceVyx0r5M3q-Nrs6JyJ8nNS8NPm~n9D-hQnO1GFVCKVbWYgMU4dvKt5ng3xnJ7X5Vsh67p-wt~xp9X9UfpvbeSPot4_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width="180">
  <track src="https://www.coursera.org/api/subtitleAssetProxy.v1/OmTwKw3pR6ik8CsN6Reofg?expiry=1556409600000&hmac=7IwWNIBFSPk5V89npl2LkfgXUARMyvC0NGSCQqw83Zc&fileExtension=vtt" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


### Choosing the Number of Principal Components

#### Lecture Notes




#### Lecture Video

<video src="https://d3c33hcgiwev3.cloudfront.net/15.5-DimensionalityReduction-ChoosingTheNumberOfPrincipalComponents.403f0fd0b22b11e4beb61117ba5cda9e/full/360p/index.mp4?Expires=1556409600&Signature=lMjpI3I6S8LpKdWKX9KobEZwwFCLtQDndT~SBa65~XttozlANDz0fIhTB3AV1eYUc0XEpjjq-YbGPBzAhMwUXdTnfbTJZaFhgcsxGZP5QJgdPDdsnL2r11DRDx~rykCz-N-0ILpInMshslbTDa-uiZOQer6zHelsHMl9Z8rbST8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width="180">
  <track src="https://www.coursera.org/api/subtitleAssetProxy.v1/80G44YfVSM6BuOGH1bjONA?expiry=1556409600000&hmac=iutZmw81OK-Wu_WnFKwOknBPhqtB4Zazm12zUF0zndE&fileExtension=vtt" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


### Advice for Applying PCA

#### Lecture Notes





#### Lecture Video

<video src="url" preload="none" loop="loop" controls="controls" style="margin-left: 2em;" muted="" poster="http://www.multipelife.com/wp-content/uploads/2016/08/video-converter-software.png" width="180">
  <track src="subtitle" kind="captions" srclang="en" label="English" default>
  Your browser does not support the HTML5 video element.
</video><br/>


### Review

### Lecture Slides

#### Dimensionality Reduction

##### Motivation I: Data Compression

+ We may want to reduce the dimension of our features if we have a lot of redundant data.
+ To do this, we find two highly correlated features, plot them, and make a new line that seems to describe both features accurately. We place all the new features on this single line.

Doing dimensionality reduction will reduce the total data we have to store in computer memory and will speed up our learning algorithm.

Note: in dimensionality reduction, we are reducing our features rather than our number of examples. Our variable $m$ will stay the same size; $n$, the number of features each example from $x^{(1)}$ to $x^{(m)}$ carries, will be reduced.


##### Motivation II: Visualization

It is not easy to visualize data that is more than three dimensions. We can reduce the dimensions of our data to 3 or less in order to plot it.

We need to find new features, $z_1, z_2$ (and perhaps $z_3$) that can effectively summarize all the other features.

Example: hundreds of features related to a country's economic system may all be combined into one feature that you call "Economic Activity."


#### Principal Component Analysis Problem Formulation

The most popular dimensionality reduction algorithm is _Principal Component Analysis (PCA)_

##### Problem formulation

Given two features, $x_1$ and $x_2$, we want to find a single line that effectively describes both features at once. We then map our old features onto this new line to get a new single feature.

The same can be done with three features, where we map them to a plane.

The goal of PCA is to reduce the average of all the distances of every feature to the projection line. This is the __projection error__.

Reduce from 2d to 1d: find a direction (a vector $u^{(1)} \ \in \ \mathbb{R}^n$) onto which to project the data so as to minimize the projection error.

The more general case is as follows: 
+ Reduce from $n$-dimension to $k$-dimension: Find $k$ vectors $u^{(1)}, u^{(2)}, \dots, u^{(k)}$ onto which to project the data so as to minimize the projection error.
+ If we are converting from 3d to 2d, we will project our data onto two directions (a plane), so $k$ will be 2.

##### PCA is not linear regression

+ In linear regression, we are minimizing the squared error from every point to our predictor line. These are vertical distances.
+ In PCA, we are minimizing the shortest distance, or shortest orthogonal distances, to our data points.

More generally, in linear regression we are taking all our examples in $x$ and applying the parameters in Θ to predict $y$.

In PCA, we are taking a number of features $x_1, x_2, \dots, x_n$, and finding a closest common dataset among them. We aren't trying to predict any result and we aren't applying any theta weights to the features.


#### Principal Component Analysis Algorithm

Before we can apply PCA, there is a data pre-processing step we must perform:


##### Data preprocessing

+ Given training set: $x(1),x(2),\dots,x(m)$
+ Preprocess (feature scaling/mean normalization):

  $$\mu_j = \dfrac{1}{m}\sum^m_{i=1}x_j^{(i)}$$

+ Replace each $x_j^{(i)}$ with $x_j^{(i)} - \mu_j$

​+ If different features on different scales (e.g., $x_1$ = size of house, $x_2$ = number of bedrooms), scale features to have comparable range of values.

Above, we first subtract the mean of each feature from the original feature. Then we scale all the features $x_j^{(i)} = \dfrac{x_j^{(i)} - \mu_j}{s_j}$

We can define specifically what it means to reduce from 2d to 1d data as follows:

$$\sum = \dfrac{1}{m} \sum_{i=1}^m (x^{(i)})(x^{(i)})^T$$

The $z$ values are all real numbers and are the projections of our features onto $u^{(1)}$.

So, PCA has two tasks: figure out $u^{(1)},\dots,u^{(k)}$ and also to find $z_1, z_2, \dots, z_m$.

The mathematical proof for the following procedure is complicated and beyond the scope of this course.

1. Compute "covariance matrix"

  $$\sum = \dfrac{1}{m} \sum_{i=1}^m (x^{(i)})(x^{(i)})^T$$
 
  This can be vectorized in Octave as:

  ```matlab
  Sigma = (1/m) * X' * X;
  ```

  We denote the covariance matrix with a capital sigma (which happens to be the same symbol for summation, confusingly---they represent entirely different things).

  Note that $x^{(i)}$ is an $n \times 1$ vector, $(x^{(i)})^T$ is an $1 \times n$ vector and $X$ is a $m \times n$ matrix (row-wise stored examples). The product of those will be an n×n matrix, which are the dimensions of $\sum$.

2. Compute "eigenvectors" of covariance matrix $\sum$

  ```matlab
  [U,S,V] = svd(Sigma);
  ```

  `svd()` is the 'singular value decomposition', a built-in Octave function.

  What we actually want out of `svd()` is the 'U' matrix of the Sigma covariance matrix: $U \in \mathbb{R}^{n \times n}$. $U$ contains $u^{(1)},\dots,u^{(n)}$, which is exactly what we want.

3. Take the first $k$ columns of the $U$ matrix and compute $z$

  We'll assign the first $k$ columns of $U$ to a variable called 'Ureduce'. This will be an $n  \times k$ matrix. We compute $z$ with:

  $$z^{(i)} = U_{reduce}^T \cdot x^{(i)}$$

  $U_{reduce}^T$ will have dimensions $k \times n$ while $x^{(i)}$ will have dimensions $n \times 1$. The product $Ureduce^T \cdot x^{(i)}$ will have dimensions $k \times 1$.

To summarize, the whole algorithm in octave is roughly:

```matlab
Sigma = (1/m) * X' * X; % compute the covariance matrix
[U,S,V] = svd(Sigma);   % compute our projected directions
Ureduce = U(:,1:k);     % take the first k directions
Z = X * Ureduce;        % compute the projected data points
```


#### Reconstruction from Compressed Representation

If we use PCA to compress our data, how can we uncompress our data, or go back to our original number of features?

To go from 1-dimension back to 2d we do: $z \in \mathbb{R} \longrightarrow x \;\in\; \mathbb{R}^2$.

We can do this with the equation: $x_{approx}^{(1)} = U_{reduce} \cdot z^{(1)}$.

Note that we can only get approximations of our original data.

Note: It turns out that the $U$ matrix has the special property that it is a Unitary Matrix. One of the special properties of a Unitary Matrix is:

$U^{−1} = U^∗\;$ where the "*" means "conjugate transpose".

Since we are dealing with real numbers here, this is equivalent to:

$U^{-1} = U^T\;$, So we could compute the inverse and use that, but it would be a waste of energy and compute cycles.


#### Choosing the Number of Principal Components

How do we choose $k$, also called the number of principal components? Recall that $k$ is the dimension we are reducing to.

One way to choose $k$ is by using the following formula:

+ Given the average squared projection error: $\dfrac{1}{m}\sum^m_{i=1} \parallel x^{(i)} - x_{approx}^{(i)}\parallel^2$
+ Also given the total variation in the data: $\dfrac{1}{m}\sum^m_{i=1} \parallel x^{(i)} \parallel^2$
+ Choose k to be the smallest value such that: $\dfrac{\dfrac{1}{m}\sum^m_{i=1} \parallel x^{(i)} - x_{approx}^{(i)} \parallel^2}{\dfrac{1}{m}\sum^m_{i=1} \parallel x^{(i)} \parallel^2} \leq 0.01$

In other words, the squared projection error divided by the total variation should be less than one percent, so that __$99\%$ of the variance is retained__.


##### Algorithm for choosing k

1. Try PCA with $k=1,2,\dots$
2. Compute $U_{reduce}, z, x$
3. Check the formula given above that $99\%$ of the variance is retained. If not, go to step one and increase $k$.

This procedure would actually be horribly inefficient. In Octave, we will call svd:

```matlab
[U,S,V] = svd(Sigma)
```

Which gives us a matrix $S$. We can actually check for 99% of retained variance using the $S$ matrix as follows:

$$\dfrac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99$$


##### Advice for Applying PCA

The most common use of PCA is to speed up supervised learning.

Given a training set with a large number of features (e.g. $x(1),\dots,x(m) \;\in\; \mathbb{R}^{10000}$) we can use PCA to reduce the number of features in each example of the training set (e.g. $z(1),\dots,z(m) \;\in\; \mathbb{R}^{1000}$).

Note that we should define the PCA reduction from $x^{(i)}$ to $z^{(i)}$ only on the training set and not on the cross-validation or test sets. You can apply the mapping z(i) to your cross-validation and test sets after it is defined on the training set.

Applications

+ Compressions
  + Reduce space of data
  + Speed up algorithm
+ Visualization of data
  + Choose $k = 2$ or $k = 3$

__Bad use of PCA__: trying to prevent overfitting. We might think that reducing the features with PCA would be an effective way to address overfitting. It might work, but is not recommended because it does not consider the values of our results y. Using just regularization will be at least as effective.

Don't assume you need to do PCA. __Try your full machine learning algorithm without PCA first.__ Then use PCA if you find that you need it.


### Errata

In the video ‘Motivation II: Visualization’, around 2:45, prof. Ng says R2, but writes ℝ. The latter is incorrect and should be R2.

In the video ‘Motivation II: Visualization’, the quiz at 5:00 has a typo where the reduced data set should be go up to $^{(n)}$ rather than $z^{(m)}$.

In the video "Principal Component Analysis Algorithm", around 1:00 the slide should read "Replace each $x_j^{(i)}$ with $x_j^{(i)}-\mu_jx$." (The second $x$ is missing the superscript (i).)

In the video "Principal Component Analysis Algorithm", the formula shown at around 5:00 incorrectly shows summation from 1 to n. The correct summation (shown later in the video) is from 1 to m. In the matrix U shown at around 9:00 incorrectly shows superscript of last column-vector "u" as m, the correct superscript is n.

In the video "Reconstruction from Compressed Representation", the quiz refers to a formula which is defined in the next video, "Choosing the Number of Principal Components"

In the video "Choosing the number of principal components" at 8:45, the summation in the denominator should be from 1 to n (not 1 to m).

In the in-video quiz in "Data Compression" at 9:47 the correct answer contains k≤n but it should be $k<n$.


### Quiz: Principal Component Analysis





