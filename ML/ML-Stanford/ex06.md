# Programming Assignment: Support Vector Machines

### 1 Support Vector Machines

+ gain an intuition of how SVMs work and how to use Gaussian kernel with SVMs

+ `ex6.m` step through the first half of the exercise

### 1.1 Example Dataset 1

+ Starting with a 2D example dataset deparated by a linear boundary

+ plot the training data
  + symbol +: positive examples
  + symbol o: negative examples
  + a natural separation indicated by the gap
  + an outlier with positive example

+ observe how outlier affects the SVM the SVM decision boundary

+ $C$ parameter
  + trying difference $C$ parameter with SVMs
  + a positive value that controls the penalty for misclassified examples
  + large $C$ parameter makes the SVM to try to classify all the examples correctly
  + play a roll similar to $\frac{1}{\lambda}$, $lambda$ is the regulation parameter w/ logistic regression

+ Example Dataset 1

  <div style="display:flex;justify-content:center;align-items:center;flex-flow:row wrap;">
    <div><a href="https://www.coursera.org/learn/machine-learning/programming/e4hZk/support-vector-machines">
      <img src="images/e06-02.png" style="margin: 0.1em;" alt="SVM Decision Boundary with C=1" title="SVM Decision Boundary with C=1" width="350">
      <img src="images/e06-03.png" style="margin: 0.1em;" alt="SVM Decision Boundary with C=100" title="SVM Decision Boundary with C=100" width="350">
    </a></div>
  </div>

  + $C=1$: the SVM puts the decision boundary in the gap between the two datasets and misclassifies the data point on the far left
  + $C=100$: the SVM classifies every single example correctly, but has a decision boundary separating every single point

+ Implementation Note:
  + Most SVM software automatically add the extra feature $x_0 = 1$ and automatically take care of the learning of the intercept term $\theta_0$
  + Octave works with training examples $x \in \mathbb{R}^n$ (rather than $x \in \mathbb{R}^{n+1}$)


### 1.2 SVM with Gaussian Kernels

+ using SVMs to do non-linear classification

+ using SVMs with Gaussian kernels on datasets not linearly separated


#### 1.2.1 Gaussian Kernel

+ implement a Gaussian kernel

+ Gaussian kernel
  + a similarity function that measures the "distance" between a pair of examples, $(x^{(i)}, x^{(j)}$)
  + $\sigma\;$: a bandwidth parameter that determines how fast the similarity metric decreases (to 0) as the examples are further apart

+ complete `gaussianKernel.m` to compute the Gaussian kernel between two examples, $(x^{(i)}, x^{(j)})$

+ The Gaussian function

  $$K_{gaussian}(x^{(i)}, x^{(j)}) = \exp(- \dfrac{\parallel x^{(i)} - x^{(j)} \parallel^2}{2\sigma^2}) = \exp(- \dfrac{\sum_{k=1}^n (x_k^{(i)} - x^{(j)})^2}{2\sigma^2})$$

+ `ex6.m` will test the kernel function and get a value of 0.324652.


#### 1.2.2 Example Dataset 2

+ load and plot dataset 2

+ observe that there is no linear decision boundary that separates the positive and negative examples for the dataset

+ using the Gaussian kernel with the SVM to learn a non-linear decision boundary

+ `ex6.m` train the SVM with the Gaussian kernel on the dataset

+ The decision boundary found by the SVM with a Gaussian kernel able to separate most of the positive and negative examples correctly


#### 1.2.3 Example Dataset 3

+ gain more practical skills on how to use a SVM with a Gaussian kernel

+ `ex6.m` loads and displays a 3rd dataset

+ Dataset `ex6data3.mat` given varians: `X`, `y`, `Xval`, `yval`

+ train the SVM classifier using the training set $(X, y)$ loaded from `dataset3Params.m`

+ Use the cross validation set `Xval`, `yval` to determine the best $C$ an $\sigma$ parameters
  + additional code to search over the parameter $C$ and $\sigma$
  + both $C$ adn $\sigma$ values: 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30
  + try al possible pairs of values for $C$ and $\sigma$

+ After determined the best $C$ and $\sigma$ parameters, modify the code in `datasetParams.m` w/ the best parameters

  <div style="display:flex;justify-content:center;align-items:center;flex-flow:row wrap;">
    <div><a href="https://www.coursera.org/learn/machine-learning/programming/e4hZk/support-vector-machines">
      <img src="images/e06-06.png" style="margin: 0.1em;" alt="Example Dataset 3" title="Example Dataset 3" width="350">
      <img src="images/e06-07.png" style="margin: 0.1em;" alt="SVM (Gaussian Kernel) Decision Boundary" title="SVM (Gaussian Kernel) Decision Boundary" width="350">
    </a></div>
  </div>


### 2 Spam Classification



### 2.1 Preprocessing Emails



### 2.2 Extracting Features from Emails



### 2.3 Training SVM for Spam Classification



### 2.4 Top Predictors for Spam



### 2.5 Try your own emails (optional)



### 2.6 Build your own dataset (optional)



### Programming Ex.6

Keep in mind that all the programming exercise solutions should handle any number of features in the training examples. Passing the test case in the PDF file is not sufficient to be sure of passing the submit grader's test case.

__Debugging Tip__

The submit script, for all the programming assignments, does not report the line number and location of the error when it crashes. The follow method can be used to make it do so which makes debugging easier.

Open ex6/lib/submitWithConfiguration.m and replace line:

```matlab
 fprintf('!! Please try again later.\n');

```

(around 28) with:

```matlab
fprintf('Error from file:%s\nFunction:%s\nOn line:%d\n', e.stack(1,1).file,e.stack(1,1).name, e.stack(1,1).line );
```

That top line says '!! Please try again later' on crash, instead of that, the bottom line will give the location and line number of the error. This change can be applied to all the programming assignments.


#### Update to ex6.m

At line 69/70, change "sigma = 0.5" to "sigma = %0.5f", and change the list of output variables from "sim" to "sigma, sim". This lets the screen output display the actual value of sigma, rather than an (incorrect) constant value.


#### Trouble with the contour plot (visualizeBoundary.m)

Octave 3.8.x and higher
If you have Octave 3.8.x, the ex6 script will not plot decision boundary, and prints 'Unknown hggroup property Color' with stack trace.

One fix is to modify line 21 in visualizeBoundary.m with this code:

```matlab
contour(X1, X2, vals, [1 1], 'linecolor', 'blue');

```

(Note: I tried this and although the error went away, I still don't see any contour line drawn; sokolov 3/22/2015)

I had the same problem with the line not displaying until i changed the [0 0] to [1 1] - tmcarthur 7/1/2016

OR

If you change line 21 to following, it will show two lines and will work with >= 3.8.x .

```matlab
contour(X1, X2, vals);
```

For more information see [here](http://lists.gnu.org/archive/html/octave-bug-tracker/2014-01/msg00226.html)

__Matlab__

In Matlab R2014b and R2015b, simply changing the [0 0] parameter on line 21 in visualizeBoundary.m to [1 1] plots the boundary.


#### processEmail no loop possible

Can use `find()` or `ismember()` for the word vocabulary cell array


#### Understanding SMO and the `svmTrain()` and `svmPredict()` methods

The `svmTrain.m` file is provided with this exercise and it contains an implementation of the Sequential Minimal Optimization (SMO) algorithm to minimize an SVM. You don't need to understand how it works in order to complete the exercise. There are comments in the code that reference numbered equations, but the code doesn't say what document those numbers reference. It turns out to be a section of the course materials from CS 229 at Stanford covering SMO, which can be found [here](http://cs229.stanford.edu/materials/smo.pdf)


#### More SVM explanations

"[An Idiot's Guide to Support Vector Machines](http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf)"


#### Information on SVMLIB

[This exercise](http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex7/ex7.html) uses the SVMLIB package to solve an exercise similar to ex6 (also by Prof Ng).



#### Using LIBSVM in MATLAB/Octave

In the optional section of this exercise, Prof Ng recommended that we use LIBSVM to [solve the problem](http://www.csie.ntu.edu.tw/~cjlin/libsvm/).

Installing LIBSVM on MATLAB/Octave is very easy.

+ After downloading and unzipping the LIBSVM package, open MATLAB/Octave.
+ Go to the directory of the MATLAB/Octave version, e.g. "E:/CourseraML/machine-learning-ex6/ex6/libsvm-3.21/matlab"
+ Enter "make" in the command window.
+ That's it! You're done! Now read the README file in the MATLAB directory, and learn how to use svmtrain and svmpredict function.
+ In short, the syntax of these two functions are: `model = svmtrain( trainingLabelVector, trainingInstanceMatrix [, 'libsvmOptions'])` and `[predictedLabel, accuracy, decisionValues/probEstimates] = svmpredict( testingLabelVector, testingInstanceMatrix, model [, 'libsvmOptions']);`


### Ex6 Tutorials

Here are the ex6 tutorials by Tom Mosher:

__gaussianKernel()__:

The numerator is the sum of the squares of the difference of two vectors. That's similar to how you computed the linear regression cost function in ex1. Then use exp() with scaling based on the 'sigma' parameter, according to the kernel formula.

dataset3Params():

One method is to use two nested for-loops - each one iterating over the range of C or sigma values given in the ex6.pdf file.

Inside the inner loop:

+ Train the model using svmTrain with X, y, a value for C, and the gaussian kernel using a value for sigma. See ex6.m at line 108 for an example of the correct syntax to use in calling svmTrain() and the gaussian kernel. Note: Use temporary variables for C and sigma when you call svmTrain(). Only use 'C' and 'sigma' for the final values you are going to return from the function.
+ Compute the predictions for the validation set using svmPredict() with model and Xval.
+ Compute the error between your predictions and yval.
+ When you find a new minimum error, save the C and sigma values that were used. Or, for each error computation, you can save the C, sigma, and error values as rows of a matrix. When all 64 computations are completed, use min() to find the row with the minimum error, then use that row index to retrieve the C and sigma values that produced the minimum error.
Sample code structure for dataset3Params():

```matlab
results = []   % create an empty results matrix

for C_test = [list of values here]
    for sigma_test = [list of values here]

        % your code goes here to train using C_test and sigma_test
        %    and compute the validation set errors

        % save the results in the matrix
        results = [results ; C_test sigma_test err_value]

    endfor
endfor

% use the min() function to find the best C and sigma values
```

Here is an example of how to find the values in the row of a matrix Q that has the lowest value in column 3:

```matlab
>> Q = rand(6,3)
Q =

   0.3840651   0.0738230   0.7128092
   0.4558660   0.6360802   0.7075968
   0.3822171   0.4430273   0.6626950
   0.0090786   0.1231786   0.4371842
   0.5808400   0.0045790   0.6251304
   0.8679802   0.0193655   0.1613009

>> [v i] = min(Q(:,3))
v =  0.16130
i =  6

>> Q(i,1)
ans =  0.86798

>> Q(i,2)
ans =  0.019366
```

---------------------------------------

__processEmail()__:

To see what variables already exist in processEmail.m, you can set a breakpoint in the processEmail.m script template in the blank area below the "YOUR CODE HERE" comment block. Use the breakpoint tool in the GUI code editor. Then run the ex6_spam script from the console.

When program execution hits the breakpoint, it will activate the console in debug mode. There you can inspect the variables "str" and "vocabList" (type the variable name into the console and press `<Enter>`).

Observe that str holds a single word, and that vocabList is a cell array of all known words. Resume execution with the 'return' command in the debugger.

For the code you need to add:

Here is an example using the `strcmp()` function showing how to find the index of a word in a cell array of words:

```matlab
small_list = {'alpha', 'bravo', 'charlie', 'delta', 'echo'}
match = strcmp('bravo', small_list)
find(match)
```

`strcmp()` returns a logical vector, with a 1 marking each location where a word occurs in the list. If there is no match, all of the logical values are 0's.

The `find()` function returns a list of the non-zero elements in "match". You can add these index values to the word_indices list by concatenation.

Note that if there is no match, `find()` returns a null vector, and this can be concatenated to the word list without any problems.

Note that your word_index list must be returned as a column vector. If you make it a row vector, the submit grader will still give you credit, but your emailFeatures() function will not work correctly.

-------------------------------------

__emailFeatures__:

The `emailFeatures()` function is one of the simplest in the entire course:

+ You're given a list of word indexes.
+ For each index in that list, you're asked to set the corresponding entries in an 'x' array to the value '1'.

A couple of different methods could be used:

+ Loop through the list of word indexes, and use each index to set the corresponding value in the 'x' array to 1.
+ Take advantage of vectorized indexing, and do the same operation in one line of code without the loop.

Note that the 'x' feature list must be a column vector, and the word_indices list (which is provided by your processEmail() function) must be a column vector.

You can complete this function by adding only one line of code.Try this example in your console:

```matlab
vec = zeros(10,1)     % included in the function template
indexes = [1 3 5]     % you're provided with a list of indexes
vec(indexes) = 1      % set the values to 1
```

------------------------------------

How do I install and use LIBSVM?

See [this page](https://www.coursera.org/learn/machine-learning/resources/TEPQT)

#### [Tutorial for emailfeatures()](https://www.coursera.org/learn/machine-learning/programming/e4hZk/support-vector-machines/discussions/threads/unXCdEtOEeWpgBJUo9Z-Uw)

The emailFeature() function is one of the simplest in the entire course:

You're given a list of word indexes.
For each index in that list, you're asked to set the corresponding entries in an 'x' array to the value '1'.
A couple of different methods could be used:

Loop through the list of word indexes, and use each index to set the corresponding value in the 'x' array to 1.
Take advantage of vectorized indexing, and do the same operation in one line of code without the loop.
You can complete this function by adding only one line of code.Try this example in your console:


```matlab
vec = zeros(10,1)     % included in the function template
indexes = [1 3 5]     % you're provided with a list of indexes
vec(indexes) = 1      % set the values to 1
```






