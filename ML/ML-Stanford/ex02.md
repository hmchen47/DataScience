# Programming Assignment: Logistic Regression

[Assignment](http://s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex2.zip)

## Logistic Regression

+ build a logistic regression model to predict whether a student gets admitted into a university.
+ build a classification model that estimates an applicant's probability of admission based the scores from those two exams.

### Visualizing the data

+ load the data and display it on a 2-dimensional plot by calling the function `plotData`.

```matlab
% Find Indices of Positive and Negative Examples
pos = find(y==1); neg = find(y == 0);

% Plot Examples
plot(X(pos, 1), X(pos, 2), 'k+','LineWidth', 2, 'MarkerSize', 7);
plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y', 'MarkerSize', 7);
```


### Implementation

#### 1.2.1 Warmup exercise: sigmoid function

+ logistic regression hypothesis

  $$h_\theta(x) = g(\theta^tx)$$

+ Sigmoid function: $g$

  $$g(z) = \dfrac{1}{1 + e^{-z}}$$}

+ testing w/ $z$ value
  + large positive ($z \rightarrow \infty$): $g(z) \approx 1$
  + large negative ($z \rightarrow -\infty$): $g(z) \approx 0$
  + $z = 0$: $g(z) = 1/2$

+ For a matrix, your function should perform the sigmoid function on every element.


#### 1.2.2 Cost function and gradient

+ Cost function in logistic regression

  $$\begin{array}{rcl} J(\theta) & = & \dfrac{1}{m} \sum_{i=1}^m \left[ -y^{(i)} \log(h_\theta(x^{(i)})) - (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right] \\\\ & = &  \dfrac{1}{m} \left[ -y^T \cdot \log(h_\theta(x)) - (1-y)^T \cdot \log(1 - h_\theta(x)) \right] \\\\ & = & \dfrac{1}{m} \left[ -y^T \cdot \log(g(X\theta)) - (1-y)^T \cdot \log(1 - g(X\theta)) \right] \end{array}$$

+ Gradient of the cost

  $$\begin{array}{rcl} \dfrac{\partial J(\theta)}{\partial \theta_j} & = &\dfrac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x^{(i)} \\\\ & = & \dfrac{1}{m} \left( \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} \begin{bmatrix} h_\theta(x^{(0)}) \\ h_\theta(x^{(2)}) \\ \cdots \\ h_\theta(x^{(m)}) \end{bmatrix} - \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} \begin{bmatrix}  y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix} \right) \\\\ & = & \dfrac{1}{m} \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} (h_\theta(x) - y) = \dfrac{1}{m} \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} (g(X\theta) - y) \end{array}$$

+ this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\theta(x)$.

#### 1.2.3 Learning parameters using fminunc

+ `fminunc`: an optimization solver that finds the minimum of an unconstrained function

+ logistic regression: to optimize the cost function $J(\theta)$ with parameters $\theta$

+ `fminunc` to find the best parameters $\theta$ for the logistic regression cost function, given a fixed dataset (of $X$ and $y$ values)

+ Inputs of `fminunc`
  + The initial values of the parameters
  + A function that, when given the training set and a particular $\theta$, computes the logistic regression cost and gradient with respect to $\theta$ for the dataset $(X, y)$

+ Example code:

  ```matlab
  % Set options for fminunc
  options = optimset('GradObj', 'on', 'MaxIter', 400);

  % Run fminunc to obtain the optimal theta
  % This function will return theta and the cost
  [theta, cost] = ...
  fminunc(@(t)(costFunction(t, X, y)), initial theta, options);
  ```

  + `GradObj` = `on`: function returns both the cost and the gradient; allows `fminunc` to use the gradient when minimizing the function
  + `MaxIter` option to 400: `fminunc` will run for at most 400 steps before it terminates
  + `@(t) (costFunction(t, X, y))`: use a "short-hand"for specifying functions for minimizing; creates a function, with argument `t`, which calls the costFunction.

+ Once fminunc completes, ex2.m will call your costFunction function using the optimal parameters of $\theta$. You should see that the cost is about 0.203.


#### 1.2.4 Evaluating logistic regression

+ For a student with an Exam 1 score of 45 and an Exam 2 score of 85, you should expect to see an admission probability of 0.776.

+ The predict function will produce "1" or "0" predictions given a dataset and a learned parameter vector $\theta$.



## Regularized logistic regression

+ implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA)

+ the test results for some microchips on two different tests

+ determine whether the microchips should be accepted or rejected


### Visualizing the data

+ dataset cannot be separated into positive and negative examples by a straight-line through the plot


### Feature mapping

+ fit the data better is to create more features from each data point

+ map the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power

    $$\text{mapFeature}(x) = \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ x_1^2 \\ x_1x_2 \\ x_2^2 \\ x_1^3 \\ \vdots \\ x_1x_2^5 \\ x_2^6 \end{bmatrix}$$

+ transformed into a 28-dimensional vector

+ A logistic regression classier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear.

+ more susceptible to overtting


### Cost function and gradient

+ Complete the code in `costFunctionReg.m` to return the cost and gradient.

+ Regularized cost function in logistic regression

    $$\begin{array}{rcl} J(\theta) & = & \dfrac{1}{m} \displaystyle \sum_{i=1}^m \left[ -y^{(i)} \log(h_\theta)x^{(i)}) -(1 - y^{(i)}) \log(1 - h_\theta(xx^{(i)})) \right] + \dfrac{\lambda}{2m} \displaystyle \sum_{j=1}^n \theta_j^2 \\\\ & = & \dfrac{1}{m} \left[ -y^T \cdot \log(h) - (1 - y)^T \cdot \log(1 - h) \right] + \dfrac{\lambda}{2m} (\theta^T \theta - \theta_0^2)\end{array}$$

    Note that you should not regularize the parameter 0.

+ The gradient of the cost function

    $$\begin{array}{rcl} \dfrac{\partial J(\theta)}{\partial \theta_0} & = & \dfrac{1}{m} \displaystyle \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\\\ \dfrac{\partial J(\theta)}{\partial \theta_j} & = & \left( \dfrac{1}{m} \displaystyle \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \dfrac{\lambda}{m} \theta_j \qquad j = 1, 2, \ldots, n \\\\  & = & \dfrac{1}{m} \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} (h_\theta(x) - y) + \dfrac{\lambda}{m} \theta_j \\\\  & = &  \dfrac{1}{m} \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} (g(X\theta) - y) + \dfrac{\lambda}{m} \theta_j \end{array}$$

+ costFunctionReg function using the initial value of $\theta$ (initialized to all zeros), the the cost is about 0.693.


### Plotting the decision boundary

+ plot the non-linear decision boundary by computing the classifier's predictions on an evenly spaced grid and then and drew a contour plot of where the predictions change from y = 0 to y = 1.


### Optional exercises

+ changes in the decision boundary as varying $\lambda$
    + small $\lambda$: the classifier gets almost every training example correct, but draws a very complicated boundary, thus overfitting the data
    + large $\lambda$: an simpler decision boundary which still separates the positives and negatives fairly well

+ if $\lambda$ is set to too high a value, you will not get a good t and the decision boundary will not follow the data so well, thus underfitting the data


## Programming Ex.2

Note for MATLAB users: If you are using MATLAB version R2015a or later, the fminunc() function has been changed in this version. The function works better, but does not give the expected result for Figure 5 in ex2.pdf, and it throws some warning messages (about a local minimum) when you run ex2_reg.m. This is normal, and you should still be able to submit your work to the grader.

Typos in the lectures (updated):

There are typos in the week 3 lectures, specifically for regularized logistic regression. This could create some confusion while doing the the last part of exercise 2. The equations in ex2.pdf are correct.

### Gradient and theta values for ex2.m

Here are the values of both cost J and the gradients for the "initial theta (zeros)" test (ex2.pdf Section 1.2.2):

```matlab
Cost at initial theta (zeros): 0.693147
Gradient at initial theta (zeros):
 -0.100000
 -12.009217
 -11.262842
```

Here are the values for both cost J and theta for the "theta found by fminunc" test (ex2.pdf Section 1.2.3):

```matlab
Cost at theta found by fminunc: 0.203498
theta:
 -25.164593
  0.206261
  0.201499
```

### `mapFeature()` discussion:

For two features x1 and x2, mapFunction calculates following terms. $1, x_1, x_2, x_1^2, x_1x_2, x_2^2, x_1^3, x_1^2x_2, x_1x_2^2, x_2^3, x_1^4, x_1^3x_2, x_1^2x_2^2, x_1x_2^3, x_2^4, x_1^5, x_1^4x_2, x_1^3x_2^2, x_1^2x_2^3, x_1x_2^4, x_2^5, x_1^6, x_1^5x_2, x_1^4x_2^2, x_1^3x_2^3, x_1^2x_2^4, x_1x_2^5, x_2^6$.

Not $100\%$ sure about this, so please take this with a grain of salt.

It appears to me that the "mapFeature" vector displayed on page 9 of the ex2.pdf is the transpose of what is intended. Also, it would be more clear if each of the variables carried the (i) superscript denoting the trial

$$mapFeature(x^{(i)}) = \begin{bmatrix}1 \\ x^{(i)}_1 \\ x^{(i)}_2 \\ (x^{(i)}_1)^2 \\ x^{(i)}_1 x^{(i)}_2 \\ (x^{(i)}_2)^2 \\ (x^{(i)}_1)^3 \\ \vdots \\ x^{(i)}_1(x^{(i)}_2)^5 \\ (x^{(i)}_2)^6 \end{bmatrix}^T$$

Of course this assumes exactly two features in the original dataset. I think of this more as "mapTrial" than as "mapFeature" because what we're really doing is mapping the original trials with two features onto a new set of trials with 28 features.

I would not have thought twice about this, had I not gulped hard at the imprecise use of the word "dimensions" in the phase, "a 28-dimensional vector" in the text which follows the expression.

This is how I interpreted it for the homework, and the results were accepted. But if I'm way off base, please delete this wiki entry.

----------------------------------------------------------

I found this Octave expression quite useful for the regularization programming exercise:

```matlab
 ones(size(theta)) - eye(size(theta))

```

---------------------------------------------------------

I found these other Octave expressions which also are quite useful for the regularization programming exercise:

```matlab
theta(2:size(theta))
theta(2:end)
```

---------------------------------------------------------


### plotData.m - color attributes

The plot() attribute "MarkerFaceColor" may not be supported on your version of Octave or MATLAB. You may need to modify it. Use the command "plot help" to see what attributes are supported. (You might just try to replace "MarkerFaceColor" with "MarkerFace", then the plot should work, although you get a warning.)


### Logistic Regression Gradient

[w.r.t.=with respect to]

Don't stumble over terminology - "the partial derivatives of the cost w.r.t. each parameter in theta" are:

$$\frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})$$

I was confused about this and kept trying to return the updated theta values ...

UPDATE (the above was really helpful, thank you for putting it here) As an additional hint: the instructions say: "[...] the gradient of the cost with respect to the parameters" - you're only asked for a gradient, don't overdo it (see above). The fact that you're not given alpha should be a hint in itself. You don't need it. You won't be iterating neither.


### Sigmoid function

1) The sigmoid function accepts only on one parameter named 'z'. This variable 'z' can represent a scalar, vector, or matrix. No other variable names should appear in the sigmoid() function.

2) The implementation of the sigmoid function should use only element-wise operators. The operators needed are addition, element-wise division (the './' operator), and the exp() function.


### Decision Boundary

Thoughts regarding why the equation, $theta_{1} + theta_{2}x_{2} + theta_{3}x_{3}$, is set equal to 0 for determining a decision boundary:

In this exercise, we're solving a __classification__ problem using logistic regression.

+ The hypothesis equation is $h_{\theta}(x) = g(z)$, where $g$ is the sigmoid function $\dfrac{1}{1+e^{-z}}$, and $z = \theta^{T}x$
+ For classification, we usually interpret a hypothesis value $h_{\theta}(x) \geq 0.5$ as predicting class "1"
+ Remember, $h_{\theta}(x) = g(z) = g(\theta^{T}x)$ for logistic regression
+ This means that $g(\theta^{T}x) \geq 0.5$ predicts class "1"
+ The sigmoid function $g(z)$ outputs $ \geq 0.5$ when $z \geq 0$ (look at a graph of the sigmoid function)
+ Remember, $z = \theta^{T}x$
+ So, $\theta^{T}x \geq 0$ predicts class "1"
+ Remember $\theta^{T}x = \theta_{1} + \theta_{2}x_{2} + \theta_{3}x_{3}$ in this example (using 1-indexing)
+ So, $\theta_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} \geq 0$ predicts class "1"
+ The decision boundary lets us see the line that has been learned in order to separate out the $y=0$ vs $y=1$ classes, in this example
This boundary is at $h_{\theta}(x) = 0.5$ (remember, this is the lowest possible value for predicting that a class is "1")
+ So, $\theta_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} = 0$ is the boundary
+ The decision boundary will be a line composed of __any__ (x2,x3) points that make this equation __equal zero__.
+ In order to plot the line along the specific data we have, we arbitrarily decide to use values of $x_{2}$ from our data, by choosing the max and min, and then add/subtract a little bit in order to make the line fit nicely. Think about it, you could continue down the line in the above equation an infinite amount in either direction, and it will still be the line dividing the two classes. However, we only have data that lies around a certain area of this line, so we make sure to only plot the line and data in that region (otherwise it would just be a line and some blank space around it).
+ Solve for $x_{3}$ since we're using $x_{2}$ values (the max & min values +/- 2 in order to make a nice line). --> $x_{3} = \dfrac{-1}{theta_3} * (theta_{2}x_{2} + theta_1)$, as seen in the Octave function.
+ Plugin the two $x_2$ values (stored in plot_x) into the above equation to get the two corresponding $x_3$ values (and store in the plot_y variable).
+ Plot a line using these values -> this will be the decision boundary.
+ Plot the rest of our data on the graph as well, and notice that the line should separate the classes.
+ The above still applies even if you're using higher-order polynomial features, with the note that instead of a decision boundary "line", it will be a decision boundary "polynomial".

Lambda effect over Decision Boundary

<div style="display:flex;justify-content:center;align-items:center;flex-flow:row wrap;">
  <div><a href="https://www.coursera.org/learn/machine-learning/resources/fz4AU">
    <img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/_rNzDIDoEeamDApmnD43Fw_a717680e958f98e9129098388bfac7cb_ML-Ex2-2.5-Lambda-animation.gif?expiry=1554336000000&hmac=2CSTkz1eUhyoxzj17p7JmDSP6c3DkeSFVHgf8lwFSxk" style="margin: 0.1em;" alt="Lambda effect over Decision Boundary" title="Lambda effect over Decision Boundary" width="350">
  </a></div>
</div>


