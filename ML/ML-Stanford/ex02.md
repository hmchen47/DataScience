# Programming Assignment: Logistic Regression

[Assignment](http://s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex2.zip)

## Logistic Regression

+ build a logistic regression model to predict whether a student gets admitted into a university.
+ build a classification model that estimates an applicant's probability of admission based the scores from those two exams.

### Visualizing the data

+ load the data and display it on a 2-dimensional plot by calling the function `plotData`.

```matlab
% Find Indices of Positive and Negative Examples
pos = find(y==1); neg = find(y == 0);

% Plot Examples
plot(X(pos, 1), X(pos, 2), 'k+','LineWidth', 2, 'MarkerSize', 7);
plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y', 'MarkerSize', 7);
```


### Implementation

#### 1.2.1 Warmup exercise: sigmoid function

+ logistic regression hypothesis

  $$h_\theta(x) = g(\theta^tx)$$

+ Sigmoid function: $g$

  $$g(z) = \dfrac{1}{1 + e^{-z}}$$}

+ testing w/ $z$ value
  + large positive ($z \rightarrow \infty$): $g(z) \approx 1$
  + large negative ($z \rightarrow -\infty$): $g(z) \approx 0$
  + $z = 0$: $g(z) = 1/2$

+ For a matrix, your function should perform the sigmoid function on every element.


#### 1.2.2 Cost function and gradient

+ Cost function in logistic regression

  $$\begin{array}{rcl} J(\theta) & = & \dfrac{1}{m} \sum_{i=1}^m \left[ -y^{(i)} \log(h_\theta(x^{(i)})) - (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right] \\\\ & = &  \dfrac{1}{m} \left[ -y^T \cdot \log(h_\theta(x)) - (1-y)^T \cdot \log(1 - h_\theta(x)) \right] \\\\ & = & \dfrac{1}{m} \left[ -y^T \cdot \log(g(X\theta)) - (1-y)^T \cdot \log(1 - g(X\theta)) \right] \end{array}$$

+ Gradient of the cost

  $$\begin{array}{rcl} \dfrac{\partial J(\theta)}{\partial \theta_j} & = &\dfrac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x^{(i)} \\\\ & = & \dfrac{1}{m} \left( \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} \begin{bmatrix} h_\theta(x^{(0)}) \\ h_\theta(x^{(2)}) \\ \cdots \\ h_\theta(x^{(m)}) \end{bmatrix} - \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} \begin{bmatrix}  y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix} \right) \\\\ & = & \dfrac{1}{m} \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} (h_\theta(x) - y) = \dfrac{1}{m} \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} (g(X\theta) - y) \end{array}$$

+ this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\theta(x)$.


## Regularized logistic regression



### Visualizing the data



### Feature mapping



### Cost function and gradient



### Plotting the decision boundary



### Optional exercises




