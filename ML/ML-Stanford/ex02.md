# Programming Assignment: Logistic Regression

[Assignment](http://s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex2.zip)

## Logistic Regression

+ build a logistic regression model to predict whether a student gets admitted into a university.
+ build a classification model that estimates an applicant's probability of admission based the scores from those two exams.

### Visualizing the data

+ load the data and display it on a 2-dimensional plot by calling the function `plotData`.

```matlab
% Find Indices of Positive and Negative Examples
pos = find(y==1); neg = find(y == 0);

% Plot Examples
plot(X(pos, 1), X(pos, 2), 'k+','LineWidth', 2, 'MarkerSize', 7);
plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y', 'MarkerSize', 7);
```


### Implementation

#### 1.2.1 Warmup exercise: sigmoid function

+ logistic regression hypothesis

  $$h_\theta(x) = g(\theta^tx)$$

+ Sigmoid function: $g$

  $$g(z) = \dfrac{1}{1 + e^{-z}}$$}

+ testing w/ $z$ value
  + large positive ($z \rightarrow \infty$): $g(z) \approx 1$
  + large negative ($z \rightarrow -\infty$): $g(z) \approx 0$
  + $z = 0$: $g(z) = 1/2$

+ For a matrix, your function should perform the sigmoid function on every element.


#### 1.2.2 Cost function and gradient

+ Cost function in logistic regression

  $$\begin{array}{rcl} J(\theta) & = & \dfrac{1}{m} \sum_{i=1}^m \left[ -y^{(i)} \log(h_\theta(x^{(i)})) - (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right] \\\\ & = &  \dfrac{1}{m} \left[ -y^T \cdot \log(h_\theta(x)) - (1-y)^T \cdot \log(1 - h_\theta(x)) \right] \\\\ & = & \dfrac{1}{m} \left[ -y^T \cdot \log(g(X\theta)) - (1-y)^T \cdot \log(1 - g(X\theta)) \right] \end{array}$$

+ Gradient of the cost

  $$\begin{array}{rcl} \dfrac{\partial J(\theta)}{\partial \theta_j} & = &\dfrac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x^{(i)} \\\\ & = & \dfrac{1}{m} \left( \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} \begin{bmatrix} h_\theta(x^{(0)}) \\ h_\theta(x^{(2)}) \\ \cdots \\ h_\theta(x^{(m)}) \end{bmatrix} - \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} \begin{bmatrix}  y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix} \right) \\\\ & = & \dfrac{1}{m} \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} (h_\theta(x) - y) = \dfrac{1}{m} \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} (g(X\theta) - y) \end{array}$$

+ this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\theta(x)$.

### 1.2.3 Learning parameters using fminunc

+ `fminunc`: an optimization solver that finds the minimum of an unconstrained function

+ logistic regression: to optimize the cost function $J(\theta)$ with parameters $\theta$

+ `fminunc` to find the best parameters $\theta$ for the logistic regression cost function, given a fixed dataset (of $X$ and $y$ values)

+ Inputs of `fminunc`
  + The initial values of the parameters
  + A function that, when given the training set and a particular $\theta$, computes the logistic regression cost and gradient with respect to $\theta$ for the dataset $(X, y)$

+ Example code:

  ```matlab
  % Set options for fminunc
  options = optimset('GradObj', 'on', 'MaxIter', 400);

  % Run fminunc to obtain the optimal theta
  % This function will return theta and the cost
  [theta, cost] = ...
  fminunc(@(t)(costFunction(t, X, y)), initial theta, options);
  ```

  + `GradObj` = `on`: function returns both the cost and the gradient; allows `fminunc` to use the gradient when minimizing the function
  + `MaxIter` option to 400: `fminunc` will run for at most 400 steps before it terminates
  + `@(t) (costFunction(t, X, y))`: use a "short-hand"for specifying functions for minimizing; creates a function, with argument `t`, which calls the costFunction.

+ Once fminunc completes, ex2.m will call your costFunction function using the optimal parameters of $\theta$. You should see that the cost is about 0.203.


#### 1.2.4 Evaluating logistic regression

+ For a student with an Exam 1 score of 45 and an Exam 2 score of 85, you should expect to see an admission probability of 0.776.

+ The predict function will produce "1" or "0" predictions given a dataset and a learned parameter vector $\theta$.



## Regularized logistic regression

+ implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA)

+ the test results for some microchips on two different tests

+ determine whether the microchips should be accepted or rejected


### Visualizing the data

+ dataset cannot be separated into positive and negative examples by a straight-line through the plot


### Feature mapping

+ fit the data better is to create more features from each data point

+ map the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power

    $$\text{mapFeature}(x) = \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ x_1^2 \\ x_1x_2 \\ x_2^2 \\ x_1^3 \\ \vdots \\ x_1x_2^5 \\ x_2^6 \end{bmatrix}$$

+ transformed into a 28-dimensional vector

+ A logistic regression classier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear.

+ more susceptible to overtting


### Cost function and gradient

+ Complete the code in `costFunctionReg.m` to return the cost and gradient.

+ Regularized cost function in logistic regression

    $$\begin{array}{rcl} J(\theta) & = & \dfrac{1}{m} \displaystyle \sum_{i=1}^m \left[ -y^{(i)} \log(h_\theta)x^{(i)}) -(1 - y^{(i)}) \log(1 - h_\theta(xx^{(i)})) \right] + \dfrac{\lambda}{2m} \displaystyle \sum_{j=1}^n \theta_j^2 \\\\ & = & \dfrac{1}{m} \left[ -y^T \cdot \log(h) - (1 - y)^T \cdot \log(1 - h) \right] + \dfrac{\lambda}{2m} (\theta^T \theta - \theta_0^2)\end{array}$$

    Note that you should not regularize the parameter 0.

+ The gradient of the cost function

    $$\begin{array}{rcl} \dfrac{\partial J(\theta)}{\partial \theta_0} & = & \dfrac{1}{m} \displaystyle \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\\\ \dfrac{\partial J(\theta)}{\partial \theta_j} & = & \left( \dfrac{1}{m} \displaystyle \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \dfrac{\lambda}{m} \theta_j \qquad j = 1, 2, \ldots, n \\\\  & = & \dfrac{1}{m} \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} (h_\theta(x) - y) + \dfrac{\lambda}{m} \theta_j \\\\  & = &  \dfrac{1}{m} \begin{bmatrix} x^{(1)}_j & x^{(2)}_j & \cdots & x^{(m)}_j \end{bmatrix} (g(X\theta) - y) + \dfrac{\lambda}{m} \theta_j \end{array}$$

+ costFunctionReg function using the initial value of $\theta$ (initialized to all zeros), the the cost is about 0.693.


### Plotting the decision boundary



### Optional exercises




