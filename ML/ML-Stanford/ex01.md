# Programming Exercise 1: Linear Regression

[Program Assignment](https://s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex1.zip)

## Simple Octave/MATLAB function

The first part of ex1.m gives you practice with Octave/MATLAB syntax and the homework submission process. In the le warmUpExercise.m, you will nd the outline of an Octave/MATLAB function. Modify it to return a 5 x 5 identity matrix by filling in the following code:

```matlab
A = eye(5);
```

After completing a part of the exercise, you can submit your solutions for grading by typing submit at the Octave/MATLAB command line.


## Linear regression with one variable

The file `ex1data1.txt` contains the dataset for our linear regression problem. The rst column is the population of a city and the second column is the prot of a food truck in that city. A negative value for prot indicates a loss.

### Plotting the Data

In `ex1.m` the dataset is loaded from the data le into the variables $X$ and $y$:

```matlab
data = load('ex1data1.txt'); % read comma separated data
X = data(:, 1); y = data(:, 2);
m = length(y); % number of training examples
```

Next, the script calls the `plotData` function to create a scatter plot of the data. Your job is to complete `plotData.m` to draw the plot; modify the fille and fill in the following code:

```matlab
plot(x, y, 'rx', 'MarkerSize', 10); % Plot the data
ylabel('Profit in $10,000s'); % Set the y􀀀axis label
xlabel('Population of City in 10,000s'); % Set the x􀀀axis label
```

### Gradient Descent

#### Update Equations (Derivation)

Dataset: $m$ samples and $n$ features

Cost function:

$$J(\theta) = \dfrac{1}{2m} \sum_{i=1}^m (h_\theta (x^{(i)}) - y^{(i)})^2$$

Hypothesis $h_\theta(x)$ is given by the linear model

$$h_\theta (x) = \theta^T x = \theta_0 + \theta_1 x_1$$

Objective:

$$\displaystyle \min_\theta J(\theta)$$


Batch gradient descent for each iteration

$$\theta_j := \theta_j - \alpha \dfrac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$
<span style="text-align: center; padding-top: 0.5em;padding-left: calc(50vw - 5em);"> (simultaneously update </span>
$\; \theta_j, \;\; \forall j$)<br/>


Notations:

$$X = \begin{bmatrix} x_0^{(1)} & x_1^{(1)} & \cdots & x_n^{(1)} \\
x_0^{(2)} & x_1^{(2)} & \cdots & x_n^{(2)} \\
\vdots & \vdots & \ddots & \vdots \\
x_0^{(m)} & x_1^{(m)} & \cdots & x_n^{(m)}
\end{bmatrix} = \begin{bmatrix} x^{(1)} \\ x^{(2)} \\ \vdots \\ x^{(m)} \end{bmatrix}\quad\quad\quad
\vec{y} = \begin{bmatrix} y^{(0)} \\ y^{(2)} \\ \vdots \\ y^{(m)}  \end{bmatrix} \quad\quad\quad
\vec{\theta} = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_m \end{bmatrix}
$$
<br/>

Vectorization of Hypothesis Function

$$\begin{array}{rcl}
h_\theta (x^{(i)}) & = & \theta_0 x_0^{(i)} + \theta_1 x_1^{(i)} + \cdots + \theta_n x_n^{(i)} \\\\
h_\theta (X) & = & \begin{bmatrix} h_\theta(x^{(1)}) \\ h_\theta(x^{(2)}) \\ \vdots \\ h_\theta(x^{(m)}) \end{bmatrix} = 
\begin{bmatrix} \theta_0 x_0^{(1)} + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} \\ \theta_0 x_0^{(2)} + \theta_1 x_1^{(2)} + \cdots + \theta_n x_n^{(2)} \\ \vdots \\ \theta_0 x_0^{(m)} + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} \end{bmatrix} = \begin{bmatrix} x_0^{(1)} & x_1^{(1)} & \cdots & x_n^{(1)} \\ x_0^{(1)} & x_1^{(2)} & \cdots & x_n^{(2)}  \\ \vdots & \vdots & \ddots & \vdots \\ x_0^{(m)} & x_1^{(m)} & \cdots & x_n^{(m)} \end{bmatrix} 
\begin{bmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{bmatrix} = X \vec{\theta}
\end{array}$$

Vectorization for Cost function

$$\begin{array}{rcl}
J(\vec{\theta}) & = & \dfrac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 \\\\ & = & \dfrac{1}{2m} \begin{bmatrix} h_\theta(x^{(1)}) - y^{(1)} & h_\theta(x^{(2)}) - y^{(2)} & \cdots & h_\theta(x^{(m)}) - y^{(m)} \end{bmatrix}  \begin{bmatrix} h_\theta(x^{(1)}) - y^{(1)} \\ h_\theta(x^{(2)}) - y^{(2)} \\ \vdots \\ h_\theta(x^{(m)}) - y^{(m)} \end{bmatrix} \\\\ & = & \dfrac{1}{2m} \begin{bmatrix} h_\theta(x^{(1)}) - y^{(1)} \\ h_\theta(x^{(2)}) - y^{(2)} \\ \vdots \\ h_\theta(x^{(m)}) - y^{(m)} \end{bmatrix}^T \begin{bmatrix} h_\theta(x^{(1)}) - y^{(1)} \\ h_\theta(x^{(2)}) - y^{(2)} \\ \vdots \\ h_\theta(x^{(m)}) - y^{(m)} \end{bmatrix} \\ \\ & = & \dfrac{1}{2m} \left( \begin{bmatrix} h_\theta(x^{(1)}) \\ h_\theta(x^{(2)}) \\ \vdots \\ h_\theta(x^{(m)})\end{bmatrix} - \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix} \right)^T \left( \begin{bmatrix} h_\theta(x^{(1)}) \\ h_\theta(x^{(2)}) \\ \vdots \\ h_\theta(x^{(m)})\end{bmatrix} - \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix} \right) \\\\ & = & \dfrac{1}{2m} (X\vec{\theta} - \vec{y})^T (X\vec{\theta} - \vec{y}) \end{array}$$


Vectorization for Batch Gradient Descent

$$\begin{array}{rcl} \vec{\theta} & := & \vec{\theta} - \alpha \dfrac{1}{m} \begin{bmatrix} \sum_{i=1}^m (h_\theta (x^{(i)} - y^{(i)}) x_0^{(i)} \\\\ \sum_{i=1}^m (h_\theta (x^{(i)} - y^{(i)}) x_1^{(i)} \\ \vdots \\ \sum_{i=1}^m (h_\theta (x^{(m)} - y^{(m)}) x_1^{(m)} \end{bmatrix} = \vec{\theta} - \dfrac{\alpha}{m} \begin{bmatrix} \sum_{i=1}^m h_\theta (x^{(i)} \cdot x_0^{(i)} - \sum_{i=1}^m y^{(i)}) \cdot x_0^{(i)} \\\\  \sum_{i=1}^m h_\theta (x^{(i)} \cdot x_1^{(i)} - \sum_{i=1}^m y^{(i)}) \cdot x_1^{(i)} \\ \vdots \\ \sum_{i=1}^m h_\theta (x^{(i)} \cdot x_m^{(i)} - \sum_{i=1}^m y^{(i)}) \cdot x_m^{(i)} \end{bmatrix} \\\\ & = & \vec{\theta} - \dfrac{\alpha}{m} \left( \underbrace{\begin{bmatrix} \sum_{i=1}^m h_\theta (x^{(i)} \cdot x_0^{(i)} \\\\ \sum_{i=1}^m h_\theta (x^{(i)} \cdot x_1^{(i)} \\ \vdots \\ \sum_{i=1}^m h_\theta (x^{(i)} \cdot x_m^{(i)}  \end{bmatrix}}_{(A)} - \underbrace{\begin{bmatrix} \sum_{i=1}^m y^{(i)}) \cdot x_0^{(i)} \\\\ \sum_{i=1}^m y^{(i)}) \cdot x_1^{(i)} \\ vdots \\ \sum_{i=1}^m y^{(i)}) \cdot x_m^{(i)}  \end{bmatrix}}_{(B)} \right)
\end{array}$$

Part (A) with $j$

$$\begin{array}{rcl} \displaystyle \sum_{i=1}^m h_\theta(x^{(i)}) \cdot x_j^{(i)} & = &h_\theta(x^{(1)}) x_0^{(1)} + h_\theta(x^{(2)}) x_j^{(2)} + \cdots + h_\theta(x^{(n)}) x_j^{(n)} \\\\ & = & \begin{bmatrix} x_j^{(1)} & x_j^{(2)} & \cdots & x_j^{(m)}\end{bmatrix} \begin{bmatrix} h_\theta(x^{(1)}) \\ h_\theta(x^{(2)}) \\ \vdots \\ h_\theta(x^{(m)}) \end{bmatrix}  \\ & = & \begin{bmatrix} x_j^{(1)} & x_j^{(2)} & \cdots & x_j^{(m)}\end{bmatrix} h_\theta(X) = \begin{bmatrix} x_j^{(1)} & x_j^{(2)} & \cdots & x_j^{(m)} \end{bmatrix} X\vec{\theta} \end{array}$$

Part (A) $\;\forall j$

$$\begin{bmatrix} \sum_{i=1}^m h_\theta (x^{(i)} \cdot x_0^{(i)} \\\\ \sum_{i=1}^m h_\theta (x^{(i)} \cdot x_1^{(i)} \\ \vdots \\ \sum_{i=1}^m h_\theta (x^{(i)} \cdot x_m^{(i)}  \end{bmatrix} = \begin{bmatrix} x_0^{(1)} & x_0^{(2)} & \cdots & x_0^{(m)} \\ x_1^{(1)} & x_1^{(2)} & \cdots & x_1^{(m)} \\ \vdots & \vdots & \ddots & \vdots \\ x_j^{(1)} & x_j^{(2)} & \cdots & x_j^{(m)} \end{bmatrix} h_\theta(X) = X^TX\vec{\theta}$$

Part (B) for $j$

$$\begin{array}{rcl} \sum_{i=1}^m y^{(i)} x_j^{(i)} & = & x_j^{(1)} y^{(1)} + x_j^{(2)} y^{(2)} + \cdots + x_j^{(m)} y^{(m)} \\\\ & = & \begin{bmatrix} x_j^{(1)} & x_j^{(2)} & \cdots & x_j^{(m)} \end{bmatrix} \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix} = \begin{bmatrix} x_j^{(1)} & x_j^{(2)} & \cdots & x_j^{(m)} \end{bmatrix} \vec{y}
\end{array}$$


Part (B) $\;\forall j$

$$\begin{bmatrix} \sum_{i=1}^m y^{(i)}) \cdot x_0^{(i)} \\\\ \sum_{i=1}^m y^{(i)}) \cdot x_1^{(i)} \\ \vdots \\ \sum_{i=1}^m y^{(i)}) \cdot x_m^{(i)}  \end{bmatrix} = \begin{bmatrix} x_0^{(1)} & x_0^{(2)} & \cdots & x_0^{(m)} \\ x_1^{(1)} & x_1^{(2)} & \cdots & x_1^{(m)} \\ \vdots & \vdots & \ddots & \vdots \\ x_j^{(1)} & x_j^{(2)} & \cdots & x_j^{(m)} \end{bmatrix} \vec{y} = X^T \vec{y}$$

Therefore, the batch gradient descent iteration can be obtained with

$$\vec{\theta} := \vec{\theta} - \alpha \dfrac{1}{m} \begin{bmatrix} \sum_{i=1}^m (h_\theta (x^{(i)} - y^{(i)}) x_0^{(i)} \\\\ \sum_{i=1}^m (h_\theta (x^{(i)} - y^{(i)}) x_1^{(i)} \\ \vdots \\ \sum_{i=1}^m (h_\theta (x^{(m)} - y^{(m)}) x_1^{(m)} \end{bmatrix} = \vec{\theta} - \dfrac{\alpha}{m} (X^TX\vec{\theta} - X^T\vec{Y}) = \vec{\theta} - \dfrac{\alpha}{m} X^T (X\vec{\theta} - \vec{y})$$

#### Implementation

+ Add another dimension to our data to accommodate the $\theta_0$ intercept term
+ Initialize the initial parameters to $\theta$ and the learning rate alpha to $0.01$.

    ```matlab
    X = [ones(m, 1), data(:,1)]; % Add a column of ones to x
    theta = zeros(2, 1); % initialize fitting parameters

    iterations = 1500;
    alpha = 0.01;
    ```

#### Computing the cost $J(\vec{\theta})$

+ implement a function to calculate $J(\vec{\theta})$
+ complete the code in the file `computeCost.m`, which is a function that computes $J(\vec{\theta})$
+ expect to see a cost of `32.07`

    ```matlab
    J = (X * theta - y)' * (X * theta - y) / (2*m);
    ```

#### Gradient descent

+ implement gradient descent in the fille `gradientDescent.m`
+ supply the updates to $\theta$ within each iteration
+ the cost $J(\vec{\theta})$ is parameterized by the vector $\vec{\theta}$, not $X$ and $\vec{y}$. i.e., minimize the value of  $J(\vec{\theta})$ by changing the values of the vector $\vec{\theta}$, not by changing $X$ or $\vec{y}$
+ to verify that gradient descent is working correctly is to look at the value of $J(\vec{\theta})$ and check that it is decreasing with each step
+ final values for $\vec{\theta}$ will also be used to make predictions on prots in areas of 35,000 and 70,000 people

    ```matlab
    for iter = 1:num_iters

        % ====================== YOUR CODE HERE ======================
        % Instructions: Perform a single gradient step on the parameter vector
        %               theta. 
        %
        % Hint: While debugging, it can be useful to print out the values
        %       of the cost function (computeCost) and gradient here.
        %

        theta = theta - alpha/m * X' * (X * theta - y)

        % ============================================================

        % Save the cost J in every iteration    
        J_history(iter) = computeCost(X, y, theta);

    end
    ```

    ```matlab
    predict1 = [1, 3.5] * theta;
    predict2 = [1, 7] * theta;
    ```


### Debugging

gradient descent:
+ Octave/MATLAB array indices start from one, not zero.
+ inspect your matrix operations to make sure that you're adding and multiplying matrices of compatible dimensions
+ By default, Octave/MATLAB interprets math operators to be matrix operators.
+ Element-wise operation: add the "dot" notation to specify this to Octave/MATLAB.


## Visualizing $J(\vec{\theta})$

+ plot the cost over a 2-dimensional grid of $\theta_0$ and $\theta_1$ values.
+ calculate $J(\vec{\theta})$ over a grid of values using the computeCost function

    ```matlab
    % initialize J vals to a matrix of 0's
    J vals = zeros(length(theta0 vals), length(theta1 vals));

    % Fill out J vals
    for i = 1:length(theta0 vals)
        for j = 1:length(theta1 vals)
            t = [theta0 vals(i); theta1 vals(j)];
            J vals(i,j) = computeCost(x, y, t);
        end
    end
    ```


## Linear regression with multiple variables

+ implement linear regression with multiple variables to predict the prices of houses
+ file `ex1data2.txt` contains a training set of housing prices in Portland, Oregon
    + the rst column is the size of the house (in square feet)
    + the second column is the number of bedrooms
    + the third column is the price of the house
+ The `ex1 multi.m` script has been set up to help you step through this exercise


### Feature Normalization

+ complete the code in featureNormalize.m to
    + Subtract the mean value of each feature from the dataset.
    + After subtracting the mean, additionally scale (divide) the feature values by their respective \standard deviations."
+ In Octave/MATLAB, you can use the `std` function to compute the standard deviation.
+ inside `featureNormalize.m`, the quantity `X(:,1)` contains all the values of x1 (house sizes) in the training set, so `std(X(:,1))` computes the standard deviation of the house sizes. 
+ At the time that `featureNormalize.m` is called, the extra column of 1's corresponding to `x0 = 1` has not yet been added to `X` (see ex1 multi.m for details).
+ Implementation Note:
    + When normalizing the features, it is important to store the values used for normalization - the mean value and the standard deviation used for the computations.
    + Given a new $x$ value (living room area and number of bedrooms), we must first normalize $x$ using the mean and standard deviation that we had previously computed from the training set.
    + After learning the parameters from the model, we often want to predict the prices of houses we have not seen before.

    ```matlab
    [m, n] = size(X);

    mu = mean(X);
    sigma = std(X);

    for iter = 1:m,
        X_norm(iter, :) = (X_norm(iter, :) - mu) ./ sigma;
    end;

    % Estimate the price of a 1650 sq-ft, 3 br house
    % ====================== YOUR CODE HERE ======================
    % Recall that the first column of X is all-ones. Thus, it does
    % not need to be normalized.

    price = [1, ([1650 3] - mu) ./ sigma] * theta; % You should change this
    ```


### Gradient Descent

Cost function in vectorized form

$$J(\theta) = \dfrac{1}{2m} (X\theta - \vec{y})^T(X\theta - \vec{y})$$

<br/>

$$\quad\quad X = \begin{bmatrix} --- & (x^{(1)})^T & --- \\ --- & (x^{(2)})^T & --- \\ & \vdots & \\ --- & (x^{(m)})^T & --- \end{bmatrix} \quad\quad \vec{y} = \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix}$$

#### Selecting learning rate

+ change the learning rate by modifying `ex1 multi.m` and changing the part of the code that sets the learning rate
+ call `gradientDescent.m` function and run gradient descent for about 50 iterations at the chosen learning rate
+ return the history of J() values in a vector $J$
+ plots the J values against the number of the iterations
+ trying values of the learning rate $\alpha$ on a log-scale, at multiplicative steps of about 3 times the previous value (i.e., 0.3, 0.1, 0.03, 0.01 and so on).


### Normal Equations

+ Close-form solution for Linear Regression

    $$\theta = (X^TX)^{-1} X^T\vec{y}$$

+ Complete the code in normalEqn.m to use the formula above to calculate $\theta$.

```matlab
theta = pinv((X' * X)) * X' * y;

% Estimate the price of a 1650 sq-ft, 3 br house
% ====================== YOUR CODE HERE ======================
price = [1 1650 3] * theta; % You should change this
```

## Programming Ex.1

### Tutorials

__Compute Cost Tutorial__

This is a step-by-step tutorial for how to complete the computeCost() function portion of ex1. You will still have to do some thinking, because I'll describe the implementation, but you have to turn it into Octave script commands. All the programming exercises in this course follow the same procedure; you are provided a starter code template for a function that you need to complete. You never have to start a new script file from scratch. This is a vectorized implementation. You're only going to write a few simple lines of code.

With a text editor (NOT a word processor), open up the computeCost.m file. Scroll down until you find the "====== YOUR CODE HERE =====" section. Below this section is where you're going to add your lines of code. Just skip over the lines that start with the '%' sign - those are instructive comments.

We'll write these three lines of code by inspecting the equation on Page 5 of ex1.pdf. The first line of code will compute a vector 'h' containing all of the hypothesis values - one for each training example (i.e. for each row of X). The hypothesis (also called the prediction) is simply the product of X and theta. So your first line of code is...

```matlab
h = {multiply X and theta, in the proper order that the ....inner dimensions match}
```

Since X is size ($m \times n$) and theta is size ($n \times 1$), you arrange the order of operators so the result is size ($m \times 1$).

The second line of code will compute the difference between the hypothesis and y - that's the error for each training example. Difference means subtract.

```matlab
error = {the difference between h and y}
```

The third line of code will compute the square of each of those error terms (using element-wise exponentiation),

An example of using element-wise exponentiation - try this in your workspace command line so you see how it works.

```matlab
v = [-2 3]
v_sqr = v.^2
```

So, now you should compute the squares of the error terms:

```matlab
error_sqr = {use what you have learned}
```

Next, here's an example of how the sum function works (try this from your command line)


Now, we'll finish the last two steps all in one line of code. You need to compute the sum of the error_sqr vector, and scale the result (multiply) by `1/(2*m)`. That completed sum is the cost value $J$.

```matlab
J = {multiply 1/(2*m) times the sum of the error_sqr vector}
```

That's it. If you run the `ex1.m` script, you should have the correct value for $J$. Then you should run one of the unit tests (available in the Forum).

Then you can run the submit script, and hopefully it will pass.

Be sure that every line of code ends with a semicolon. That will suppress the output of any values to the workspace. Leaving out the semicolons will surely make the grader unhappy.

__Gradient Descent Tutorial__ - also applies to `gradientDescentMulti()` - includes test cases.

I use the vectorized method, hopefully you're comfortable with vector math. Using this method means you don't have to fuss with array indices, and your solution will automatically work for any number of features or training examples.

What follows is a vectorized implementation of the gradient descent equation on the bottom of Page 5 in ex1.pdf.

Reminder that 'm' is the number of training examples (the rows of X), and 'n' is the number of features (the columns of X). 'n' is also the size of the theta vector ($n \times 1$).

Perform all of these steps within the provided for-loop from 1 to the number of iterations. Note that the code template provides you this for-loop - you only have to complete the body of the for-loop. The steps below go immediately below where the script template says "======= YOUR CODE HERE ======".

1) The hypothesis is a vector, formed by multiplying the X matrix and the theta vector. $X$ has size ($m \times n$), and theta is ($n \times 1$), so the product is ($m \times 1$). That's good, because it's the same size as 'y'. Call this hypothesis vector 'h'.
2) The "errors vector" is the difference between the 'h' vector and the 'y' vector.
3) The change in theta (the "gradient") is the sum of the product of X and the "errors vector", scaled by alpha and `1/m`. Since X is ($m \times n$), and the error vector is ($m \times 1$), and the result you want is the same size as theta (which is ($n \times 1$), you need to transpose X before you can multiply it by the error vector.

    The vector multiplication automatically includes calculating the sum of the products.

    When you're scaling by alpha and 1/m, be sure you use enough sets of parenthesis to get the factors correct.

4) Subtract this "change in theta" from the original value of theta. A line of code like this will do it:

    ```matlab
    theta = theta - theta_change;
    ```

That's it. Since you're never indexing by m or n, this solution works identically for both `gradientDescent()` and `gradientDescentMulti()`.

__Feature Normalization Tutorial__

There are a couple of methods to accomplish this. The method here is one I use that doesn't rely on automatic broadcasting or the `bsxfun()` or `repmat()` functions.

You can use the `mean()` and `sigma()` functions to get the mean and std deviation for each column of X. These are returned as row vectors (1 x n)

Now you want to apply those values to each element in every row of the $X$ matrix. One way to do this is to duplicate these vectors for each row in $X$, so they're the same size.

One method to do this is to create a column vector of all-ones - size ($m \times 1$) - and multiply it by the mu or sigma row vector ($1 \times n$). Dimensionally, $(m \times 1) * (1 \times n)$ gives you a $(m \times n)$ matrix, and every row of the resulting matrix will be identical.

Now that X, mu, and sigma are all the same size, you can use element-wise operators to compute X_normalized.

Try these commands in your workspace:

```matlab
% creates a test matrix
mu = mean(X)

% returns a row vector
sigma = std(X)

% returns a row vector
m = size(X, 1)

% returns the number of rows in X
mu_matrix = ones(m, 1) * mu

sigma_matrix = ones(m, 1) * sigma
```

Now you can subtract the mu matrix from X, and divide element-wise by the sigma matrix, and arrive at X_normalized.

You can do this even easier if you're using a Matlab or Octave version that supports automatic broadcasting - then you can skip the "multiply by a column of 1's" part.

You can also use the `bsxfun()` or `repmat()` functions. Be advised the `bsxfun()` has a non-obvious syntax that I can never remember, and `repmat()` runs rather slowly.

### Test Cases

computeCost:

```matlab
computeCost( [1 2; 1 3; 1 4; 1 5], [7;6;5;4], [0.1;0.2] )
% ans = 11.9450

computeCost( [1 2 3; 1 3 4; 1 4 5; 1 5 6], [7;6;5;4], [0.1;0.2;0.3])
% ans = 7.0175
```

gradientDescent:

Test Case 1:

```matlab
theta J_hist] = gradientDescent([1 5; 1 2; 1 4; 1 5],[1 6 4 2]',[0 0]',0.01,1000);
% then type in these variable names, to display the final results

theta
% theta =
%  5.2148
%  -0.5733

J_hist(1)
% ans = 5.9794

J_hist(1000)
% ans = 0.85426
```

For debugging, here are the first few theta values computed in the `gradientDescent()` for-loop for this test case:

```matlab
first iteration
theta =
  0.032500
  0.107500
% second iteration
theta =
  0.060375
  0.194887
% third iteration
theta =
  0.084476
  0.265867
% fourth iteration
theta =
  0.10550
  0.32346
```

The values can be inspected by adding the "keyboard" command within your for-loop. This exits the code to the debugger, where you can inspect the values. Use the "return" command to resume execution.

Test Case 2:

This test case is similar, but uses a non-zero initial theta value.

```matlab
theta J_hist] = gradientDescent([1 5; 1 2],[1 6]',[.5 .5]',0.1,10);
>> theta
theta =
1.70986  
0.19229
>> J_hist
J_hist =
  5.8853  
  5.7139  
  5.5475  
  5.3861  
  5.2294  
  5.0773  
  4.9295  
  4.7861  
  4.6469  
  4.5117
```

`featureNormalize()`:

```matlab
 = featureNormalize([1 ; 2 ; 3])
% result
Xn =
  -1  
  0  
  1
mu =  2
sigma =  1
[Xn mu sigma] = featureNormalize(magic(3))
% result
Xn =   
  1.13389 -1.00000 0.37796  
  -0.75593 0.00000 0.75593 
  -0.37796 1.00000 -1.13389
mu =
  5   5   5
sigma =
  2.6458   4.0000   2.6458
%--------------
[Xn mu sigma] = featureNormalize([-ones(1,3); magic(3)])
% results
Xn =  
  -1.21725  -1.01472  -1.21725
  1.21725  -0.56373   0.67625
  -0.13525   0.33824   0.94675
  0.13525   1.24022  -0.40575
mu =
  3.5000   3.5000   3.5000
sigma =
  3.6968   4.4347   3.6968
```

`computeCostMulti`

```matlab
X = [ 2 1 3; 7 1 9; 1 8 1; 3 7 4 ];
y = [2 ; 5 ; 5 ; 6];
theta_test = [0.4 ; 0.6 ; 0.8];
computeCostMulti( X, y, theta_test )
% result
ans =  5.2950
```

`gradientDescentMulti`

```matlab
 = [ 2 1 3; 7 1 9; 1 8 1; 3 7 4 ];
y = [2 ; 5 ; 5 ; 6];
[theta J_hist] = gradientDescentMulti(X, y, zeros(3,1), 0.01, 100);

% results

>> theta
theta =

   0.23680
   0.56524
   0.31248

>> J_hist(1)
ans =  2.8299

>> J_hist(end)
ans =  0.0017196
```

`normalEqn`

```matlab
y = [2 ; 5 ; 5 ; 6];
theta = normalEqn(X,y)

% results
theta =

   0.0083857
   0.5681342
   0.4863732
```

### Debugging Tip

The submit script, for all the programming assignments, does not report the line number and location of the error when it crashes. The follow method can be used to make it do so which makes debugging easier.

Open ex1/lib/submitWithConfiguration.m and replace line:

```matlab
 fprintf('!! Please try again later.\n');

```

(around 28) with:

```matlab
fprintf('Error from file:%s\nFunction:%s\nOn line:%d\n', e.stack(1,1).file,e.stack(1,1).name, e.stack(1,1).line );
```

That top line says '!! Please try again later' on crash, instead of that, the bottom line will give the location and line number of the error. This change can be applied to all the programming assignments.

Note for OS X users
If you are using OS X and get this error message when you run ex1.m and expect to see a plot figure:

```bash
gnuplot> set terminal aqua enhanced title "Figure 1" size 560 420  font "*,6" dashlength 1
                  ^
     line 0: unknown or ambiguous terminal type; type just 'set terminal' for a list
```

... try entering this command in the workspace console to change the terminal type:

```bash
setenv("GNUTERM","qt")
```

How to check format of function arguments

So that you may print the argument just by typing its name in the body of the function on a distinct line and call submit() in Octave.

For example I may print the theta argument in the "Compute cost for one variable" exercise by writing this in my computeCost.m file. Of course, it will fail because 5 is just random number, but it will show me the value of theta:

```matlab
function J = computeCost(X, y, theta)
    m = length(y);
    J = 0
    theta
    J = 5  % I have added this line just to show that the argument you want to print doesn't have to be on the last line
end
```

### Testing matrix operations in Octave

In our programming exercises, there are many complex matrix operations where it may not be clear what form the result is in. I find it helpful to create a few basic matrices and vectors to test out my operations. For instance the following commands can be copied to a file to be used at any time for testing an operation.

```matlab
X = [1 2 3; 1 2 3; 1 2 3; 1 2 3; 1 5 6] % Make sure X has more rows than theta and isn't square
y = [1; 2; 3; 4; 5]
theta = [1; 1; 1]
```

With these basic matrices and vectors you can model most of the programming exercises. If you don't know what form specific operations in the exercises take, you can test it in the Octave shell.

One thing that got me was using formulas like `theta' * x` where x was a single row in X. All the notes show x as being a $m \times 1$ vector, but `X(i,:)` is a 1xm vector. Using the terminal, I figured out that I had to transpose x. It is very helpful.

### Repeating previous operations in Octave

When using the great unit tests by `Vinh`, if your function doesn't work the first time -- after you to edit and save your function file, then in your Octave window - just type ctrl-p to back up to what you typed previously, then enter to run it. (once you've gone back, can use ctrl-n for next) (more info @ https://www.gnu.org/software/octave/doc/interpreter/Commands-For-History.html)

### Warm up exercise

If you type "ex1.m" you will get an error - just use "ex1". Press 'Run' in Matlab editor.

### Compute cost for one variable

theta is a matrix of size $2 \times 1$; first row is `theta[0]` and second one is `theta[1]` (I following index convention of videos here) Also fill arbitrary (non-zero) initial values to `theta[0]` and `theta[1]`.

### Gradient descent for one variable

See the 5th segment of Week 1 Video II ("Gradient Descent") for a key tip on simultaneous updates of theta.

### Feature normalization

Use the [zscore](http://www.gnu.org/software/octave/doc/interpreter/Basic-Statistical-Functions.html#XREFzscore) function to normalize

repmat function can be used here.

The `bsxfun` is helpful for applying a function (limited to two arguments) in an element-wise fashion to rows of a matrix using a vector of source values. This is useful for feature normalization. An example you can enter at the octave command line:

```matlab
Z=[1 1 1; 2 2 2;];
v=[1 1 1];
bsxfun(@minus,Z,v);
ans =
    0   0   0
    1   1   1
```

In this case, the corresponding elements of v are subtracted from each row of Z. The minus(a,b) function is equivalent to computing (a-b).

(other mathematical functions: @plus, @rdivide)

In Octave >= 3.0.6 you can use [broadcast feature to abbreviate](https://www.gnu.org/software/octave/doc/interpreter/Broadcasting.html#Broadcasting)

```matlab
Z=[1 1 1; 2 2 2;];
v=[1 1 1];
Z - v   % or Z .- v
ans =
   0   0   0
   1   1   1
```

A note regarding Feature Normalization when a feature is a constant: `<provided by a ML-005 student>`

When I used the feature normalization routine we used in class it did not occur to me that some features of the training examples may have constant values, which means that the sigma vector has zeroes for those features. Thus when I divide by sigma to normalize the matrix NaNs filled in some slots. This causes gradient descent to get lost wandering through a NaN wasteland, but never reporting why.The fix is easy. In featureNormalize, after sigma is calculated but before the division takes place, insert

```matlab
sigma( sigma == 0 ) = 1;         % to keep away the NaN's and Inf's
```

Once this was done, gradient descent ran fine.

TA note: for the ML class exercises, you do not need this trick, because the scripts add the column of bias units after the features are normalized. But for your use outside of the class exercises, this may be a useful technique.

### Gradient descent for multiple variables

The lecture notes "Week 2" under section Matrix Notation basically spells out one line solution to the problem.

When predicting prices using theta derived from gradient descent, do not forget to normalize input x or you'll get multimillion house value (wrong one).

### Normal Equations

I found that the line `data = csvread('ex1data2.txt');` in ex1_multi.m is not needed as we previously load this data via `data = load('ex1data2.txt');`

Prior steps normalized X, this line sets X back to the original values. To have theta from gradient descent and from the normal equations to be close run the normal equations using normalized features as well. Therefor do not reload X.

Comment: I think the point in reloading is to show that you actually get the same results even without doing anything with the data beforehand. Of course for this script its not effective, but in a real application you would use only one of the approaches. Similar considerations would argue against feature normalization. Therefore do reload X.


