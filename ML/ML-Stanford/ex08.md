# Programming Assignment: Anomaly Detection and Recommender Systems

## 1 Anomaly detection

+ implement an anomaly detection algorithm to detect anomalous behavior in server computers

+ features:
  + throughput (mb/s)
  + latency (ms)

+ unlabeled dataset $\{ x^{(1)}, x^{(2)}, \dots, x^{(m)} \}$ with $m=307$
  + majority of examples: "normal" (non-anomalous) examples of he servers operating normally
  + only some examples of servers acting anomalously

+ use Gaussian model to detect anomalous examples in dataset

+ procedure
  + visualize the dataset
  + fit a Gaussian distribution
  + find values with low probability
  + considered as anomalies w/ low probability

+ apply the anomaly detection algorithm to a larger dataset with many dimensions


### 1.1 Gaussian distribution

+ fit a model to the data's distribution

+ training set: $\{ x^{(1)}, x^{(2)}, \dots, x^{(m)}\}, x^{(i)} \in \mathbb{R}^n$

+ estimate the Gaussian distribution for each of the features $x_i$
  + find parameters $\mu_i$ and $\sigma_i$ that fit the data inb the $i$-th dimension $x_i^{(1)}, \dots, x_i^{(m)}, \forall i = 1, \dots, n$
  + Gaussian distribution

    $$p(x; \mu_, \sigma) = \dfrac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

    where $\mu$ = mean, $\sigma^2$ = variance


### 1.2 Estimating parameters for a Gaussian

+ estimate the parameters, $(\mu_i, \sigma_i^2)$
  + the mean

    $$\mu_i = \dfrac{1}{m} \sum_{j=1}^m x_i^{(i)}$$

  + the variance

    $$\sigma_i^2 = \dfrac{1}{m} \sum_{j=1}^m (x_i^{(j)} - \mu_i)^2$$

+ complete code in `estimateGaussian.m`
  + input: data matrix $X$
  + output: 
    + $n$-min vector `mu` that holds the mean of all the $n$ features
    + $n$-dim vector `sigma2` that holdws the variance of all the features
  + Octave: the `var` using $\frac{1}{m-1}$ by default when computing $\sigma_i^2$

+ run `ex8.m` to visualize the contours of the fitted gaussian distribution


### 1.3 Selecting the threshold, $\epsilon$

+ investigate which examples have a very high probability given this distribution and which examples have a very low probability

+ low probability: most likely to be the anomalies in the dataset

+ determine which examples to be the anomalies: select a threshold based on a cross validation set

+ implement an algorithm to select the threshold $\epsilon$ using the $F_1$ score on a cross validation set

+ complete the code in `selectThreshold.m`
  + use a cross validation set $\{(x_{cv}^{(1)}, y_{cv}^{(1)}), \dots, (x_{cv}^{(m_{cv})}, y_{cv}^{(m_{cv})}) \}$
  + $y=1\;$: anomalous example
  + $y=0\;$: normal example
  + compute $p(x_{cv}^{(i)})$ for each cross validation  example
  + `selectThreshold.m` arguments
    + `pval`: the vector of all of these probabilities $p(x_{cv}^{(1)}), \dots, p(x_{cv}^{(m_{cv})})$
    + `yval`: the corresponding labels $y_{cv}^{(1)}, \dots, y_{cv}^{(m_{cv})}$
  + return two values:
    + the threshold $\epsilon$
    + the $F_1$ score

+ Anomaly: an example $x$ with low probability $p(x) < \epsilon$

+ $F_1$ score: how well you're doing on finding the ground truth anomalies given a certain threshold

+ With different $\epsilon$ values, compute the resulting $F_1$ score by computing how many examples the current threshold classifies correctly and incorrectly.

+ The $F_1$ score
  + computed using precision ($prec$) and recall ($rec$)
  
    $$F_1 = \dfrac{2 \cdot prec \cdot rec}{prec + rec}$$
  + compute precision and recall

    $$prec = \dfrac{tp}{tp + fp} \qquad rec = \dfrac{tp}{tp + fn}$$
  + Notation
    + $tp$ is the number of true positive: the ground truth label says it's an anomaly and our algorithm correctly classified it is an anomaly
    + $fp$ is the number of false positive: the ground truth label says it's not an anomaly, but our incorrectly classified it is as an anomaly
    + $fn$ is the number of false negative: thr ground truth label says it's an anomaly, but our algorithm incorrectly classified it as not being anomalous

+ `selectThreshold.m` loops through many different $\epsilon$ values and select the best $\epsilon$ based on the $F_1$ score

+ complete `selectThreshold.m`
  + implement the computation of the $F_1$ score using a for-loop over all the cross validation examples (to compute the values $tp, fp, fn$)
  + expect `epsilon` about $8.99e-05$

+ Implementation Note:
  + vectorized implementation: $tp, fp, fn$
  + implemented by Octave's equality test between a vector and a single number
  + several binary values in an $n$-dim binary vector $v \in \{0, 1\}^n$, find out how many values in this vector are 0 by using: `sum(v ==0)`
  + apply logical `and` operator to such binary vectors
  + E.g., let `cvPredictionns` be a binary vector of the size of your number of cross validation set, where $i$-th element is 1 if your algorithm considers $x_{cv}^{(i)}$ an anomaly, and 0 otherwise
  + E.g., compute the number of false positive using : `fp = sum((cvPredictions == 1) & (yval == 0))`

+ run `ex8.m` to detect anomaly and circle the anomalies in the plot


### 1.4 High dimensional dataset

+ run anomaly detection algorithm on a more realistic and much harder dataset

+ each example with 11 features, capturing many more properties of computer servers

+ to estimate the Gaussian parameters ($\mu_i$ and $\sigma_i^2$), evaluate the probabilities for both the training data $X$ from which you estimated the Gaussian parameters

+ cross-validation set `Xval` 

+ use `selectThreshold` to find the best threshold $\epsilon$

+ Expect a value epsilon of about 1.26e-18 and 117 anomalies found


## 2 Recommender Systems

+ implement the collaborative filtering learning algorithm and apply it to a dataset of movie ratings

+ dataset
  + rating scale: 1~5
  + $n_u = 934$ users
  + $n_m = 1682$ movies

+ work with `ex8_cofi.m`

+ implement function `cofiCostFunc.m` to compute the collaborative filtering objective function and gradient

+ use `fmincg.m` to learn the parameters for collaborative filtering


### 2.1 Movie ratings dataset

+ load the dataset `ex8_movies.mat`, providing the variable `Y` and `R` in Octave environment

+ The matrix `Y` (a num_movies x num_users matrix) stores the rating $y^{(i, j)}$ (from 1 to 5).

+ The matrix `R`: an binary-valued indicator matrix, where $R(i, j) = 1$ if user $j$ gave a rating to movie $i$, and $R(i, j) = 0$ otherwise.

+ objective of collaborative filtering: predict movie ratings for the movies that users have not yet rated, that is, the entries with $R(i, j) = 0$.

+ used to recommend the movies with the highest predicted ratings to the user

+ `ex8_cofi.m` computes the average movie rating for the movie anmd output the average rating to the screen

+ The matrices

  $$\text{X } = \begin{bmatrix} - & (x^{(1)})^T & - \\ - & (x^{(2)})^T & - \\ & \vdots & \\ - & (x^{(n_m)})^T & - \end{bmatrix},  \qquad\qquad \text{Theta } = \begin{bmatrix} - & (\theta^{(1)})^T & - \\ - & (\theta^{(2)})^T & - \\ & \vdots & \\ - & (\theta^{(n_u)})^T & - \ \end{bmatrix}$$

  + the $i$-th row of `X`  corresponding to the feature vector $x^{(i)}$ for the $i$-th movie
  + the $j$-th row of `Theta` corresponding to one parameter vector $\theta^{(j)}$, for the $j$-th user
  + both $x^{(i)}$ and $\theta^{(i)}$ are $n$-dim vectors

+ Dimensional analysis
  + $n = 100 \therefore x^{(i)} \in \mathbb{R}^{100} \text{ and } \theta^{(j)} \in \mathbb{R}^{100}$
  + `X` ($n_m \times 100$ matrix)
  + `Theta` ($n_u \times 100$ matrix) 


### 2.2 Collaborative filtering learning algorithm

+ implement the collaborative filtering learning algorithm

+ start by implementing the cost function without regularization

+ Consider a set of $n$-dim parameter vectors $x^{(1)}, \dots, x^{(n_m)}$ and $\theta^{(1)}, \dots, \theta^{(n_u)}$, where the miodel predicts the rating for moview $i$ by user $j$ as $y^{(i, j)} = (\theta^{(j)})^Tx^{(i)}$

+ given a dataset that consists of a set of ratings produced by some users on some movies, to learn the parameter vectors $x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)}, \dots, \theta^{(n_u)}$ that produce the best fit (minimizes the squared error)

+ complete the code in `cofiCostFunc.m` to compute the cost function and gradient for collaborative filtering

+ to use an off-the-shelf minimizer such as `fmincg`, the cost function has been set up to unroll the parameters into a single vector `params`

+ the same vector unrolling method in the neural networks programming exercise


#### 2.2.1 Collaborative filtering cost function

+ collaborative filtering cost function (without regularization)

  $$J(x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)}, \dots, \theta^{(n_u)}) = \dfrac{1}{2} \sum_{(i, j): r(i, j) = 1} ((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})^2$$

+ modify `cofiCostFunc.m` to return this cost in the variable$J$

+ Note: accumulate the cost for user $j$ and movie $i$ only if $R(i, j) = 1$

+ run `ex8_cofi.m` and expect to see output of 22.22

+ Implementation Note
  + stronly encourage using a vectorized implementation to compute $J$
  + $J$  called many times by the optimization package `fmincg`
  + use `R` to set selected entry\ies to 0, e.g., `R .* M` as an element-wise multiplication between `M` and `R`
  + `sum(sum9R .* M))`: the sum of all the elements of M for which the corresponding element in `R` equals 1


#### 2.2.2 Collaborative filtering gradient

+ implement the gradient 9without regularization)

+ complete code in `cofiCostFunc.m` to return the variables `X_grad` and `Theta_grad`
  + `X_grad`: a matrix of the same size as `X` and similarly
  + `Theta_grad`: a matrix of the same size as `Theta`

+ Gradients of cost function

  $$\begin{array}{rcl} \dfrac{\partial J}{\partial x_k^{(i)}} & = & \sum_{j:r(i, j)= 1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i, j)} \right) \theta_k^{(j)} \\\\ \dfrac{\partial J}{\partial \theta_k^{(j)}} & = & \sum_{i:r(i, j)= 1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i, j)} \right) x_k^{(j)} \end{array}$$

+ the function returns the gradient for both sets of variables by unrolling them into a single vector

+ complete the code to compute the gradients

+ run `ex8_cofi.m` to execute a gradient check (`checkCostFunction`) to numerically check the implementation of gradients

+ Implementation Note:
  + able to implement the gradient with a for-loop over movies (for computing $\frac{\partial J}{\partial x_k^{(i)}}$) and a for-loop over users (for computing $\frac{\partial J}{\theta_k^{(j)}}$)
  + initial implement: start with an unconvectorized version, by implementing another inner for-loop that computes each element in the summation
  + vectorized the inner for-loops: left with only two for-loops (one for looping over movies to compute $\frac{\partial J}{\partial x_k^{(i)}}$ for each movie, and one for looping over users to compute $\frac{\partial J}{\partial \theta_k^{(j)}}$ for each user)

+ Implementation Tip
  + come up a way to compute all the derivatives associated with $x_1^{(i)}, x_2^{(i)}, \dots, x_n^{(i)}$ (i.e., the derivative terms associated with the feature vector $x^{(i)]}$) at the sam etime
  + define the derivatives for the feature vector of the $i$-th movie

    $$(X_{grad}(i,:))^T = \begin{bmatrix} \frac{\partial J}{\partial x_1^{(i)}} \\ \frac{\partial J}{\partial x_2^{(i)}} \\ \vdots \\ \frac{\partial J}{\partial x_n^{(i)}} \end{bmatrix} = \sum_{j: r(i, j)=1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i, j)} \right) \theta^{(j)}$$

    + start by indexing into `Theta` and `Y` to select only the elements of interests (i.e., those with $r(i, j) = 1$)
    + consider the features for the $i$-th movie
    + only concern the users who had given ratings to the movie
    + allow to remove all the other users from `Theta` and `Y`
  + set `idx = find(R(i, :) == 1)`to be a list if all the users that have rated movie $i$
  + allow to create the temporary matrices $Theta_{temp} = Theta(idx, :)$ and $Y_{temp} = Y(idx, :)$ that index into `Theta` and `Y` to give you only the set of users which have rated the $i$-th movie
  + The derivatives

    $$X_{grad}(i, :) = (X(i, :) \ast Theta_{temp}^T - y_{temp}) \ast Theta_{temp}$$

    + the vectorized computation returns a row-vector
  + After vectorized he computations of the derivatives wrt $x^{(i)}$, use a similar method to vectorize the derivatives wrt $\theta^{(j)}$


#### 2.2.3 Regularized cost function

+ The cost function for collaborative filtering with regularization

  $$J(x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)}, \dots, \theta^{(n_u)}) = \dfrac{1}{2} \sum_{(i, j): r(i, j) = 1} ((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})^2 + \left( \dfrac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^T \right) + \left( \dfrac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 \right)$$

+ add regularization to original computations of the cost function, $J$

+ run `ex8_cofi.m` and expect to see a cost of about 31.34


#### 2.2.4 Regularized gradient

+ implement regularization for the gradient

+ the gradients for the regularized cost function

  $$\begin{array}{rcl} \dfrac{\partial J}{\partial x_k^{(i)}} & = & \sum_{j:r(i, j)= 1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i, j)} \right) \theta_k^{(j)} + \lambda x_k^{(i)} \\\\ \dfrac{\partial J}{\partial \theta_k^{(j)}} & = & \sum_{i:r(i, j)= 1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i, j)} \right) x_k^{(j)} + \lambda \theta_k^{(j)} \end{array}$$

  + add $\lambda x^{(i)}$ to `X_grad(i, :)`
  + add $\lambda \theta^{(j)}$ to `Theta_grad(i, :)`

+ run `ex8_cofi.m` for numerical check


### 2.3 Learning movie recommendations

+ training algorithm to make movie recommendations

+ `movie-idx.txt` lists all movies and their number in the dataset


#### 2.3.1 Recommendations

+ after additional ratings added into dataset, the `ex8_cofi.m` trains the collaborative filtering model

+ learn `Theta` and `X` parameters

+ to predict the rating of movie $i$ for user $j$, compute $(\theta^{(j)})^T x^{(i)}$

+ compute the ratings for all the movies and users and display the movies that recommends, according to ratings


## Vectorized Collaborative Filtering Cost Function & Gradients

### Notations

$$\begin{array}{crl} x^{(i)} = \begin{bmatrix} x_1^{(i)} \\ x_2^{(i)} \\ \vdots \\ x_n^{(i)} \end{bmatrix} & \implies & X = \underbrace{\begin{bmatrix} (x^{(1)})^T \\ (x^{(2)})^T \\ \vdots \\ (x^{(n_m)})^T \end{bmatrix}}_{n_m \times n} = \begin{bmatrix} x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\ x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)}  \\ \vdots & \vdots & \ddots & \vdots \\ x_1^{(n_m)} & x_2^{(n_m)} & \cdots & x_n^{(n_m)} \end{bmatrix} \\\\ \theta^{(j)} = \begin{bmatrix} \theta_1^{(i)} \\ \theta_2^{(i)} \\ \vdots \\ \theta_n^{(i)} \end{bmatrix} & \implies & \theta = \underbrace{\begin{bmatrix} (\theta^{(1)})^T \\ (\theta^{(2)})^T \\ \vdots \\ (\theta^{(n_m)})^T \end{bmatrix}}_{n_u \times n} = \begin{bmatrix} \theta_1^{(1)} & \theta_2^{(1)} & \cdots & \theta_n^{(1)} \\ \theta_1^{(2)} & \theta_2^{(2)} & \cdots & \theta_n^{(2)}  \\ \vdots & \vdots & \ddots & \vdots \\ \theta_1^{(n_m)} & \theta_2^{(n_m)} & \cdots & \theta_n^{(n_m)} \end{bmatrix} \end{array}$$

<br/>

$$ e_{n_m} = \underbrace{\begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}}_{n_m \times 1} \qquad e_{n_u} = \underbrace{\begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}}_{n_u \times 1} \qquad R = \begin{bmatrix} r(1,1) & r(1, 2) & \cdots & r(1, n_u) \\ r(2,1) & r(2, 2) & \cdots & r(2, n_u) \\ \vdots & \vdots & \ddots & \vdots \\ r(n_m, 1) & r(n_m, 2) & \cdots & r(n_m, n_u) \\ \end{bmatrix} \quad Y = \begin{bmatrix} y^{(1, 1)} & y^{(1, 2)} & \cdots & y^{(1, n_u)} \\ y^{(2, 1)} & y^{(2, 2)} & \cdots & y^{(2, n_u)} \\ \vdots & \vdots & \ddots & \vdots \\ y^{(n_m, 1)} & y^{(n_m, 2)} & \cdots & y^{(n_m, n_u)} \end{bmatrix}$$

### Cost Function

+ Dimension analysis:

  $$J(x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)}, \dots, \theta^{(n_u)}) = \dfrac{1}{2} \sum_{(i, j): r(i, j) = 1} (\underbrace{(\theta^{(j)})^Tx^{(i)}}_{(A)} - y^{(i, j)})^2$$

  Term (A): $Y_{pred} (n_m \times n_u) \Leftarrow (n_m \times n) \times (n \times n_u) \;\therefore\; Y_{pred} = X \cdot \theta^T$
  
+ Vectorized Cost Function

  $$\begin{array}{rcl} J(x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)}, \dots, \theta^{(n_u)}) & = & \dfrac{1}{2} \sum_{(i, j): r(i, j) = 1} ((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})^2 \\\\ & = & \frac{1}{2} \cdot e_{n_m}^T \left( ((X \cdot \theta^T) \circ R) -  Y \right)^{\circ 2} \cdot e_{n_u} \end{array}$$

  + Hadamard product ($\circ$)
    + Def:

      $$[A \circ B]_{ij} = [A]_{ij} \cdot [B]_{ij}$$

    + Analogous operations

      $$\begin{array}{rcl} B = A^{\circ 2} & \implies & B_{ij} = A_{ij}^2 \\\\ B = A^{\circ \frac{1}{2}} & \implies & B_{ij} = A_{ij}^{\frac{1}{2}} \\\\ B = A^{\circ -1} & \implies & B_{ij} = A_{ij}^{-1} \\\\ C = A \oslash B & \implies & C_{ij} = \dfrac{A_{ij}}{B_{ij}} \end{array}$$


### Gradient

+ derivatives of the feature vector

  $$(X_{grad}(i,:))^T = \begin{bmatrix} \frac{\partial J}{\partial x_1^{(i)}} \\ \frac{\partial J}{\partial x_2^{(i)}} \\ \vdots \\ \frac{\partial J}{\partial x_n^{(i)}} \end{bmatrix} = \sum_{j: r(i, j)=1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i, j)} \right) \theta^{(j)}$$


## Programming Discussion


### Error in `ex8_cofi.m`


There is an error in the `ex8_cofi.m` script. In Part 7, around line 200, the code that calls `fmincg()` and `cofiCostFunc()` should use the Ynorm variable, instead of Y.

Figure 4 in `ex8.pdf` is also incorrect - no movies should have ratings higher than 5.

This issue is also documented in the Errata lists in the Resources menu.


### ex8 tutorial for `cofiCostFunc()`

Vectorized tutorial for cost and gradients with regularization

+ Definitions:
    + $R$: a matrix of observations (binary values). Dimensions are (movies x users)
    + $Y$: a matrix of movie ratings: Dimensions are (movies x users)
    + $X$: a matrix of movie features (0 to 5): Dimensions are (movies x features)
    + $Theta$: a matrix of feature weights: Dimensions are (users x features)

+ Compute the predicted movie ratings for all users using the product of $X$ and $Theta$. A transposition may be needed. Dimensions of the result should be (movies x users).
+ Compute the movie rating error by subtracting Y from the predicted ratings.
+ Compute the "error_factor" my multiplying the movie rating error by the R matrix. The error factor will be 0 for movies that a user has not rated. Use the type of multiplication by R (vector or element-wise) so the size of the error factor matrix remains unchanged (movies x users).

(Note: there is a quirk in the submit grader's test case that requires you to use the R matrix to ignore movies that have had no ratings).

#### Calculate the cost:

+ Using the formula on Page 9 of ex8.pdf, compute the unregularized cost as a scaled sum of the squares of all of the terms in error_factor. The result should be a scalar.
+ Test your code using ex8_cofi.m and the additional test cases. You should get a passing grade for this portion from the submit script.

#### Calculate the gradients (ref: the formulas on Page 10 of ex8,pdf):

+ The $X$ gradient is the product of the error factor and the Theta matrix. The sum is computed automatically by the vector multiplication. Dimensions are (movies x features)
+ The $Theta$ gradient is the product of the error factor and the X matrix. A transposition may be needed. The sum is computed automatically by the vector multiplication. Dimensions are (users x features)
+  Test your code, then submit this portion.

#### Calculate the regularized cost:

+ Using the formula on the top of Page 13 of ex8.pdf, compute the regularization term as the scaled sum of the squares of all terms in Theta and X. The result should be a scalar. Note that for Recommender Systems there are no bias terms, so regularization should include all columns of X and Theta.
+ Add the regularized and un-regularized cost terms.
+ Test your code, then submit this portion.

#### Calculate the gradient regularization terms (ref: the formulas in the middle of Page 13 of ex8.pdf)

+ The X gradient regularization is the X matrix scaled by lambda.
+ The Theta gradient regularization is the Theta matrix scaled by lambda.
+ Add the regularization terms to their unregularized values.
+ Test your code, then submit this portion.


### Programming Exercise 8

#### Debugging Tip

The submit script, for all the programming assignments, does not report the line number and location of the error when it crashes. The follow method can be used to make it do so which makes debugging easier.

Open ex8/lib/submitWithConfiguration.m and replace line:

```matlab
 fprintf('!! Please try again later.\n');
```

(around 28) with:

```matlab
fprintf('Error from file:%s\nFunction:%s\nOn line:%d\n', e.stack(1,1).file,e.stack(1,1).name, e.stack(1,1).line );
```

That top line says '!! Please try again later' on crash, instead of that, the bottom line will give the location and line number of the error. This change can be applied to all the programming assignments.

#### error in ex8_cofi.m (reported by Charles Davis in session ML-005)

line 199 in `ex8_cofi.m` reads

```matlab
theta = fmincg (@(t)(cofiCostFunc(t, Y, R, num_users, num_movies, num_features, lambda)), initial_parameters, options);
```

but I believe it should be

```matlab
theta = fmincg (@(t)(cofiCostFunc(t, Ynorm, R, num_users, num_movies, num_features, lambda)), initial_parameters, options);

```

...to avoid creating ratings > 5 at line 219. This doesn't affect the submissions of course, just the cosmetics of the recommendations.

Supporting analysis: Y is normalized in line 181, creating Ynorm, but then it is never used. The video lecture "Implementation Detail: Mean Normalization" at 5:34 makes it pretty clear that the normalized Y matrix should be used for calculating theta.

This errata also means that "ex8.pdf" Figure 4 is incorrect, since it shows movies with ratings greater than 5-stars.


#### Item 2: The grader uses Y with non-zero values

When using the R matrix (to ignore movies that have not been rated), do not rely on Y(i,j) to be 0 when a user has not rated a film. This expectation is true for the ex8_cofi.m script, but that is NOT true for the test case used by the submit grader for Part 3 through Part 6.

Note: This might no longer be true, the grader seems to be using Y(i,j) == 0 for when a user has not rated a film


#### Item 3: Regularization

Note: Unlike previous assignments when we performed regularization, for this exercise, we do NOT skip the 1st column of Theta or X when computing regularization. This is because we are not specifying bias units in the collaborative filtering algorithm (since the algorithm determines all of the theta values, it can set one to the '1' value if it leads to the optimum solution). Therefore, all values of Theta and X should be considered in regularization.

1.2 Estimating parameters for a Gaussian
the var function can actually return normalization with 1/m instead of 1/(m-1). Set the second argument 0 for 1/(m-1) and 1 for 1/m.


#### errors in cofiCostFunc.m

line 9 should read "% Unfold the X and Theta matrices from params"


### FAQ for Week 9 and programming exercise 8


#### FAQ for the video lectures

The Lecture Notes are available in the Resources menu.

The lecture slides are available in the Review section of each week of the course materials.


#### FAQ for the quiz

(blank)

#### FAQ for Programming Exercise 8

Note: Tutorials and additional Test Cases are available in the Resources menu.

__Q1) I get the correct results for the collaborative filtering exercise, but I don't get any points from the submit grader. Why?__

The tutorial for this exercise (via the Resources menu) tells about a characteristic of the submit grader's test case which your cofiCostFunc() code must cope with.

__Q2) Why don't my movie recommendations match Figure 4 in ex8.pdf?__

Figure 4 in ex8.pdf was created using a defective version of ex8_cofi.m. The error has been repaired, but ex8.pdf has not been re-generated yet.

Your results should look something like this, but the specific movies will vary due to the random initialization of X and Theta.

```matlab
Top recommendations for you:
Predicting rating 5.0 for movie Marlene Dietrich: Shadow and Light (1996)
Predicting rating 5.0 for movie Great Day in Harlem, A (1994)
Predicting rating 5.0 for movie Saint of Fort Washington, The (1993)
Predicting rating 5.0 for movie Aiqing wansui (1994)
Predicting rating 5.0 for movie Santa with Muscles (1996)
Predicting rating 5.0 for movie Star Kid (1997)
Predicting rating 5.0 for movie They Made Me a Criminal (1939)
Predicting rating 5.0 for movie Entertaining Angels: The Dorothy Day Story (1996)
Predicting rating 5.0 for movie Prefontaine (1997)
Predicting rating 5.0 for movie Someone Else's America (1995)
```

### [Test cases for ex8_cofi - Recommender Systems](https://www.coursera.org/learn/machine-learning/discussions/weeks/9/threads/X0DEZEQ6EeWPFgp1wk3USw)

Here are test cases for the second part of ex8.

Note: There is an error in ex8_cofi.m. You will need to edit the script, see the tutorial for ex8 in the Resources menu.

```matlab
---------------------------------------
Test 3a (Collaborative Filtering Cost):
input:
params = [ 1:14 ] / 10;
Y = magic(4);
Y = Y(:,1:3);
R = [1 0 1; 1 1 1; 0 0 1; 1 1 0] > 0.5;     % R is logical
num_users = 3;
num_movies = 4;
num_features = 2;
lambda = 0;
J = cofiCostFunc(params, Y, R, num_users, num_movies, num_features, lambda)
output:
J =  311.63
---------------------------------------
Test 4a (Collaborative Filtering Gradient):
input:
params = [ 1:14 ] / 10;
Y = magic(4);
Y = Y(:,1:3);
R = [1 0 1; 1 1 1; 0 0 1; 1 1 0] > 0.5;     % R is logical
num_users = 3;
num_movies = 4;
num_features = 2;
lambda = 0;
[J, grad] = cofiCostFunc(params, Y, R, num_users, num_movies, num_features, lambda)

output:
J =  311.63

grad =
  -16.1880
  -23.5440
   -5.1590
  -14.9720
  -21.4380
  -30.4620
   -6.5660
  -19.5440
   -3.4230
   -7.0280
   -3.4140
  -12.2590
  -16.0600
   -9.7420

---------------------------------------
Test 5a (Regularized Cost):
input:
params = [ 1:14 ] / 10;
Y = magic(4);
Y = Y(:,1:3);
R = [1 0 1; 1 1 1; 0 0 1; 1 1 0] > 0.5;     % R is logical
num_users = 3;
num_movies = 4;
num_features = 2;
lambda = 6;
J = cofiCostFunc(params, Y, R, num_users, num_movies, num_features, lambda)

output:
J =  342.08

---------------------------------------
Test 6a (Gradient with regularization):
input:
params = [ 1:14 ] / 10;
Y = magic(4);
Y = Y(:,1:3);
R = [1 0 1; 1 1 1; 0 0 1; 1 1 0] > 0.5;     % R is logical
num_users = 3;
num_movies = 4;
num_features = 2;
lambda = 6;
[J, grad] = cofiCostFunc(params, Y, R, num_users, num_movies, num_features, lambda)
output:
J =  342.08

grad =
  -15.5880
  -22.3440
   -3.3590
  -12.5720
  -18.4380
  -26.8620
   -2.3660
  -14.7440
    1.9770
   -1.0280
    3.1860
   -5.0590
   -8.2600
   -1.3420
-----------
Test 6b (a user with no reviews):
input:
params = [ 1:14 ] / 10;
Y = magic(4);
Y = Y(:,1:3);
R = [1 0 1; 1 1 1; 0 0 0; 1 1 0] > 0.5;     % R is logical
num_users = 3;
num_movies = 4;
num_features = 2;
lambda = 6;
[J, grad] = cofiCostFunc(params, Y, R, num_users, num_movies, num_features, lambda)
output:
J =  331.08

grad =
  -15.5880
  -22.3440
    1.8000
  -12.5720
  -18.4380
  -26.8620
    4.2000
  -14.7440
    1.9770
   -1.0280
    4.5930
   -5.0590
   -8.2600
    1.9410
```


I'm finding difficulties computing the gradient. I'm putting here X_grad as an example:

I did the following steps:

1- Compute the hypothesis:

$$(\theta^{(j)})^T x^{(i)} - y^{(i, j)}$$

Result is a 5*4 matrix.

2- Multiply hypo matrix by R in order to zero any unrated movie by a user.

Result is 5*4 matrix.

3- Now I've all hypo per user but, I'm really not sure what the (red circled) term means (this is what we multiply the hypo from step#2 by):

$$\dfrac{\partial J}{\partial x_k^{(i)}} = \sum_{j:r(i, j) = 1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i, j)} \right) \theta_k^{(j)}$$

I suspect it to be the features of the users, but, how then to relate this to the hypo?Still matrix multiplication can be done, but the results will never be correct.


### [Vectorized Implementation of Computing Gradient](https://www.coursera.org/learn/machine-learning/discussions/weeks/9/threads/dogoz4LNEeaehxKamXPqUw)

the rough idea on what I did. You may draft the process of your matrix calculation on a paper and slowly convert it into the vectorize formula.

```matlab
for i=1:num_features
  X_grad(:,i) = your vectorize function in one line;
  Theta_grad(:,i) = your vectorize function in one line;
end
```


### [Computing gradient can be fully vectorized](https://www.coursera.org/learn/machine-learning/discussions/weeks/9/threads/-5gH3Y4KEeWJ8Q5UvS9Vkw)

+ In the programming assignment, we are advised to partly vectorized the computation of gradient so that there are only two loops left. However, the computation can be fully vectorized. According to honor code, I am not allowed to provide the code. But in fact J, X_grad and theta_grad can be calculated each with one line of code.

+ As in the previous exercises, I found that going straight for the fully-vectorized solution was much simpler than following the implementation tip in the PDF, which seems unnecessarily complicated. The vectorized solution is actually really simple.

  A few hints:

  1) Look at the cost function from the last step, and then look again at the gradient functions. Notice any similarities? Assuming you vectorized the cost function, you can reuse part of the calculation.

  2) Extract the common part into a variable and use it in both the cost function and the gradient calculations. This will greatly improve performance as well as simplifying your code.

  3) The gradient calculations should each now be a simple matter of multiplying 2 matrices. Pay close attention to matrix dimensions.

  I hope this helps and doesn't step over the line in regards to the honour code.

+ Finding vectorized X_grad and Theta_grad is similar to the computation of J. First find M then use R to get rid of unwanted entries and then multiply it with Theta to have X_grad. During multiplications keep in mind the dimensions of the matrices such that the resulting X_grad has the same dimensions as that of X, which is also mentioned in ex8.pdf. Similarly for Theta_grad, first find M then use R and then multiply it with X. Consider transposing during the process to obtain the dimensions of Theta_grad equivalent to that of Theta. I hope it helps. Thanks


### [Some matrix math useful in vectorization](https://www.coursera.org/learn/machine-learning/discussions/weeks/9/threads/Sl_Oy-hMEeavigrMdw1r5g)

The following is to supplement the implementation in p. 12 of Ex8.pdf. The essence is to use component representation to get the matrix form of the gradient components. Product of two matrices, A = BC, can be written as

$$[A]_{ij} = \sum_k [B]_{ik}[C]_{kj}$$.

Lets proceed. First, the cost function

$$J = \frac{1}{2} \sum_{i,j} \tilde{M}_{ij}^2$$

with the matrix element $\tilde{M}_{ij} = ( \sum_l X^i_l \Theta^j_l − y_{ij})r_{ij} = M_{ij} r_{ij}$. Since we know that $r_{ij}$ is either zero or one, it follows that $r^2_{ij} = r_{ij}$. Thus, one can rewrite the above as,

$$J = \frac{1}{2} \sum_{i,j} r_{ij}M_{ij}^2$$

The gradient component associated with $X^i_k$ can be shown to be,

$$\dfrac{\partial J}{\partial X^l_k := [Xg]_{lk}} = \sum_{ij} \tilde{M}_{ij} \dfrac{\partial M_{ij}}{\partial X^l_k}$$

One may progress to show that the derivative

$$\dfrac{\partial M_{ij}}{\partial X^l_k} = I_{li} \Theta^j_k$$

where $I$ is identity matrix. Thus, one can get

$$[Xg]_{lk} = \sum_{ij} I_{li} \tilde{M}_{ij} \Theta^j_k := [\tilde{M} \Theta]_{lk}$$

for the components associated with X. Similarly, one can show that

$$[\Theta_g]_{lk} = [\tilde{M}^T X]_{lk}$$

is for the gradient components for $\Theta$.

By doing this, I do not have to use the idx vector in p.12 to take care the sum with constraint twice. The constraint was taken care of in generating the $\tilde{M}$ matrix. Hope this will help someone with experience in matrix math.













