# Programming Assignment: Anomaly Detection and Recommender Systems

## 1 Anomaly detection

+ implement an anomaly detection algorithm to detect anomalous behavior in server computers

+ features:
  + throughput (mb/s)
  + latency (ms)

+ unlabeled dataset $\{ x^{(1)}, x^{(2)}, \dots, x^{(m)} \}$ with $m=307$
  + majority of examples: "normal" (non-anomalous) examples of he servers operating normally
  + only some examples of servers acting anomalously

+ use Gaussian model to detect anomalous examples in dataset

+ procedure
  + visualize the dataset
  + fit a Gaussian distribution
  + find values with low probability
  + considered as anomalies w/ low probability

+ apply the anomaly detection algorithm to a larger dataset with many dimensions


### 1.1 Gaussian distribution

+ fit a model to the data's distribution

+ training set: $\{ x^{(1)}, x^{(2)}, \dots, x^{(m)}\}, x^{(i)} \in \mathbb{R}^n$

+ estimate the Gaussian distribution for each of the features $x_i$
  + find parameters $\mu_i$ and $\sigma_i$ that fit the data inb the $i$-th dimension $x_i^{(1)}, \dots, x_i^{(m)}, \forall i = 1, \dots, n$
  + Gaussian distribution

    $$p(x; \mu_, \sigma) = \dfrac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

    where $\mu$ = mean, $\sigma^2$ = variance


### 1.2 Estimating parameters for a Gaussian

+ estimate the parameters, $(\mu_i, \sigma_i^2)$
  + the mean

    $$\mu_i = \dfrac{1}{m} \sum_{j=1}^m x_i^{(i)}$$

  + the variance

    $$\sigma_i^2 = \dfrac{1}{m} \sum_{j=1}^m (x_i^{(j)} - \mu_i)^2$$

+ complete code in `estimateGaussian.m`
  + input: data matrix $X$
  + output: 
    + $n$-min vector `mu` that holds the mean of all the $n$ features
    + $n$-dim vector `sigma2` that holdws the variance of all the features
  + Octave: the `var` using $\frac{1}{m-1}$ by default when computing $\sigma_i^2$

+ run `ex8.m` to visualize the contours of the fitted gaussian distribution


### 1.3 Selecting the threshold, $\epsilon$

+ investigate which examples have a very high probability given this distribution and which examples have a very low probability

+ low probability: most likely to be the anomalies in the dataset

+ determine which examples to be the anomalies: select a threshold based on a cross validation set

+ implement an algorithm to select the threshold $\epsilon$ using the $F_1$ score on a cross validation set

+ complete the code in `selectThreshold.m`
  + use a cross validation set $\{(x_{cv}^{(1)}, y_{cv}^{(1)}), \dots, (x_{cv}^{(m_{cv})}, y_{cv}^{(m_{cv})}) \}$
  + $y=1\;$: anomalous example
  + $y=0\;$: normal example
  + compute $p(x_{cv}^{(i)})$ for each cross validation  example
  + `selectThreshold.m` arguments
    + `pval`: the vector of all of these probabilities $p(x_{cv}^{(1)}), \dots, p(x_{cv}^{(m_{cv})})$
    + `yval`: the corresponding labels $y_{cv}^{(1)}, \dots, y_{cv}^{(m_{cv})}$
  + return two values:
    + the threshold $\epsilon$
    + the $F_1$ score

+ Anomaly: an example $x$ with low probability $p(x) < \epsilon$

+ $F_1$ score: how well you're doing on finding the ground truth anomalies given a certain threshold

+ With different $\epsilon$ values, compute the resulting $F_1$ score by computing how many examples the current threshold classifies correctly and incorrectly.

+ The $F_1$ score
  + computed using precision ($prec$) and recall ($rec$)
  
    $$F_1 = \dfrac{2 \cdot prec \cdot rec}{prec + rec}$$
  + compute precision and recall

    $$prec = \dfrac{tp}{tp + fp} \qquad rec = \dfrac{tp}{tp + fn}$$
  + Notation
    + $tp$ is the number of true positive: the ground truth label says it's an anomaly and our algorithm correctly classified it is an anomaly
    + $fp$ is the number of false positive: the ground truth label says it's not an anomaly, but our incorrectly classified it is as an anomaly
    + $fn$ is the number of false negative: thr ground truth label says it's an anomaly, but our algorithm incorrectly classified it as not being anomalous

+ `selectThreshold.m` loops through many different $\epsilon$ values and select the best $\epsilon$ based on the $F_1$ score

+ complete `selectThreshold.m`
  + implement the computation of the $F_1$ score using a for-loop over all the cross validation examples (to compute the values $tp, fp, fn$)
  + expect `epsilon` about $8.99e-05$

+ Implementation Note:
  + vectorized implementation: $tp, fp, fn$
  + implemented by Octave's equality test between a vector and a single number
  + several binary values in an $n$-dim binary vector $v \in \{0, 1\}^n$, find out how many values in this vector are 0 by using: `sum(v ==0)`
  + apply logical `and` operator to such binary vectors
  + E.g., let `cvPredictionns` be a binary vector of the size of your number of cross validation set, where $i$-th element is 1 if your algorithm considers $x_{cv}^{(i)}$ an anomaly, and 0 otherwise
  + E.g., compute the number of false positive using : `fp = sum((cvPredictions == 1) & (yval == 0))`

+ run `ex8.m` to detect anomaly and circle the anomalies in the plot


### 1.4 High dimensional dataset

+ run anomaly detection algorithm on a more realistic and much harder dataset

+ each example with 11 features, capturing many more properties of computer servers

+ to estimate the Gaussian parameters ($\mu_i$ and $\sigma_i^2$), evaluate the probabilities for both the training data $X$ from which you estimated the Gaussian parameters

+ cross-validation set `Xval` 

+ use `selectThreshold` to find the best threshold $\epsilon$

+ Expect a value epsilon of about 1.26e-18 and 117 anomalies found


## 2 Recommender Systems

+ implement the collaborative filtering learning algorithm and apply it to a dataset of movie ratings

+ dataset
  + rating scale: 1~5
  + $n_u = 934$ users
  + $n_m = 1682$ movies

+ work with `ex8_cofi.m`

+ implement function `cofiCostFunc.m` to compute the collaborative filtering objective function and gradient

+ use `fmincg.m` to learn the parameters for collaborative filtering


### 2.1 Movie ratings dataset

+ load the dataset `ex8_movies.mat`, providing the variable `Y` and `R` in Octave environment

+ The matrix `Y` (a num_movies x num_users matrix) stores the rating $y^{(i, j)}$ (from 1 to 5).

+ The matrix `R`: an binary-valued indicator matrix, where $R(i, j) = 1$ if user $j$ gave a rating to movie $i$, and $R(i, j) = 0$ otherwise.

+ objective of collaborative filtering: predict movie ratings for the movies that users have not yet rated, that is, the entries with $R(i, j) = 0$.

+ used to recommend the movies with the highest predicted ratings to the user

+ `ex8_cofi.m` computes the average movie rating for the movie anmd output the average rating to the screen

+ The matrices

  $$\text{X } = \begin{bmatrix} - & (x^{(1)})^T & - \\ - & (x^{(2)})^T & - \\ & \vdots & \\ - & (x^{(n_m)})^T & - \end{bmatrix},  \qquad\qquad \text{Theta } = \begin{bmatrix} - & (\theta^{(1)})^T & - \\ - & (\theta^{(2)})^T & - \\ & \vdots & \\ - & (\theta^{(n_u)})^T & - \ \end{bmatrix}$$

  + the $i$-th row of `X`  corresponding to the feature vector $x^{(i)}$ for the $i$-th movie
  + the $j$-th row of `Theta` corresponding to one parameter vector $\theta^{(j)}$, for the $j$-th user
  + both $x^{(i)}$ and $\theta^{(i)}$ are $n$-dim vectors

+ Dimensional analysis
  + $n = 100 \therefore x^{(i)} \in \mathbb{R}^{100} \text{ and } \theta^{(j)} \in \mathbb{R}^{100}$
  + `X` ($n_m \times 100$ matrix)
  + `Theta` ($n_u \times 100$ matrix) 


### 2.2 Collaborative filtering learning algorithm



#### 2.2.1 Collaborative filtering cost function



#### 2.2.2 Collaborative filtering gradient



#### 2.2.3 Regularized cost function



#### 2.2.4 Regularized gradient



### 2.3 Learning movie recommendations



#### 2.3.1 Recommendations




## Programming Discussion


### Error in `ex8_cofi.m`


There is an error in the `ex8_cofi.m` script. In Part 7, around line 200, the code that calls `fmincg()` and `cofiCostFunc()` should use the Ynorm variable, instead of Y.

Figure 4 in `ex8.pdf` is also incorrect - no movies should have ratings higher than 5.

This issue is also documented in the Errata lists in the Resources menu.


### ex8 tutorial for `cofiCostFunc()`

Vectorized tutorial for cost and gradients with regularization

+ Definitions:
    + $R$: a matrix of observations (binary values). Dimensions are (movies x users)
    + $Y$: a matrix of movie ratings: Dimensions are (movies x users)
    + $X$: a matrix of movie features (0 to 5): Dimensions are (movies x features)
    + $Theta$: a matrix of feature weights: Dimensions are (users x features)

+ Compute the predicted movie ratings for all users using the product of $X$ and $Theta$. A transposition may be needed. Dimensions of the result should be (movies x users).
+ Compute the movie rating error by subtracting Y from the predicted ratings.
+ Compute the "error_factor" my multiplying the movie rating error by the R matrix. The error factor will be 0 for movies that a user has not rated. Use the type of multiplication by R (vector or element-wise) so the size of the error factor matrix remains unchanged (movies x users).

(Note: there is a quirk in the submit grader's test case that requires you to use the R matrix to ignore movies that have had no ratings).

#### Calculate the cost:

+ Using the formula on Page 9 of ex8.pdf, compute the unregularized cost as a scaled sum of the squares of all of the terms in error_factor. The result should be a scalar.
+ Test your code using ex8_cofi.m and the additional test cases. You should get a passing grade for this portion from the submit script.

#### Calculate the gradients (ref: the formulas on Page 10 of ex8,pdf):

+ The $X$ gradient is the product of the error factor and the Theta matrix. The sum is computed automatically by the vector multiplication. Dimensions are (movies x features)
+ The $Theta$ gradient is the product of the error factor and the X matrix. A transposition may be needed. The sum is computed automatically by the vector multiplication. Dimensions are (users x features)
+  Test your code, then submit this portion.

#### Calculate the regularized cost:

+ Using the formula on the top of Page 13 of ex8.pdf, compute the regularization term as the scaled sum of the squares of all terms in Theta and X. The result should be a scalar. Note that for Recommender Systems there are no bias terms, so regularization should include all columns of X and Theta.
+ Add the regularized and un-regularized cost terms.
+ Test your code, then submit this portion.

#### Calculate the gradient regularization terms (ref: the formulas in the middle of Page 13 of ex8.pdf)

+ The X gradient regularization is the X matrix scaled by lambda.
+ The Theta gradient regularization is the Theta matrix scaled by lambda.
+ Add the regularization terms to their unregularized values.
+ Test your code, then submit this portion.


### Programming Exercise 8

#### Debugging Tip

The submit script, for all the programming assignments, does not report the line number and location of the error when it crashes. The follow method can be used to make it do so which makes debugging easier.

Open ex8/lib/submitWithConfiguration.m and replace line:

```matlab
 fprintf('!! Please try again later.\n');
```

(around 28) with:

```matlab
fprintf('Error from file:%s\nFunction:%s\nOn line:%d\n', e.stack(1,1).file,e.stack(1,1).name, e.stack(1,1).line );
```

That top line says '!! Please try again later' on crash, instead of that, the bottom line will give the location and line number of the error. This change can be applied to all the programming assignments.

#### error in ex8_cofi.m (reported by Charles Davis in session ML-005)

line 199 in `ex8_cofi.m` reads

```matlab
theta = fmincg (@(t)(cofiCostFunc(t, Y, R, num_users, num_movies, num_features, lambda)), initial_parameters, options);
```

but I believe it should be

```matlab
theta = fmincg (@(t)(cofiCostFunc(t, Ynorm, R, num_users, num_movies, num_features, lambda)), initial_parameters, options);

```

...to avoid creating ratings > 5 at line 219. This doesn't affect the submissions of course, just the cosmetics of the recommendations.

Supporting analysis: Y is normalized in line 181, creating Ynorm, but then it is never used. The video lecture "Implementation Detail: Mean Normalization" at 5:34 makes it pretty clear that the normalized Y matrix should be used for calculating theta.

This errata also means that "ex8.pdf" Figure 4 is incorrect, since it shows movies with ratings greater than 5-stars.


#### Item 2: The grader uses Y with non-zero values

When using the R matrix (to ignore movies that have not been rated), do not rely on Y(i,j) to be 0 when a user has not rated a film. This expectation is true for the ex8_cofi.m script, but that is NOT true for the test case used by the submit grader for Part 3 through Part 6.

Note: This might no longer be true, the grader seems to be using Y(i,j) == 0 for when a user has not rated a film


#### Item 3: Regularization

Note: Unlike previous assignments when we performed regularization, for this exercise, we do NOT skip the 1st column of Theta or X when computing regularization. This is because we are not specifying bias units in the collaborative filtering algorithm (since the algorithm determines all of the theta values, it can set one to the '1' value if it leads to the optimum solution). Therefore, all values of Theta and X should be considered in regularization.

1.2 Estimating parameters for a Gaussian
the var function can actually return normalization with 1/m instead of 1/(m-1). Set the second argument 0 for 1/(m-1) and 1 for 1/m.


#### errors in cofiCostFunc.m

line 9 should read "% Unfold the X and Theta matrices from params"


### FAQ for Week 9 and programming exercise 8


#### FAQ for the video lectures

The Lecture Notes are available in the Resources menu.

The lecture slides are available in the Review section of each week of the course materials.


#### FAQ for the quiz

(blank)

#### FAQ for Programming Exercise 8

Note: Tutorials and additional Test Cases are available in the Resources menu.

__Q1) I get the correct results for the collaborative filtering exercise, but I don't get any points from the submit grader. Why?__

The tutorial for this exercise (via the Resources menu) tells about a characteristic of the submit grader's test case which your cofiCostFunc() code must cope with.

__Q2) Why don't my movie recommendations match Figure 4 in ex8.pdf?__

Figure 4 in ex8.pdf was created using a defective version of ex8_cofi.m. The error has been repaired, but ex8.pdf has not been re-generated yet.

Your results should look something like this, but the specific movies will vary due to the random initialization of X and Theta.

```matlab
Top recommendations for you:
Predicting rating 5.0 for movie Marlene Dietrich: Shadow and Light (1996)
Predicting rating 5.0 for movie Great Day in Harlem, A (1994)
Predicting rating 5.0 for movie Saint of Fort Washington, The (1993)
Predicting rating 5.0 for movie Aiqing wansui (1994)
Predicting rating 5.0 for movie Santa with Muscles (1996)
Predicting rating 5.0 for movie Star Kid (1997)
Predicting rating 5.0 for movie They Made Me a Criminal (1939)
Predicting rating 5.0 for movie Entertaining Angels: The Dorothy Day Story (1996)
Predicting rating 5.0 for movie Prefontaine (1997)
Predicting rating 5.0 for movie Someone Else's America (1995)
```




