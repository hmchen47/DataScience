# Programming Assignment: Regularized Linear Regression and Bias/Variance

## 1 Regularized Linear Regression

+ implement regularized linear regression to predict the amount of water flowing out of a dam using the change of water level in a reservoir

+ Observe diagnostics of debugging learning algorithms and examine the effects of bias v.s. variance


### 1.1 Visualizing the dataset

+ visualizing the dataset containing historical records on the change in the water level, $x$, and the amount of water flowing out of the dam, $y$

+ Three parties of dataset
    + a training set to learn: `X`, `y`
    + a cross validation set for determining the regularization parameter: `Xval`, `yval`
    + a test set for evaluating performance, "unseen" examples dueing training: `Xtest`, `ytest`

+ `ex5.m` plots the training data

+ implement linear regression and use to fit a straight line to the data and plot learning curve

+ implement polynomial regression to find a better fit to the data


### 1.2 Regularized linear regression cost function

+ Cost function of regularized linear regression

    $$J(\theta) = \dfrac{1}{2m} \left( \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 \right) + \dfrac{\lambda}{2m} \left( \sum_{j=1}^n \theta_j^2 \right)$$

    + $\lambda\;$: regularization parameter to control the degree of regularization [prevent from overfitting]
    + put a penalty on the overall cost $J$
    + $\theta\uparrow \implies J \uparrow$

+ Complete code in `linearRegCostFunction.m`
    + write a function to calculate the regularized linear regressioncost function
    + try vectorized than looped
    + `ex5.m` runs the cost function using theta initialized at `[1; 1]`


### 1.3 Regularized linear regression gradient

+ The partial derivatives of regularized linear regression's cost for $\theta_j$

  $$\dfrac{\partial J(\theta)}{\partial \theta_0}  = \begin{cases} \dfrac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} & \text{for } j = 0 \\\\ \dfrac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \dfrac{\lambda}{m} \theta_j & \text{for } j \geq 1 \end{cases}$$

+ Add code in `linearRegCost.m` to calculate the the gradient, `grad`

+ `ex.m` run the gradient code with theta initialized at `[1; 1]` and expect gradient of `[-15.30; 598.250]`


### 1.4 Fitting linear regression

+ `ex5.m` runs `trainLinearReg.m` to compute the optimal values of $\theta$, using `fminunc` to optimize the cost function

+ Set regularization $\lambda = 0$
    + current implementation of linear regression trying to fit a 2-dim $\theta$
    + regularization not incredibly helpful for a $\theta$ of such low dimension

+ Using polynomial regression with regularization

+ `ex5.m` plots the best fit line.
    + not a good fit due to ono-linear pattern
    + not always easy to visualize the data and model

## 2 Bias-variance

+ the bias-variance tradeoff: important concept in machine learning
    + nodels with high bias: not complex enough for the data and tend to underfit
    + models with high variance: overfit to the training data


### 2.1 Learning curves

+ implement code to generate the learning curve for debugging learning algorithms

+ learning curve: plot with training anc cross validation error as a function of training set size

+ fill `learningCurve.m` to return a vector of errors for the training set and cross validation set

+ Different training set size required to plot learning curve
    + To obtain different training set sizes, different subsets of the original training set $X$.
    + for a training set size of $i$, the first $i$ examples used, i.e., `X(1:i, :)` and `y(1:i, :)`

+ Use the `trainLinearReg` function to find $\theta$ parameters

+ After learning the $\theta$ parameters, compute the __error__ on the training and cross validation sets

+ The training error for a dataset

    $$J_{train}(\theta) = \dfrac{1}{m} \left[ \sum_{i=1}^{2m} (h_\theta(x^{(i)}) - y^{(i)})^2 \right]$$

    + the training error not include the regularization term

+ One way to compute the training error: compute the training error by the existing cost function and setting $\lambda = 0$ only

+ When computing the training error, compute it on the training subsets (i.e., `X(1:n, :)` and ``y(1:n, :)`) (instead of the entire training set)

+ For the cross validation error, compute it over the entire cross validation set.

+ Computed errors in the vectors: `error_train` and `error_val`

+ The plotted figure with train and cross validation errors are high.  It reflects a __high bias__ problem in the model.


## 3 Polynomial regression

+ Hypothesis of a polynomial regression

    $$\begin{array}{rcl} h_\theta(x) &=& \theta_0 + \theta_1 \ast (\text{water level}) + \theta_2 \ast (\text{water level})^2 + \cdots + \theta_p \ast (\text{water level})^p \\ &=& \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_p x_p \end{array}$$
    + Note that by defining $x_1 = (\text{water level}), x_2 = (\text{water level})^2, \ldots, x_p = (\text{water level})^p$
    + A linear regression model where the features are the various powers of the original value (waterLevel)

+ Add more features using the higher powers of the existing feature $x$ in the dataset

+ Complete code in `polyFeatures.m` so that the function maps the original training set $X$ of size $m \times 1$ into the higher powers

+ When a training set $X$ of size $m \times 1$ passed into the function and return a $m \times p$ matrix `X_poly`, where column 1 holds the original values of $X$, column 2 holds the values of `X.^2`, and column 3 holds the values of `X.^3`, and so on.

+ Now, the function maps features to a higher dimension, and `ex5.m` will apply it to the training set, the testset, and the cross validation set.


### 3.1 Learning Polynomial Regression

+ `ex5.m` using `polyFeatures.m` to train polynomial regression using the linear regression cost function.

+ Solve a linear regression optimization problem even with polynomial terms
    + the polynomial terms simply turn into features used for linear regression

+ feature normalization
    + using a polynomial of degree 8
    + no work well as the features badly scaled, e.g., $x = 40 \;\rightarrow\; x_8 = 40^8 = 6.5 \times 10^{12}$
    + feature normalization required

+ `ex5.m` calls `featureNormalize` and normalize the features of the training set, storing `mu`, `sigma` parameters separately

+ `ex5.m` generates two plots for polynomial regression with $\lambda = 0$
    + polynomial fit:
        + following data points very well -> low training error
        + very complex
        + even drop off at the extremes
        + indicating overfitting the training data
    + learning curve
        + low training error
        + high cross validation error
        + gap between the training and cross validation errors indicating high variance problem
    + Overcome the overfitting (high-variance) problem: add regularization to the model


### 3.2 Adjusting the regularization parameter (Optional)

+ observe how the regularization parameter affects the bias-variance of regularized polynomial regression

+ modify the $\lambda$ parameter in `ex5.m`
    + $\lambda = 1, 100$
    + plot polynomial learning curve and polynomial fit


### 3.3 Selecting $\lambda$ using a cross validation set

+ observed that the value of $\lambda$ significantly affect rge results of regularized polynomial regression on the training and cross validation set

+ a model without regularization ($\lambda = 0$) fits the training set well, but does not generalize

+ a model with too much regularization ($\lambda = 100$)  not fit the training set and testing set well

+ a good choice of $\lambda$ provide a good fit to the data.

+ a good choice of $\lambda$ can provide a good fit tot he data.

+ implement an automated method to select the $\lambda$ parameter

+ use a cross validation set to evaluate how good each $\lambda$ value is

+ after selecting the best $\lambda$ value using the cross validation set, evaluate the model on the test set to estimate how well the model will perform on actual unseen data

+ complete the code in `validateCurve.m` 
    + using `trainLinearReg` function to train the model using different values of $\lambda$
    + compute the training error and cross validation error 
    + try $\lambda \in \{0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10 \}$

+ run `ex5.m` to plot a cross validation curve of error v.s. $\lambda$ to select which $\lambda$ parameter to use


### 3.4 Computing test set error (Optional)

+ to get better indication of the model's performance in the real world, it is important to evaluate the "final" model on a test set that was not used in any part of training

+ compute the test error using the best value of $\lambda$ found: test erro = $3.8599$ for $\lambda = 3$


### 3.5 Plotting learning curves with randomly selected (Optional)

+ With small training set to plot learning curves for debugging learning algorithms, a helpful way is to average cross multiple sets of randomly selected examples to determine the training error and cross validation error

+ To determine the training error and cross validation error for $i$ examples
    + randmly select $i$ examples from the training set and $i$ examples from cross validation set
    + learn the parameters $\theta$ using the randomly choosen training set
    + evaluate the parameters $\theta$ on the randomly chosen training set and cross validation set
    + repeat the above steps multiple time (said 50)
    + average the errors to determine the training error and cross validation error for $i$ examples

+ Implement the above strategy for computing the learning curves


## Programming Ex.5

Proposed erratum: the Optional exercise (Section 3.5) instructs you to select i examples from the cross-validation set. Shouldn't you always validate on the full cross-validation set as in section 2.1?

Other miscellany:

shouldn't it be "vs." instead of "v.s."?
p. 3 "overal"
p. 6 "wil"
p. 7 "For use polynomial regression [sic]"
p. 7 "zero-eth" - shouldn't this be "zero-th"?
p. 9 "where the low training error is low [sic]"

#### Debugging Tip

The submit script, for all the programming assignments, does not report the line number and location of the error when it crashes. The follow method can be used to make it do so which makes debugging easier.

Open ex5/lib/submitWithConfiguration.m and replace line:

```matlab
 fprintf('!! Please try again later.\n');
```

(around 28) with:

```matlab
fprintf('Error from file:%s\nFunction:%s\nOn line:%d\n', e.stack(1,1).file,e.stack(1,1).name, e.stack(1,1).line );
```

That top line says '!! Please try again later' on crash, instead of that, the bottom line will give the location and line number of the error. This change can be applied to all the programming assignments.


### Ex5 Tutorials

#### [ex5 tutorial linearRegCostFunction](https://www.coursera.org/learn/machine-learning/discussions/all/threads/UAv1DB62EeWd3iIAC7VAtA)

Here is a brief tutorial for the `linearRegCostFunction()`.

We last did a linear regression exercise back in ex1, so start with these two tutorials for computeCost() and gradientDescent(). Since they're vectorized, they work equally well for any multiple-variable linear regression.

[computeCost tutorial](https://www.coursera.org/learn/machine-learning/discussions/t35D1xn3EeWA7CIAC5WDNQ)

[gradientDescent tutorial](https://www.coursera.org/learn/machine-learning/discussions/-m2ng_KQEeSUBCIAC9QURQ)

You only need the first three steps of the gradientDescent() tutorial, plus scaling by 1/m (ignore the 'alpha' variable, it is not used in this exercise). That's gives us the gradient. Since we let fmincg() perform gradient descent for us, we just have to compute the cost and gradient. We don't use a for-loop over the number of iterations, or use any learning rate. The fmincg() function does that for us.

So now you've got unregularized cost J, and unregularized gradient 'grad'.

For the cost regularization:

+ Set theta(1) to 0.
+ Compute the sum of all of the theta values squared. One handy way to do this is sum(theta.^2). Since theta(1) has been forced to zero, it doesn't add to the regularization term.
+ Now scale this value by lambda / (2*m), and add it to the unregularized cost.


For the gradient regularization:

+ The regularized gradient term is theta scaled by (lambda / m). Again, since theta(1) has been set to zero, it does not contribute to the regularization term.
+ Add this vector to the unregularized portion.


That's it. Here is a [test case](https://www.coursera.org/learn/machine-learning/discussions/O25D0QykEeWZSyIAC5bWOg) for this function


Other posts:

+ The unregularized part of cost is 1.3533.
+ The regularized part of the cost is 0.33833.
+ Use `sum()` when you use element-wise multiplication. <br/>
    When you use a vector multiplication, the sum is included automatically.
+ Perhaps compare your code with what is in [the tutorial](https://www.coursera.org/learn/machine-learning/discussions/m0ZdvjSrEeWddiIAC9pDDA) for this exercise
+ That use of "costFunction" is done using the "anonymous function" method. It is really more of a function pointer than a real function. <br/> In ex5, the function "trainLinearReg()" handles this for you. <br/> Open up trainLinearReg.m in a text editor, and look at lines 13 through 19. You don't have to change anything here. trainLinearReg() calls `fmincg()` using your cost function. <br/>Your learningCurve() and validationCurve() functions just call trainLinearReg() to get the job done.


#### [Tutorial for polyFeatures()](https://www.coursera.org/learn/machine-learning/discussions/weeks/6/threads/YbO2RaVGEeaCbg44JUM1Vg)

There are a couple of different methods that work for the `polyFeatures()` function.

One is to use the `bsxfun()` function, with the @power operator, like this:

```matlab
X_poly = bsxfun(@power, vector1, vector2)
```

... where vector1 is a column vector of the feature values 'X', and vector2 is a row vector of exponents from 1 to 'p'.

Other options involve using the element-wise exponent operator '.^', and converting both X and the vector of exponent values into equal-sized matrices by multiplying each by a vectors of all-ones.


#### [ex5: tips for learningCurve()](https://www.coursera.org/learn/machine-learning/discussions/weeks/6/threads/Y_DZmpkgEeWNbBIwwhtGwQ)

This thread is the tutorial for the `learningCurve()` function.

The thread is closed to comments (to prevent issues with the Forum software over time). If you have questions, please post them in a new thread.

------------------------

Note: Almost all of the code you need for this function is provided in the code examples and hints in the `learningCurve.m` script.

Step 1) Use a for-loop to iterate over the length of the training set. The "Hint" in `learningCurve.m` gives you the code to use.

Step 2) Create a subset of the "X" matrix and the 'y' vector, using the elements 1 through 'i'. The first "Note" in `learningCurve.m` gives you the code to use. This causes the training set size to increase by one for each iteration through the training set. You will use this subset for training (Step 3) and measuring the training set error (Step 4).

Step 3) Use the trainLinearReg() function to learn the theta vector for the current size of training set (see page 6 of ex5.pdf).

Step 4) Then use your cost function to compute the training set error. Do not include regularization. Store the training set cost in error_train(i).

Step 5) Then use your cost function to compute the validation set error, using Xval and yval. Do not include regularization. Do not create any subsets of the validation set. Store the validation set error in error_val(i).

Tips:

+ Use the lambda parameter - from the `learningCurve()` parameter list - every time you call trainLinearReg().
+ __do not__ set lambda = 0 inside the `learningCurve()` function. You are going to experiment with different lambda values in `ex5.m`, and the submit grader doesn't use lambda = 0. So do not hard-code lambda = 0 inside the `learningCurve()` function.
+ When you compute the training set error and the validation set error, use your cost function with a zero for the lambda parameter. We want to measure the error in the hypothesis, without including any additional penalties for the theta values.
+ When you run the `ex5` script, you may get some "divide by zero" warnings. These are expected and normal. `fmincg()` generates "divide by zero" warnings whenever the training set has only one or two examples. Do not worry about it.


#### [Validation curve question](https://www.coursera.org/learn/machine-learning/discussions/all/threads/AdGhzAX1EeWyEyIAC7PmUA/replies/7XjBAQ-MEeWUtiIAC9TNkg?page=2)

For `validationCurve()`, you always use the entire training set, and the entire validation set. The only item you are varying is the value of lambda when you compute theta on the training set.

Also, do not use regularization when measuring the training error and the validation error.


#### [FAQ for Week 6 and programming exercise 5](https://www.coursera.org/learn/machine-learning/discussions/weeks/6/threads/P3Cp9j_ZEeaDRA5SxbW7qQ)

##### VIDEO LECTURE FAQ

1. How can I get the lecture notes and slides?

    Lecture Notes are available in the Resources menu.

    The lecture slides are now available in the "Review" section of each week's course materials.

2. The lecture "Error metrics for Skewed Classes" doesn't give the definition of Accuracy.

  Here it is: Accuracy = (tp + tn) / total population, or:

  $$\text{Accuracy} = \dfrac{tp+ tn}{tp+tn+fp+fn}$$


##### QUIZ FAQ

1. In the quiz for "Machine Learning System Design", I can't get the right answer for the F1 score.

    Be sure you use the '.' character for the decimal point, not a comma ','.

    Round your answer to two digits of accuracy.

2. I can't get the correct answers for "Machine Learning System Design" Question 4 or Question 5.

Hint: consider the training and cross-validation sets as having identical performance. Most of the other true/false answers are taken directly from the video lectures.

##### PROGRAMMING ASSIGNMENT FAQ

Note: Tutorials and additional Test Cases are available in the Resources menu.

1. For `learningCurve()` and `validationCurve()`, why don't we include regularization when computing $J_{train}$, $J_{cv}$, and $J_{test}$?

    Regularization is built-in to theta when you train the system. We do not need to include it twice.

    When we measure Jtrain, Jcv, and Jtest, we want to measure the true error, without any additional penalties.

    Tip: when computing Jtrain and Jcv, do not ever use the line of code "lambda = 0". Doing that will erase the value of lambda that the submit grader is using to check your code.

2. My learning curve plot looks exactly like the one in ex5.pdf. Why doesn't the submit grader give me points?

    + Use the the additional test case (from the Resources menu) to see if your cost function gives the correct results when there is only one training example.
    + The ex5.m script uses lambda = 0. But the submit script uses non-zero values for lambda. Use the additional test cases to check whether your learningCurve() handles non-zero lambda values correctly. Read the tutorial for more tips.
    + Do not set "lambda = 0" inside any of your functions. You can pass a '0' as the value of the lambda parameter, but don't change the value of lambda itself.

3. How do I compute $J_{test}$ for the optional Section 3.4?

    The ex5.m script creates the test set for you.

    + Train on X_poly and y using the best lambda found from Section 3.3.
    + Measure Jtest on X_poly_test and ytest, without regularization

4. Why do we use all of the CV set in the learning curve and validation curve, but only a subset of the CV set for the optional Section 3.5?

    In section 3.2 and 3.3, we use the whole CV set because we are only making one measurement. Using the entire CV set gives us the most general Jcv estimate.

    In section 3.5, we are going to average Jcv over multiple random selections, so the generalization comes through averaging. There would be little added benefit from using the entire CV set.

5. What does Prof Ng mean when he says we're "fitting another parameter 'd' to the CV set"?

    That's not actually what we're doing. He suggests adjusting the model based on the Jcv value. But there is a problem.

    Each set of data can only be used for one purpose. If you use the CV set to adjust the regularization (the validation curve process), then you cannot also use the CV set to select the best polynomial degree. This would result in overfitting the CV set.

    You could use a second validation set to select the best polynomial degree. The exercise does not require you to do this.

    A second method is possible: You can create all possible combinations of the parameters you're varying, and evaluate all the combinations using only one validation set. So, if you have six possible values for lambda and seven values for the polynomial degree, you'd have 42 models to evaluate. Select the combination that gives the lowest validation set cost (or error). Note: You'll use exactly this method in Week 7 (ex6) for optimizing the SVM method.

6. Why does my code for learningCurve() generate "divide by zero" warnings?

    This is normal behavior from fmincg() when there are only one or two examples in the training set. It is not a problem - the language handles these sorts of math issues gracefully, it is only a warning, not an error, and you do not need to fix it.

7. My "Fig. 4" doesn't look like the one in ex5.pdf. Is that a problem?

    Your Fig. 4 may look like this:

    <div style="display:flex;justify-content:center;align-items:center;flex-flow:row wrap;">
    <div><a href="https://www.coursera.org/learn/machine-learning/discussions/weeks/6/threads/P3Cp9j_ZEeaDRA5SxbW7qQ">
        <img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/IGxu2W_wEeakkArq5_q3Ow_616947332b95065e457654999c2c561e_ex5_fig_alt.PNG?expiry=1555200000000&hmac=s1VAwCLNTimtg5HlRwOlNwCFue7I_KLtLPRZEkL8cUc" style="margin: 0.1em;" alt="text" title="caption" width="250">
    </a></div>
    </div>

    If so, that's OK. The exact values in the tails of the curve (outside of the training range) depend on the details of your implementation, whether you're using MATLAB or Octave, and what math library it includes.

8. How do I fix this error message?

    "Operands to the || and && operators must be convertible to logical scalar values.

    Error in fmincg (line 95)

    while ((f2 > f1+z1*RHO*d1) || (d2 > -SIG*d1)) && (M > 0)"

    Answer: Your cost function must return J as a scalar, and grad as a column vector.

9. How do I get started on random learning curve (optional section 3.5)?

    Here is the general process for implementation:

    + Make a copy of your learningCurve.m script. Call it "learningCurveRandom.m". You need to also modify the function title so it matches the script file name.
    + Inside this function, within the "for i=1:m" loop, create a new "inner" for loop from 1 to 50.
    + Inside this inner loop, you have to randomly select 'i' examples from the training set, and 'i' examples from the validation set. The range of the examples for the training set is 'size(X,1), and the range of the examples for the validation set is 'size(Xval,1'). You must use two separate random permutations for this. The "randperm()" function comes in handy here.
    + Just as in the learningCurve() function, you train with the training set and measure Jtrain and Jcv. Save each of these 50 values into separate vectors. You will need to zero-out these vectors each time you iterate to a new value of 'i'.
    + Outside of the 50-iteration loop, you save error_train(i) and error_val(i) as the mean of those vectors.

    Here is the overall structure:

    ```matlab
    % you need these values later
    m = size(X,1)     % the number of training examples
    r = size(Xval,1)  % the number of validation examples

    for i = 1:m
    % create two empty vectors for the Jtrain and Jcv values
    for j = 1:50
        % use 'm' to select 'i' random examples from the training set
        % use 'r' to select 'i' random examples from the validation set
        % compute theta
        % compute Jtrain and Jcv and save the values
    end
    % compute the mean of the Jtrain vector and save it in error_train(i)
    % compute the mean of the Jcv vector and save it in error_val(i)
    end
    ```

    For testing, add some code to ex5.m that calls your learningCurveRandom() function, and plots the results.

10. What are the correct results for the learningCurve() test in ex5.m "Part 5 - linear regression" with lambda = 0?

    Note: See also Q12.

    ```matlab
    # Training Examples     Train Error     Cross Validation Error
            1               0.000000        205.121096
            2               0.000000        110.300366
            3               3.286595        45.010231
            4               2.842678        48.368911
            5               13.154049       35.865165
            6               19.443963       33.829962
            7               20.098522       31.970986
            8               18.172859       30.862446
            9               22.609405       31.135998
            10              23.261462       28.936207
            11              24.317250       29.551432
            12              22.373906       29.433818
    ```

11. What are the correct results for the learningCurve() test in ex5.m "Part 7 - polynomial regression" with lambda = 0?

    Note: See also Q12.

    ```matlab
    # Training Examples     Train Error     Cross Validation Error
            1               0.000000        160.721900
            2               0.000000        160.121510
            3               0.000000        61.754825
            4               0.000000        61.928895
            5               0.000000        6.597447
            6               0.003103        9.657370
            7               0.027029        10.547395
            8               0.062886        8.043061
            9               0.177758        7.509234
            10              0.162784        8.959875
            11              0.089135        7.769895
            12              0.045197        14.116201
    ```

12. I got the correct values shown in FAQ Q10 and Q11. Why does "submit" fail?

    The grader uses a nonzero lambda. Use the Test Cases.


14. Some of my values from Q10 and Q11 are not exactly correct, but the grader says my code is correct. Why?

    I think this is a numerical accuracy issue, caused because the ex5 exercise is rather poorly designed.

    + ex5 uses an unnecessarily large value for the polynomial degree.
    + The training set is extremely small, so tiny variations in the normalized values can show up as differences in the gradients, which impacts the theta values and therefore the J_train and J_cv cost values.
    + The different math libraries used in MATLAB and Octave (and in different versions of those tools) can give slightly different results.

    In summary: If your code passes the grader, and passes the additional Test Cases, don't worry about the values shown in this FAQ.

15. Why does overfitting make the validation set cost increase?

    Overfitting causes the cost on the training set to decrease, but the cost on the validation set to increase.

    Here is a simple example. Let's say the entire data set is four points. They are not co-linear.

    Let's take three of the points as the training set, and let's set the polynomial degree to 2.

    That means the hypothesis will be a parabola, and as we know from geometry, any three points in a plane can be perfectly fit by a parabola.

    So the training cost will be zero.

    Now let's take that fourth point as the validation set.

    It is not required to be on the parabola, since it was not in the training set. So the validation set cost (using theta from the training set) will be non-zero.

    That's overfitting.

16. In Optional section 3.5, why don't we use the entire validation set every time?

    We're going to average together the results from 50 experiments. We don't need to use the whole validation set each time. Using a subset saves some execution time.










