# Programming Assignment: Regularized Linear Regression and Bias/Variance

### 1 Regularized Linear Regression

+ implement regularized linear regression to predict the amount of water flowing out of a dam using the change of water level in a reservoir

+ Observe diagnostics of debugging learning algorithms and examine the effects of bias v.v. variance


### 1.1 Visualizing the dataset

+ visualizing the dataset containing historical records on the change in the water level, $x$, and the amount of water flowing out of the dam, $y$

+ Three parties of dataset
    + a training set to learn: `X`, `y`
    + a cross validation set for determining the regularization parameter: `Xval`, `yval`
    + a test set for evaluating performance, "unseen" examples dueing training: `Xtest`, `ytest`

+ `ex5.m` plots the training data

+ implement linear regression and use to fit a straight line to the data and plot learning curve

+ implement polynomial regression to find a better fit to the data


### 1.2 Regularized linear regression cost function

+ Cost function of regularized linear regression

    $$J(\theta) = \dfrac{1}{2m} \left( \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 \right) + \dfrac{\lambda}{2m} \left( \sum_{j=1}^n \theta_j^2 \right)$$

    + $\lambda\;$: regularization parameter to control the degree of regularization [prevent from overfitting]
    + put a penalty on the overall cost $J$
    + $\theta\uparrow \implies J \uparrow$

+ Complete code in `linearRegCostFunction.m`
    + write a function to calculate the regularized linear regressioncost function
    + try vectorized than looped
    + `ex5.m` runs the cost function using theta initialized at `[1; 1]`


### 1.3 Regularized linear regression gradient

+ The partial derivatives of regularized linear regression's cost for $\theta_j$

    $$\dfrac{\partial J(\theta)}{\partial \theta_0}  = \begin{cases} \dfrac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} & \text{for } j = 0 \\\\ \dfrac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \dfrac{\lambda}{m} \theta_j & \text{for } j \geq 1 \end{cases}$$

+ Add code in `linearRegCost.m` to calculate the the gradient, `grad`

+ `ex.m` run the gradient code with theta initialized at `[1; 1]` and expect gradient of `[-15.30; 598.250]`


### 1.4 Fitting linear regression

+ `ex5.m` runs `trainLinearReg.m` to compute the optimal values of $\theta$, using `fminunc` to optimize the cost function

+ Set regularization $\lambda = 0$
    + current implementation of linear regression trying to fit a 2-dim $\theta$
    + regularization not incredibly helpful for a $\theta$ of such low dimension

+ Using polynomial regression with regularization

+ `ex5.m` plots the best fit line.
    + not a good fit due to ono-linear pattern
    + not always easy to visualize the data and model



### 2 Bias-variance

+ the bias-variance tradeoff: important concept in machine learning
    + nodels with high bias: not complex enough for the data and tend to underfit
    + models with high variance: overfit to the training data


### 2.1 Learning curves

+ implement code to generate the learning curve for debugging learning algorithms

+ learning curve: plot with training anc cross validation error as a function of training set size

+ fill `learningCurve.m` to return a vector of errors for the training set and cross validation set

+ Different training set size required to plot learning curve
    + To obtain different training set sizes, different subsets of the original training set $X$.
    + for a training set size of $i$, the first $i$ examples used, i.e., `X(1:i, :)` and `y(1:i, :)`

+ Use the `trainLinearReg` function to find $\theta$ parameters

+ After learning the $\theta$ parameters, compute the __error__ on the training and cross validation sets

+ The training error for a dataset

    $$J_{train}(\theta) = \dfrac{1}{m} \left[ (h_\theta(x^{(i)}) - y^{(i)})^2 \right]$$

    + the training error not include the regularization term

+ One way to compute the training error: compute the training error by the existing cost function and setting $\lambda = 0$ only

+ When computing the training error, compute it on the training subsets (i.e., `X(1:n, :)` and ``y(1:n, :)`) (instead of the entire training set)

+ For the cross validation error, compute it over the entire cross validation set.

+ Computed errors in the vectors: `error_train` and `error_val`

+ The plotted figure with train and cross validation errors are high.  It reflects a __high bias__ problem in the model.


### 3 Polynomial regression

+ Hypothesis of a polynomial regression

    $$\begin{array}{rcl} h_\theta(x) &=& \theta_0 + \theta_1 \ast (\text{water level}) + \theta_2 \ast (\text{water level})^2 + \cdots + \theta_p \ast (\text{water level})^p \\ &=& \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_p x_p \end{array}$$
    + Note that by defining $x_1 = (\text{water level}), x_2 = (\text{water level})^2, \ldots, x_p = (\text{water level})^p$
    + A linear regression model where the features are the various powers of the original value (waterLevel)

+ Add more features using the higher powers of the existing feature $x$ in the dataset

+ Complete code in `polyFeatures.m` so that the function maps the original training set $X$ of size $m \time 1$ into the higher powers

+ When a training set $X$ of size $m \times 1$ passed into the function and return a $m \times p$ matrix `X_poly`, where column 1 holds the original values of $X$, column 2 holds the values of `X.^2`, and column 3 holds the values of `X.^3`, and so on.

+ Now, the function maps features to a higher dimension, and `ex5.m` will apply it to the training set, the testset, and the cross validation set.


### 3.1 Learning Polynomial Regression

+ `ex5.m` using `polyFeatures.m` to train polynomial regression using the linear regression cost function.

+ Solve a linear regression optimization problem even with polynomial terms
    + the polynomial terms simply turn into features used for linear regression

+ feature normalization
    + using a polynomial of degree 8
    + no work well as the features badly scaled, e.g., $x = 40 \;\rightarrow\; x_8 = 40^8 = 6.5 \times 10^{12}$
    + feature normalization required

+ `ex5.m` calls `featureNormalize` and normalize the features of the training set, storing `mu`, `sigma` parameters separately

+ `ex5.m` generates two plots for polynomial regression with $\almbda = 0$
    + polynomial fit:
        + following data points very well -> low training error
        + very complex
        + even drop off at the extremes
        + indicating overfitting the training data
    + learning curve
        + low training error
        + high cross validation error
        + gap between the training and cross validation errors indicating high variance problem
    + Overcome the overfitting (high-variance) problem: add regularization to the model


### 3.2 Adjusting the regularization parameter (Optional)

+ observe how the regularization parameter affects the bias-variance of regularized polynomial regression

+ modify the $\lambda$ parameter in `ex5.m`
    + $\lambda = 1, 100$
    + plot polynomial learning curve and polynomial fit


### 3.3 Selecting $\lambda$ using a cross validation set



### 3.4 Computing test set error (Optional)



### 3.5 Plotting learning curves with randomly selected (Optional)



### Programming Ex.5

Proposed erratum: the Optional exercise (Section 3.5) instructs you to select i examples from the cross-validation set. Shouldn't you always validate on the full cross-validation set as in section 2.1?

Other miscellany:

shouldn't it be "vs." instead of "v.s."?
p. 3 "overal"
p. 6 "wil"
p. 7 "For use polynomial regression [sic]"
p. 7 "zero-eth" - shouldn't this be "zero-th"?
p. 9 "where the low training error is low [sic]"

#### Debugging Tip

The submit script, for all the programming assignments, does not report the line number and location of the error when it crashes. The follow method can be used to make it do so which makes debugging easier.

Open ex5/lib/submitWithConfiguration.m and replace line:

```matlab
 fprintf('!! Please try again later.\n');
```

(around 28) with:

```matlab
fprintf('Error from file:%s\nFunction:%s\nOn line:%d\n', e.stack(1,1).file,e.stack(1,1).name, e.stack(1,1).line );
```

That top line says '!! Please try again later' on crash, instead of that, the bottom line will give the location and line number of the error. This change can be applied to all the programming assignments.


### Ex5 Tutorials

#### [ex5 tutorial linearRegCostFunction](https://www.coursera.org/learn/machine-learning/discussions/all/threads/UAv1DB62EeWd3iIAC7VAtA)

Here is a brief tutorial for the `linearRegCostFunction()`.

We last did a linear regression exercise back in ex1, so start with these two tutorials for computeCost() and gradientDescent(). Since they're vectorized, they work equally well for any multiple-variable linear regression.

[computeCost tutorial](https://www.coursera.org/learn/machine-learning/discussions/t35D1xn3EeWA7CIAC5WDNQ)

[gradientDescent tutorial](https://www.coursera.org/learn/machine-learning/discussions/-m2ng_KQEeSUBCIAC9QURQ)

You only need the first three steps of the gradientDescent() tutorial, plus scaling by 1/m (ignore the 'alpha' variable, it is not used in this exercise). That's gives us the gradient. Since we let fmincg() perform gradient descent for us, we just have to compute the cost and gradient. We don't use a for-loop over the number of iterations, or use any learning rate. The fmincg() function does that for us.

So now you've got unregularized cost J, and unregularized gradient 'grad'.

For the cost regularization:

+ Set theta(1) to 0.
+ Compute the sum of all of the theta values squared. One handy way to do this is sum(theta.^2). Since theta(1) has been forced to zero, it doesn't add to the regularization term.
+ Now scale this value by lambda / (2*m), and add it to the unregularized cost.


For the gradient regularization:

+ The regularized gradient term is theta scaled by (lambda / m). Again, since theta(1) has been set to zero, it does not contribute to the regularization term.
+ Add this vector to the unregularized portion.


That's it. Here is a [test case](https://www.coursera.org/learn/machine-learning/discussions/O25D0QykEeWZSyIAC5bWOg) for this function


Other posts:

+ The unregularized part of cost is 1.3533.
+ The regularized part of the cost is 0.33833.
+ Use `sum()` when you use element-wise multiplication. <br/>
    When you use a vector multiplication, the sum is included automatically.
+ Perhaps compare your code with what is in [the tutorial](https://www.coursera.org/learn/machine-learning/discussions/m0ZdvjSrEeWddiIAC9pDDA) for this exercise
+ That use of "costFunction" is done using the "anonymous function" method. It is really more of a function pointer than a real function. <br/> In ex5, the function "trainLinearReg()" handles this for you. <br/> Open up trainLinearReg.m in a text editor, and look at lines 13 through 19. You don't have to change anything here. trainLinearReg() calls `fmincg()` using your cost function. <br/>Your learningCurve() and validationCurve() functions just call trainLinearReg() to get the job done.


#### [Tutorial for polyFeatures()](https://www.coursera.org/learn/machine-learning/discussions/weeks/6/threads/YbO2RaVGEeaCbg44JUM1Vg)

There are a couple of different methods that work for the `polyFeatures()` function.

One is to use the `bsxfun()` function, with the @power operator, like this:

```matlab
X_poly = bsxfun(@power, vector1, vector2)
```

... where vector1 is a column vector of the feature values 'X', and vector2 is a row vector of exponents from 1 to 'p'.

Other options involve using the element-wise exponent operator '.^', and converting both X and the vector of exponent values into equal-sized matrices by multiplying each by a vectors of all-ones.


#### [ex5: tips for learningCurve()](https://www.coursera.org/learn/machine-learning/discussions/weeks/6/threads/Y_DZmpkgEeWNbBIwwhtGwQ)

This thread is the tutorial for the `learningCurve()` function.

The thread is closed to comments (to prevent issues with the Forum software over time). If you have questions, please post them in a new thread.

------------------------

Note: Almost all of the code you need for this function is provided in the code examples and hints in the `learningCurve.m` script.

Step 1) Use a for-loop to iterate over the length of the training set. The "Hint" in `learningCurve.m` gives you the code to use.

Step 2) Create a subset of the "X" matrix and the 'y' vector, using the elements 1 through 'i'. The first "Note" in `learningCurve.m` gives you the code to use. This causes the training set size to increase by one for each iteration through the training set. You will use this subset for training (Step 3) and measuring the training set error (Step 4).

Step 3) Use the trainLinearReg() function to learn the theta vector for the current size of training set (see page 6 of ex5.pdf).

Step 4) Then use your cost function to compute the training set error. Do not include regularization. Store the training set cost in error_train(i).

Step 5) Then use your cost function to compute the validation set error, using Xval and yval. Do not include regularization. Do not create any subsets of the validation set. Store the validation set error in error_val(i).

Tips:

+ Use the lambda parameter - from the `learningCurve()` parameter list - every time you call trainLinearReg().
+ __do not__ set lambda = 0 inside the `learningCurve()` function. You are going to experiment with different lambda values in `ex5.m`, and the submit grader doesn't use lambda = 0. So do not hard-code lambda = 0 inside the `learningCurve()` function.
+ When you compute the training set error and the validation set error, use your cost function with a zero for the lambda parameter. We want to measure the error in the hypothesis, without including any additional penalties for the theta values.
+ When you run the `ex5` script, you may get some "divide by zero" warnings. These are expected and normal. `fmincg()` generates "divide by zero" warnings whenever the training set has only one or two examples. Do not worry about it.


#### [Validation curve question](https://www.coursera.org/learn/machine-learning/discussions/all/threads/AdGhzAX1EeWyEyIAC7PmUA/replies/7XjBAQ-MEeWUtiIAC9TNkg?page=2)

For `validationCurve()`, you always use the entire training set, and the entire validation set. The only item you are varying is the value of lambda when you compute theta on the training set.

Also, do not use regularization when measuring the training error and the validation error.












