{"cells":[{"cell_type":"markdown","metadata":{},"source":["**This notebook is an exercise in the [Feature Engineering](https://www.kaggle.com/learn/feature-engineering) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/principal-component-analysis).**\n","\n","---\n"]},{"cell_type":"markdown","metadata":{},"source":["# Introduction #\n","\n","In this exercise, you'll work through several applications of PCA to the [*Ames*](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) dataset."]},{"cell_type":"markdown","metadata":{},"source":["Run this cell to set everything up!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Setup feedback system\n","from learntools.core import binder\n","binder.bind(globals())\n","from learntools.feature_engineering_new.ex5 import *\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from sklearn.decomposition import PCA\n","from sklearn.feature_selection import mutual_info_regression\n","from sklearn.model_selection import cross_val_score\n","from xgboost import XGBRegressor\n","\n","# Set Matplotlib defaults\n","plt.style.use(\"seaborn-whitegrid\")\n","plt.rc(\"figure\", autolayout=True)\n","plt.rc(\n","    \"axes\",\n","    labelweight=\"bold\",\n","    labelsize=\"large\",\n","    titleweight=\"bold\",\n","    titlesize=14,\n","    titlepad=10,\n",")\n","\n","\n","def apply_pca(X, standardize=True):\n","    # Standardize\n","    if standardize:\n","        X = (X - X.mean(axis=0)) / X.std(axis=0)\n","    # Create principal components\n","    pca = PCA()\n","    X_pca = pca.fit_transform(X)\n","    # Convert to dataframe\n","    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n","    X_pca = pd.DataFrame(X_pca, columns=component_names)\n","    # Create loadings\n","    loadings = pd.DataFrame(\n","        pca.components_.T,  # transpose the matrix of loadings\n","        columns=component_names,  # so the columns are the principal components\n","        index=X.columns,  # and the rows are the original features\n","    )\n","    return pca, X_pca, loadings\n","\n","\n","def plot_variance(pca, width=8, dpi=100):\n","    # Create figure\n","    fig, axs = plt.subplots(1, 2)\n","    n = pca.n_components_\n","    grid = np.arange(1, n + 1)\n","    # Explained variance\n","    evr = pca.explained_variance_ratio_\n","    axs[0].bar(grid, evr)\n","    axs[0].set(\n","        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n","    )\n","    # Cumulative Variance\n","    cv = np.cumsum(evr)\n","    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n","    axs[1].set(\n","        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n","    )\n","    # Set up figure\n","    fig.set(figwidth=8, dpi=100)\n","    return axs\n","\n","\n","def make_mi_scores(X, y):\n","    X = X.copy()\n","    for colname in X.select_dtypes([\"object\", \"category\"]):\n","        X[colname], _ = X[colname].factorize()\n","    # All discrete features should now have integer dtypes\n","    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n","    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n","    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n","    mi_scores = mi_scores.sort_values(ascending=False)\n","    return mi_scores\n","\n","\n","def score_dataset(X, y, model=XGBRegressor()):\n","    # Label encoding for categoricals\n","    for colname in X.select_dtypes([\"category\", \"object\"]):\n","        X[colname], _ = X[colname].factorize()\n","    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n","    score = cross_val_score(\n","        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n","    )\n","    score = -1 * score.mean()\n","    score = np.sqrt(score)\n","    return score\n","\n","\n","df = pd.read_csv(\"../input/fe-course-data/ames.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's choose a few features that are highly correlated with our target, `SalePrice`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features = [\n","    \"GarageArea\",\n","    \"YearRemodAdd\",\n","    \"TotalBsmtSF\",\n","    \"GrLivArea\",\n","]\n","\n","print(\"Correlation with SalePrice:\\n\")\n","print(df[features].corrwith(df.SalePrice))"]},{"cell_type":"markdown","metadata":{},"source":["We'll rely on PCA to untangle the correlational structure of these features and suggest relationships that might be usefully modeled with new features.\n","\n","Run this cell to apply PCA and extract the loadings."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = df.copy()\n","y = X.pop(\"SalePrice\")\n","X = X.loc[:, features]\n","\n","# `apply_pca`, defined above, reproduces the code from the tutorial\n","pca, X_pca, loadings = apply_pca(X)\n","print(loadings)"]},{"cell_type":"markdown","metadata":{},"source":["# 1) Interpret Component Loadings\n","\n","Look at the loadings for components `PC1` and `PC3`. Can you think of a description of what kind of contrast each component has captured? After you've thought about it, run the next cell for a solution."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# View the solution (Run this cell to receive credit!)\n","q_1.check()"]},{"cell_type":"markdown","metadata":{},"source":["-------------------------------------------------------------------------------\n","\n","Your goal in this question is to use the results of PCA to discover one or more new features that improve the performance of your model. One option is to create features inspired by the loadings, like we did in the tutorial. Another option is to use the components themselves as features (that is, add one or more columns of `X_pca` to `X`).\n","\n","# 2) Create New Features\n","\n","Add one or more new features to the dataset `X`. For a correct solution, get a validation score below 0.140 RMSLE. (If you get stuck, feel free to use the `hint` below!)"]},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":0},"outputs":[],"source":["X = df.copy()\n","y = X.pop(\"SalePrice\")\n","\n","# YOUR CODE HERE: Add new features to X.\n","# ____\n","\n","score = score_dataset(X, y)\n","print(f\"Your score: {score:.5f} RMSLE\")\n","\n","\n","# Check your answer\n","q_2.check()"]},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":0},"outputs":[],"source":["# Lines below will give you a hint or solution code\n","#q_2.hint()\n","#q_2.solution()"]},{"cell_type":"markdown","metadata":{},"source":["-------------------------------------------------------------------------------\n","\n","The next question explores a way you can use PCA to detect outliers in the dataset (meaning, data points that are unusually extreme in some way). Outliers can have a detrimental effect on model performance, so it's good to be aware of them in case you need to take corrective action. PCA in particular can show you anomalous *variation* which might not be apparent from the original features: neither small houses nor houses with large basements are unusual, but it is unusual for small houses to have large basements. That's the kind of thing a principal component can show you.\n","\n","Run the next cell to show distribution plots for each of the principal components you created above."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.catplot(\n","    y=\"value\",\n","    col=\"variable\",\n","    data=X_pca.melt(),\n","    kind='boxen',\n","    sharey=False,\n","    col_wrap=2,\n",");"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, in each of the components there are several points lying at the extreme ends of the distributions -- outliers, that is.\n","\n","Now run the next cell to see those houses that sit at the extremes of a component:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# You can change PC1 to PC2, PC3, or PC4\n","component = \"PC1\"\n","\n","idx = X_pca[component].sort_values(ascending=False).index\n","df.loc[idx, [\"SalePrice\", \"Neighborhood\", \"SaleCondition\"] + features]"]},{"cell_type":"markdown","metadata":{},"source":["# 3) Outlier Detection\n","\n","Do you notice any patterns in the extreme values? Does it seem like the outliers are coming from some special subset of the data?\n","\n","After you've thought about your answer, run the next cell for the solution and some discussion."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# View the solution (Run this cell to receive credit!)\n","q_3.check()"]},{"cell_type":"markdown","metadata":{},"source":["# Keep Going #\n","\n","[**Apply target encoding**](https://www.kaggle.com/ryanholbrook/target-encoding) to give a boost to categorical features."]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","\n","\n","\n","*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/221677) to chat with other Learners.*"]}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}