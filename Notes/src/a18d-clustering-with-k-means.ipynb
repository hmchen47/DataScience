{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Introduction #\n\nThis lesson and the next make use of what are known as *unsupervised learning* algorithms. Unsupervised algorithms don't make use of a target; instead, their purpose is to learn some property of the data, to represent the structure of the features in a certain way. In the context of feature engineering for prediction, you could think of an unsupervised algorithm as a \"feature discovery\" technique.\n\n**Clustering** simply means the assigning of data points to groups based upon how similar the points are to each other. A clustering algorithm makes \"birds of a feather flock together,\" so to speak.\n\nWhen used for feature engineering, we could attempt discover groups of customers representing a market segment, for instance, or geographic areas that share similar weather patterns. Adding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity.\n\n# Cluster Labels as a Feature #\n\nApplied to a single real-valued feature, clustering acts like a traditional \"binning\" or [\"discretization\"](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_classification.html) transform. On multiple features, it's like \"multi-dimensional binning\" (sometimes called *vector quantization*).\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/sr3pdYI.png\" width=800, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center><strong>Left:</strong> Clustering a single feature. <strong>Right:</strong> Clustering across two features.\n</center></figcaption>\n</figure>\n\nAdded to a dataframe, a feature of cluster labels might look like this:\n\n| Longitude | Latitude | Cluster |\n|-----------|----------|---------|\n| -93.619   | 42.054   | 3       |\n| -93.619   | 42.053   | 3       |\n| -93.638   | 42.060   | 1       |\n| -93.602   | 41.988   | 0       |"},{"cell_type":"markdown","metadata":{},"source":"It's important to remember that this `Cluster` feature is categorical. Here, it's shown with a label encoding (that is, as a sequence of integers) as a typical clustering algorithm would produce; depending on your model, a one-hot encoding may be more appropriate.\n\nThe motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It's a \"divide and conquer\" strategy.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/rraXFed.png\" width=800, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Clustering the YearBuilt feature helps this linear model learn its relationship to SalePrice.\n</center></figcaption>\n</figure>\n\nThe figure shows how clustering can improve a simple linear model. The curved relationship between the `YearBuilt` and `SalePrice` is too complicated for this kind of model -- it *underfits*. On smaller chunks however the relationship is *almost* linear, and that the model can learn easily.\n\n# k-Means Clustering #\n\nThere are a great many clustering algorithms. They differ primarily in how they measure \"similarity\" or \"proximity\" and in what kinds of features they work with. The algorithm we'll use, k-means, is intuitive and easy to apply in a feature engineering context. Depending on your application another algorithm might be more appropriate.\n\n**K-means clustering** measures similarity using ordinary straight-line distance (Euclidean distance, in other words). It creates clusters by placing a number of points, called **centroids**, inside the feature-space. Each point in the dataset is assigned to the cluster of whichever centroid it's closest to. The \"k\" in \"k-means\" is how many centroids (that is, clusters) it creates. You define the k yourself.\n\nYou could imagine each centroid capturing points through a sequence of radiating circles. When sets of circles from competing centroids overlap they form a line. The result is what's called a **Voronoi tessallation**. The tessallation shows you to what clusters future data will be assigned; the tessallation is essentially what k-means learns from its training data.\n\nThe clustering on the [*Ames*](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) dataset above is a k-means clustering. Here is the same figure with the tessallation and centroids shown.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/KSoLd3o.jpg.png\" width=450, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>K-means clustering creates a Voronoi tessallation of the feature space.\n</center></figcaption>\n</figure>\n\nLet's review how the k-means algorithm learns the clusters and what that means for feature engineering. We'll focus on three parameters from scikit-learn's implementation: `n_clusters`, `max_iter`, and `n_init`.\n\nIt's a simple two-step process. The algorithm starts by randomly initializing some predefined number (`n_clusters`) of centroids. It then iterates over these two operations:\n1. assign points to the nearest cluster centroid\n2. move each centroid to minimize the distance to its points\n\nIt iterates over these two steps until the centroids aren't moving anymore, or until some maximum number of iterations has passed (`max_iter`).\n\nIt often happens that the initial random position of the centroids ends in a poor clustering. For this reason the algorithm repeats a number of times (`n_init`) and returns the clustering that has the least total distance between each point and its centroid, the optimal clustering.\n\nThe animation below shows the algorithm in action. It illustrates the dependence of the result on the initial centroids and the importance of iterating until convergence.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/tBkCqXJ.gif\" width=550, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>The K-means clustering algorithm on Airbnb rentals in NYC.\n</center></figcaption>\n</figure>\n\nYou may need to increase the `max_iter` for a large number of clusters or `n_init` for a complex dataset. Ordinarily though the only parameter you'll need to choose yourself is `n_clusters` (k, that is). The best partitioning for a set of features depends on the model you're using and what you're trying to predict, so it's best to tune it like any hyperparameter (through cross-validation, say).\n\n# Example - California Housing #\n\nAs spatial features, [*California Housing*](https://www.kaggle.com/camnugent/california-housing-prices)'s `'Latitude'` and `'Longitude'` make natural candidates for k-means clustering. In this example we'll cluster these with `'MedInc'` (median income) to create economic segments in different regions of California."},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":"\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\ndf = pd.read_csv(\"../input/fe-course-data/housing.csv\")\nX = df.loc[:, [\"MedInc\", \"Latitude\", \"Longitude\"]]\nX.head()"},{"cell_type":"markdown","metadata":{},"source":"Since k-means clustering is sensitive to scale, it can be a good idea rescale or normalize data with extreme values. Our features are already roughly on the same scale, so we'll leave them as-is."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Create cluster feature\nkmeans = KMeans(n_clusters=6)\nX[\"Cluster\"] = kmeans.fit_predict(X)\nX[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n\nX.head()"},{"cell_type":"markdown","metadata":{},"source":"Now let's look at a couple plots to see how effective this was. First, a scatter plot that shows the geographic distribution of the clusters. It seems like the algorithm has created separate segments for higher-income areas on the coasts."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"sns.relplot(\n    x=\"Longitude\", y=\"Latitude\", hue=\"Cluster\", data=X, height=6,\n);"},{"cell_type":"markdown","metadata":{},"source":"The target in this dataset is `MedHouseVal` (median house value). These box-plots show the distribution of the target within each cluster. If the clustering is informative, these distributions should, for the most part, separate across `MedHouseVal`, which is indeed what we see."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"X[\"MedHouseVal\"] = df[\"MedHouseVal\"]\nsns.catplot(x=\"MedHouseVal\", y=\"Cluster\", data=X, kind=\"boxen\", height=6);"},{"cell_type":"markdown","metadata":{},"source":"# Your Turn #\n\n[**Add a feature of cluster labels**](https://www.kaggle.com/kernels/fork/14393920) to *Ames* and learn about another kind of feature clustering can create."},{"cell_type":"markdown","metadata":{},"source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/221677) to chat with other Learners.*"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}