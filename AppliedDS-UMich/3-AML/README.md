# Applied Machine Learning in Python

## Module 1: [Fundamentals of Machine Learning - Intro to SciKit Learn](./01-FundML.md)

+ [Course Syllabus](./01-FundML.md#course-syllabus)
+ [Introduction](./01-FundML.md#introduction)
+ [Key Concepts in Machine Learning](./01-FundML.md#key-concepts-in-machine-learning)
+ [Python Tools for Machine Learning](./01-FundML.md#python-tools-for-machine-learning)
+ [Module 1 Notebook](./01-FundML.md#module-1-notebook)
+ [An Example Machine Learning Problem](./01-FundML.md#an-example-machine-learning-problem)
+ [Examining the Data](./01-FundML.md#examining-the-data)
+ [K-Nearest Neighbors Classification](./01-FundML.md#k-nearest-neighbors-classification)
+ [Zachary Lipton: The Foundations of Algorithmic Bias (optional)](./01-FundML.md#zachary-lipton-the-foundations-of-algorithmic-bias-optional)
    + [Introduction](./01-FundML.md#introduction-1)
    + [Algorithms](./01-FundML.md#algorithms)
    + [Machine Learning](./01-FundML.md#machine-learning)
    + [Bias](./01-FundML.md#bias)
    + [Surrogate Objectives](./01-FundML.md#surrogate-objectives)
    + [Defining Bias](./01-FundML.md#defining-bias)
    + [Takeaways](./01-FundML.md#takeaways)
    + [Data Science Ethics Course](./01-FundML.md#data-science-ethics-course)
+ [Quiz: Module 1 Quiz](./01-FundML.md#quiz-module-1-quiz)


## [Assignment 1](./asgn01.md)

+ [Notebook Links](asgn01.md#notebook-links)
+ [Useful Info Links](asgn01.md#useful-info-links)
    + [Assignment 1 FAQs](asgn01.md#assignment-1-faqs)
    + [More on the Grader](asgn01.md#more-on-the-grader)
+ [Solution](asgn01.md#solution)



## Module 2: [Supervised Machine Learning](./02-Supervised1.md)

+ [Module 2 Notebook](./02-Supervised1.md#module-2-notebook)
+ [Introduction to Supervised Machine Learning](./02-Supervised1.md#introduction-to-supervised-machine-learning)
+ [Overfitting and Underfitting](./02-Supervised1.md#overfitting-and-underfitting)
+ [Supervised Learning: Datasets](./02-Supervised1.md#supervised-learning-datasets)
+ [K-Nearest Neighbors: Classification and Regression](./02-Supervised1.md#k-nearest-neighbors-classification-and-regression)
+ [Linear Regression: Least-Squares](./02-Supervised1.md#linear-regression-least-squares)
+ [Linear Regression: Ridge, Lasso, and Polynomial Regression](./02-Supervised1.md#linear-regression-ridge-lasso-and-polynomial-regression)
+ [Logistic Regression](./02-Supervised1.md#logistic-regression)
+ [Linear Classifiers: Support Vector Machines](./02-Supervised1.md#linear-classifiers-support-vector-machines)
+ [Multi-Class Classification](./02-Supervised1.md#multi-class-classification)
+ [Kernelized Support Vector Machines](./02-Supervised1.md#kernelized-support-vector-machines)
+ [Cross-Validation](./02-Supervised1.md#cross-validation)
+ [Decision Trees](./02-Supervised1.md#decision-trees)
+ [A Few Useful Things to Know about Machine Learning](./02-Supervised1.md#a-few-useful-things-to-know-about-machine-learning)
    1. [Learning = Representation + Evaluation + Optimization](./p0-UsefulThings.md#learning--representation--evaluation--optimization)
    2. [It's generalization that counts](./p0-UsefulThings.md#its-generalization-that-counts)
    3. [Data alone is not enough](./p0-UsefulThings.md#data-alone-is-not-enough)
    4. [Overfitting has many faces](./p0-UsefulThings.md#overfitting-has-many-faces)
    5. [Intitution fails in high dimensions](./p0-UsefulThings.md#intuition-fails-in-high-dimensions)
    6. [Theoretical guarantees are not what they seem](./p0-UsefulThings.md#theoretical-guarantees-are-not-what-they-seem)
    7. [Feature engineering is the key](./p0-UsefulThings.md#feature-engineering-is-the-key)
    8. [More data beats a cleaverer algorithm](./p0-UsefulThings.md#more-data-beats-a-cleaverer-algorithm)
    9. [Learn many models, not just one](./p0-UsefulThings.md#learn-many-models-not-just-one)
    0. [Simplicity does not imply accuracy](./p0-UsefulThings.md#simplicity-does-not-imply-accuracy)
    1. [Represntable does not imply learnable](./p0-UsefulThings.md#represntable-does-not-imply-learnable)
    2. [Correlation does not imply causation](./p0-UsefulThings.md#correlation-does-not-imply-causation)
    3. [Regerences](./p0-UsefulThings.md#references)
+ [Ed Yong: Genetic Test for Autism Refuted (optional)](./02-Supervised1.md#ed-yong-genetic-test-for-autism-refuted-optional)
+ [Quiz: Module 2 Quiz](./02-Supervised1.md#quiz-module-2-quiz)
+ [Assignment 2: Q6](./02-Supervised1.md#assignment-2-q6)
+ [Classifier Visualization Playspace](./02-Supervised1.md#classifier-visualization-playspace)


### [Assignment 2](./asgn02.md)

+ [Notebook](./asgn02.md#notebook)
+ [Useful Links](./asgn02.md#useful-links)
    + [Q1 and Q2 - answer not matching grader answer](./asgn02.md#q1-and-q2---answer-not-matching-grader-answer)
    + [Assignment 2, Q 6, which dataset to use?](./asgn02.md#assignment-2-q-6-which-dataset-to-use)
+ [Solution](./asgn02.md#solution)
    + [Regression](./asgn02.md#regression)
    + [Classification](./asgn02.md#classification)


## Module 3: [Evaluation](./03-Evaluation.md)

+ [Module 3 Notebook](./03-Evaluation.md#module-3-notebook)
+ [Model Evaluation & Selection](./03-Evaluation.md#model-evaluation--selection)
+ [Confusion Matrices & Basic Evaluation Metrics](./03-Evaluation.md#confusion-matrices--basic-evaluation-metrics)
+ [Classifier Decision Functions](./03-Evaluation.md#classifier-decision-functions)
+ [Precision-recall and ROC curves](./03-Evaluation.md#precision-recall-and-roc-curves)
+ [Multi-Class Evaluation](./03-Evaluation.md#multi-class-evaluation)
+ [Regression Evaluation](./03-Evaluation.md#regression-evaluation)
+ [Practical Guide to Controlled Experiments on the Web (optional)](./03-Evaluation.md#practical-guide-to-controlled-experiments-on-the-web-optional)
    + [Introduction](./p1-ControlledExp.md#introduction)
    + [Motivating Examples](./p1-ControlledExp.md#motivating-examples)
        + [Checkout Page at Doctor FootCare](./p1-ControlledExp.md#checkout-page-at-doctor-footcare)
        + [Ratings of Microsoft Office Help Articles](./p1-ControlledExp.md#ratings-of-microsoft-office-help-articles)
        + [Results and ROI](./p1-ControlledExp.md#results-and-roi)
    + [Controlled Experiments](./p1-ControlledExp.md#controlled-experiments)
        + [Terminology](./p1-ControlledExp.md#terminology)
        + [Hypothesis Testing and Sample Size](./p1-ControlledExp.md#hypothesis-testing-and-sample-size)
        + [Extensions for Online Settings](./p1-ControlledExp.md#extensions-for-online-settings)
        + [Limitations](./p1-ControlledExp.md#limitations)
    + [Implementation Architecture](./p1-ControlledExp.md#implementation-architecture)
        + [Randomization Algorithm](./p1-ControlledExp.md#randomization-algorithm)
        + [Assignment Method](./p1-ControlledExp.md#assignment-method)
    + [Lesson Learned](./p1-ControlledExp.md#lesson-learned)
        + [Analysis](./p1-ControlledExp.md#analysis)
        + [Trust and Execution](./p1-ControlledExp.md#trust-and-execution)
        + [Culture and Business](./p1-ControlledExp.md#culture-and-business)
    + [Summary](./p1-ControlledExp.md#summary)
+ [Model Selection: Optimizing Classifiers for Different Evaluation Metrics](./03-Evaluation.md#model-selection-optimizing-classifiers-for-different-evaluation-metrics)
+ [Quiz: Module 3 Quiz](./03-Evaluation.md#quiz-module-3-quiz)


### [Assignment 3](./asgn03.md)

+ [NoteBook links](asgn03.md#notebook-links)
+ [Useful links](asgn03.md#useful-links)
    + [Question 4 & Question 6](asgn03.md#question-4--question-6)
    + [Meaning of evaluation metrics like precision score per class](asgn03.md#meaning-of-evaluation-metrics-like-precision-score-per-class)
    + [Assignment Q 5](asgn03.md#assignment-q-5)
+ [Solution](asgn03.md#solution)


## Module 4: [Supervised Machine Learning - Part 2](./04-Supervised2.md)

+ [Module 4 Notebook](./04-Supervised2.md)
+ [Naive Bayes Classifiers](./04-Supervised2.md#naive-bayes-classifiers)
+ [Random Forests](./04-Supervised2.md#random-forests)
+ [Gradient Boosted Decision Trees](./04-Supervised2.md#gradient-boosted-decision-trees)
+ [Neural Networks](./04-Supervised2.md#neural-networks)
+ [Neural Networks Made Easy (optional)](./04-Supervised2.md#neural-networks-made-easy-optional)
    + [Thinking by brute force](./04-Supervised2.md#thinking-by-brute-force)
    + [Teaching machines to learn](./04-Supervised2.md#teaching-machines-to-learn)
    + [Machines — they’re just like us!](./04-Supervised2.md#machines--theyre-just-like-us)
    + [All aboard the network training](./04-Supervised2.md#all-aboard-the-network-training)
    + [So many layers…](./04-Supervised2.md#so-many-layers)
    + [If at first you don’t succeed, try, try, try again](./04-Supervised2.md#if-at-first-you-dont-succeed-try-try-try-again)
+ [Play with Neural Networks: TensorFlow Playground (optional)](./04-Supervised2.md#play-with-neural-networks-tensorflow-playground-optional)
+ [Deep Learning (Optional)](./04-Supervised2.md#deep-learning-optional)
+ [Deep Learning in a Nutshell: Core Concepts (optional)](./04-Supervised2.md#deep-learning-in-a-nutshell-core-concepts-optional)
    + [Core Concepts](04-Supervised2.md#core-concepts)
    + [Fundamental Concepts](04-Supervised2.md#fundamental-concepts)
    + [Convolutional Deep Learning](04-Supervised2.md#convolutional-deep-learning)
+ [Assisting Pathologists in Detecting Cancer with Deep Learning (optional)](./04-Supervised2.md#assisting-pathologists-in-detecting-cancer-with-deep-learning-optional)
+ [Data Leakage](./04-Supervised2.md#data-leakage)
+ [The Treachery of Leakage (optional)](./04-Supervised2.md#the-treachery-of-leakage-optional)
    + [When is a data leaky?](04-Supervised2.md#when-is-a-data-leaky)
    + [Freedom of Information](04-Supervised2.md#freedom-of-information)
    + [There’s no such thing as a time machine](04-Supervised2.md#theres-no-such-thing-as-a-time-machine)
    + [What can you do?](04-Supervised2.md#what-can-you-do)
+ [Leakage in Data Mining: Formulation, Detection, and Avoidance (optional)](./04-Supervised2.md#leakage-in-data-mining-formulation-detection-and-avoidance-optional)
    + [Introduction](p3-Leakage.md#introduction)
    + [Leakage in the KDD Literature](p3-Leakage.md#leakage-in-the-kdd-literature)
    + [Formulation](p3-Leakage.md#formulation)
        + [Preliminaries and Legitimacy](p3-Leakage.md#preliminaries-and-legitimacy)
        + [Leaking Feature](p3-Leakage.md#leaking-feeature)
        + [Leakage in Training Examples](p3-Leakage.md#leakage-in-training-examples)
        + [Discussion](p3-Leakage.md#discussion)
    + [Avoidance](p3-Leakage.md#avoidance)
        + [Methodology](p3-Leakage.md#methodology)
        + [External Leakage in Comparisons](p3-Leakage.md#external-leakage-in-comparisons)
    + [Detection](p3-Leakage.md#detection)
    + [(Not) Fixing Leakage](p3-Leakage.md#not-fixing-leakage)
    + [Conclusion](p3-Leakage.md#conclusion)
+ [Data Leakage Example: The ICML 2013 Whale Challenge (optional)](./04-Supervised2.md#data-leakage-example-the-icml-2013-whale-challenge-optional)
+ [Rules of Machine Learning: Best Practices for ML Engineering (optional)](./04-Supervised2.md#rules-of-machine-learning-best-practices-for-ml-engineering-optional)
    + [Terminology](p4-MLRules.md#terminology)
    + [Overview](p4-MLRules.md#overview)
    + [Before Machine Learning](p4-MLRules.md#before-machine-learning)
        + [Rule #01](p4-MLRules.md#rule-01): Don’t be afraid to launch a product without machine learning.
        + [Rule #02](p4-MLRules.md#rule-02): Make metrics design and implementation a priority.
        + [Rule #03](p4-MLRules.md#rule-03): Choose machine learning over a complex heuristic.
    + [ML Phase I: Your First Pipeline](p4-MLRules.md#ml-phase-i-your-first-pipeline)
        + [Rule #04](p4-MLRules.md#rule-04): Keep the first model simple and get the infrastructure right.
        + [Rule #05](p4-MLRules.md#rule-05): Test the infrastructure independently from the machine learning.
        + [Rule #06](p4-MLRules.md#rule-06): Be careful about dropped data when copying pipelines.
        + [Rule #07](p4-MLRules.md#rule-07): Turn heuristics into features, or handle them externally. 
    + [Monitoring](p4-MLRules.md#monitoring)
        + [Rule #08](p4-MLRules.md#rule-08): Know the freshness requirements of your system.
        + [Rule #09](p4-MLRules.md#rule-09): Detect problems before exporting models.
        + [Rule #10](p4-MLRules.md#rule-10): Watch for silent failures.
        + [Rule #11](p4-MLRules.mdrule-11): Give feature sets owners and documentation.
    + [Your First Objective](p4-MLRules.md#your-first-objective)
        + [Rule #12](p4-MLRules.md#rule-12): Don’t overthink which objective you choose to directly optimize.
        + [Rule #13](p4-MLRules.md#rule-13): Choose a simple, observable and attributable metric for your first objective.
        + [Rule #14](p4-MLRules.md#rule-14): Starting with an interpretable model makes debugging easier.
        + [Rule #15](p4-MLRules.md#rule-15): Separate Spam Filtering and Quality Ranking in a Policy Layer.
    + [ML Phase II: Feature Engineering](p4-MLRules.md#ml-phase-ii-feature-engineering)
        + [Rule #16](p4-MLRules.md#rule-16): Plan to launch and iterate.
        + [Rule #17](p4-MLRules.md#rule-17): Start with directly observed and reported features as opposed to learned features.
        + [Rule #18](p4-MLRules.md#rule-18): Explore with features of content that generalize across contexts.
        + [Rule #19](p4-MLRules.md#rule-19): Use very specific features when you can.
        + [Rule #20](p4-MLRules.md#rule-20): Combine and modify existing features to create new features in humanunderstandable ways.
        + [Rule #21](p4-MLRules.md#rule-21): The number of feature weights you can learn in a linear model is roughly proportional to the amount of data you have.
        + [Rule #22](p4-MLRules.md#rule-): Clean up features you are no longer using.
    + [Human Analysis of the System](p4-MLRules.md#human-analysis-of-the-system)
        + [Rule #23](p4-MLRules.md#rule-23): You are not a typical end user.
        + [Rule #24](p4-MLRules.md#rule-24): Measure the delta between models.
        + [Rule #25](p4-MLRules.md#rule-25): When choosing models, utilitarian performance trumps predictive power.
        + [Rule #26](p4-MLRules.md#rule-26): Look for patterns in the measured errors, and create new features.
        + [Rule #27](p4-MLRules.md#rule-27): Try to quantify observed undesirable behavior.
        + [Rule #28](p4-MLRules.md#rule-28): Be aware that identical shortterm behavior does not imply identical longterm behavior.
    + [Training-Serving Skew](p4-MLRules.md#training-serving-skew)
        + [Rule #29](p4-MLRules.md#rule-29): The best way to make sure that you train like you serve is to save the set of features used at serving time, and then pipe those features to a log to use them at training time.
        + [Rule #30](p4-MLRules.md#rule-30): Importance weight sampled data, don’t arbitrarily drop it!
        + [Rule #31](p4-MLRules.md#rule-31): Beware that if you join data from a table at training and serving time, the data in the table may change.
        + [Rule #32](p4-MLRules.md#rule-32): Reuse code between your training pipeline and your serving pipeline whenever possible.
        + [Rule #33](p4-MLRules.md#rule-33): If you produce a model based on the data until January 5th, test the model on the data from January 6th and after.
        + [Rule #34](p4-MLRules.md#rule-34): In binary classification for filtering (such as spam detection or determining interesting emails), make small shortterm sacrifices in performance for very clean data.
        + [Rule #35](p4-MLRules.md#rule-35): Beware of the inherent skew in ranking problems.
        + [Rule #36](p4-MLRules.md#rule-36): Avoid feedback loops with positional features.
        + [Rule #37](p4-MLRules.md#rule-37): Measure Training/Serving Skew.
    + [ML Phase III: Slowed Growth, Optimization Refinement, and Complex Models](p4-MLRules.md#ml-phase-iii-slowed-growth-optimization-refinement-and-complex-models)
        + [Rule #38](p4-MLRules.md#rule-38): Don’t waste time on new features if unaligned objectives have become the issue.
        + [Rule #39](p4-MLRules.md#rule-39): Launch decisions will depend upon more than one metric.
        + [Rule #40](p4-MLRules.md#rule-40): Keep ensembles simple.
        + [Rule #41](p4-MLRules.md#rule-41): When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.
        + [Rule #42](p4-MLRules.md#rule-42): Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.
        + [Rule #43](p4-MLRules.md#rule-43): Your friends tend to be the same across different products. Your interests tend not to be.
+ [Quiz: Module 4 Quiz](./04-Supervised2.md#quiz-module-4-quiz)



### [Assignment 4](./asgn04.md)

+ [Notebooks](asgn04.md#notebooks)
+ [Useful Links for Assignment](asgn04.md#useful-links-for-assignment)
    + [No AUC output in the feedback from AG? Check this!](asgn04.md#no-auc-output-in-the-feedback-from-ag-check-this)
    + [Assignment 4 FAQs](asgn04.md#assignment-4-faqs)
    + [Assignment 4 AUC Competition(also a good start point)](asgn04.md#assignment-4-auc-competitionalso-a-good-start-point)
    + [How to read train.csv via pd.read_csv](asgn04.md#how-to-read-traincsv-via-pdread_csv)
+ [Wikipedia: Missing Data](asgn04.md#wikipedia-missing-data)
+ [How to Treat Missing Values in Your Data](asgn04.md#how-to-treat-missing-values-in-your-data)
    + [Introduction](asgn04.md#introduction)
    + [Techniques](asgn04.md#techniques)
    + [Illustration](asgn04.md#illustration)
    + [Conclusion](asgn04.md#conclusion)
+ [Handling Missing Values when Applying Classification Models](asgn04.md#handling-missing-values-when-applying-classification-models)
    1. [Abstract](p5-Missing.md#abstract)
    1. [Introduction](p5-Missing.md#introduction)
    1. [Treatments for Missing Values at Prediction Time](p5-Missing.md#treatments-for-missing-values-at-prediction-time)
    1. [Experimental Comparison of Prediction-time Treatments for Missing Values](p5-Missing.md#experimental-comparison-of-prediction-time-treatments-for-missing-values)
        1. [Experimental Setup](p5-Missing.md#experimental-setup)
        1. [Comparison of PVI, DBI and Reduced Modeling](p5-Missing.md#comparison-of-pvi-dbi-and-reduced-modeling)
        1. [Feature Imputability and Modeling Error](p5-Missing.md#feature-imputability-and-modeling-error)
        1. [Evaluation using Ensembles of Trees](p5-Missing.md#evaluation-using-ensembles-of-trees)
        1. [Evaluation using Logistic Regression](p5-Missing.md#evaluation-using-logistic-regression)
        1. [Evaluation with “Naturally Occurring” Missing Values](p5-Missing.md#evaluation-with-naturally-occurring-missing-values)
        1. [Evaluation with Multiple Missing Values](p5-Missing.md#evaluation-with-multiple-missing-values)
    1. [Hybrid Models for Efficient Prediction with Missing Values](p5-Missing.md#hybrid-models-for-efficient-prediction-with-missing-values)
        1. [Likelihood-based Hybrid Solutions](p5-Missing.md#likelihood-based-hybrid-solutions)
        1. [Reduced-Feature Ensembles (ReFE)](p5-Missing.md#reduced-feature-ensembles-refe)
        1. [Larger Ensembles](p5-Missing.md#larger-ensembles)
        1. [ReFEs with Increasing Numbers of Missing Values](p5-Missing.md#refes-with-increasing-numbers-of-missing-values)
    1. [Related Work](p5-Missing.md#related-work)
    1. [Limitations](p5-Missing.md#limitations)
    1. [Conclusions](p5-Missing.md#conclusions)
+ [Solution](asgn04.md#solution)
    + [Assignment Description](asgn04.md#assignment-description)
    + [Solution 2](asgn04.md#solution-2)
    + [Solution 3](asgn04.md#solution-3)


## Optional: [Unsupervised Machine Learning](./05-Unsupervised.md)

+ [Unsupervised Learning Notebook](./05-Unsupervised.md#unsupervised-learning-notebook)
+ [Introduction](./05-Unsupervised.md#introduction)
+ [Dimensionality Reduction and Manifold Learning](./05-Unsupervised.md#dimensionality-reduction-and-manifold-learning)
+ [Clustering](./05-Unsupervised.md#clustering)
+ [How to Use t-SNE Effectively](./05-Unsupervised.md#how-to-use-t-sne-effectively)
    + [Introduction](05-Unsupervised.md#introduction-1)
    + [Those hyperparameters really matter](05-Unsupervised.md#those-hyperparameters-really-matter)
    + [Cluster sizes in a t-SNE plot mean nothing](05-Unsupervised.md#cluster-sizes-in-a-t-sne-plot-mean-nothing)
    + [Distances between clusters might not mean anything](05-Unsupervised.md#distances-between-clusters-might-not-mean-anything)
    + [Random noise doesn’t always look random](05-Unsupervised.md#random-noise-doesnt-always-look-random)
    + [You can see some shapes, sometimes](05-Unsupervised.md#you-can-see-some-shapes-sometimes)
    + [For topology, you may need more than one plot](05-Unsupervised.md#for-topology-you-may-need-more-than-one-plot)
    + [Conclusion](05-Unsupervised.md#conclusion)
+ [How Machines Make Sense of Big Data: an Introduction to Clustering Algorithms](./05-Unsupervised.md#how-machines-make-sense-of-big-data-an-introduction-to-clustering-algorithms)
    + [Introduction](05-Unsupervised.md#introduction-2)
    + [K-means clustering](05-Unsupervised.md#k-means-clustering)
    + [Hierarchical clustering](05-Unsupervised.md#hierarchical-clustering)
    + [Graph Community Detection](05-Unsupervised.md#graph-community-detection)
    + [Conclusion](05-Unsupervised.md#conclusion-1)






