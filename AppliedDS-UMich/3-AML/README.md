# Applied Machine Learning in Python

## Module 1: [Fundamentals of Machine Learning - Intro to SciKit Learn](./01-FundML.md)

+ [Course Syllabus](./01-FundML.md#)
+ [Introduction](./01-FundML.md#)
+ [Key Concepts in Machine Learning](./01-FundML.md#)
+ [Python Tools for Machine Learning](./01-FundML.md#)
+ [Module 1 Notebook](./01-FundML.md#)
+ [An Example Machine Learning Problem](./01-FundML.md#)
+ [Examining the Data](./01-FundML.md#)
+ [K-Nearest Neighbors Classification](./01-FundML.md#)
+ [Zachary Lipton: The Foundations of Algorithmic Bias (optional)](./01-FundML.md#)
+ [Quiz: Module 1 Quiz](./01-FundML.md#)


## [Assignment 1](./asgn01.md)


## Module 2: [Supervised Machine Learning](./02-Supervised1.md)

+ [Module 2 Notebook](./02-Supervised1.md#)
+ [Introduction to Supervised Machine Learning](./02-Supervised1.md#)
+ [Overfitting and Underfitting](./02-Supervised1.md#)
+ [Supervised Learning: Datasets](./02-Supervised1.md#)
+ [K-Nearest Neighbors: Classification and Regression](./02-Supervised1.md#)
+ [Linear Regression: Least-Squares](./02-Supervised1.md#)
+ [Linear Regression: Ridge, Lasso, and Polynomial Regression](./02-Supervised1.md#)
+ [Logistic Regression](./02-Supervised1.md#)
+ [Linear Classifiers: Support Vector Machines](./02-Supervised1.md#)
+ [Multi-Class Classification](./02-Supervised1.md#)
+ [Kernelized Support Vector Machines](./02-Supervised1.md#)
+ [Cross-Validation](./02-Supervised1.md#)
+ [Decision Trees](./02-Supervised1.md#)
+ [A Few Useful Things to Know about Machine Learning](./02-Supervised1.md#)
+ [Ed Yong: Genetic Test for Autism Refuted (optional)](./02-Supervised1.md#)
+ [Quiz: Module 2 Quiz](./02-Supervised1.md#)
+ [Classifier Visualization Playspace](./02-Supervised1.md#)


## [Assignment 2](./asgn02.md)



## Module 3: [Evaluation](./03-Evaluation.md)

+ [Module 3 Notebook](./03-Evaluation.md#module-3-notebook)
+ [Model Evaluation & Selection](./03-Evaluation.md#model-evaluation--selection)
+ [Confusion Matrices & Basic Evaluation Metrics](./03-Evaluation.md#confusion-matrices--basic-evaluation-metrics)
+ [Classifier Decision Functions](./03-Evaluation.md#classifier-decision-functions)
+ [Precision-recall and ROC curves](./03-Evaluation.md#precision-recall-and-roc-curves)
+ [Multi-Class Evaluation](./03-Evaluation.md#multi-class-evaluation)
+ [Regression Evaluation](./03-Evaluation.md#regression-evaluation)
+ [Practical Guide to Controlled Experiments on the Web (optional)](./03-Evaluation.md#practical-guide-to-controlled-experiments-on-the-web-optional)
+ [Model Selection: Optimizing Classifiers for Different Evaluation Metrics](./03-Evaluation.md#model-selection-optimizing-classifiers-for-different-evaluation-metrics)
+ [Quiz: Module 3 Quiz](./03-Evaluation.md#quiz-module-3-quiz)


## [Assignment 3](./asgn03.md)


## Module 4: [Supervised Machine Learning - Part 2](./04-Supervised2.md)

+ [Module 4 Notebook](./04-Supervised2.md)
+ [Naive Bayes Classifiers](./04-Supervised2.md#naive-bayes-classifiers)
+ [Random Forests](./04-Supervised2.md#random-forests)
+ [Gradient Boosted Decision Trees](./04-Supervised2.md#gradient-boosted-decision-trees)
+ [Neural Networks](./04-Supervised2.md#neural-networks)
+ [Neural Networks Made Easy (optional)](./04-Supervised2.md#neural-networks-made-easy-optional)
    + [Thinking by brute force](./04-Supervised2.md#thinking-by-brute-force)
    + [Teaching machines to learn](./04-Supervised2.md#teaching-machines-to-learn)
    + [Machines — they’re just like us!](./04-Supervised2.md#machines---theyre-just-like-us)
    + [All aboard the network training](./04-Supervised2.md#all-abord-the-network-training)
    + [So many layers…](./04-Supervised2.md#so-many-layers)
    + [If at first you don’t succeed, try, try, try again](./04-Supervised2.md#if-at-first-you-dont-success-try-try-try-again)
+ [Play with Neural Networks: TensorFlow Playground (optional)](./04-Supervised2.md#play-with-neural-networks-tensorflow-playground-optional)
+ [Deep Learning (Optional)](./04-Supervised2.md#deep-learning-optional)
+ [Deep Learning in a Nutshell: Core Concepts (optional)](./04-Supervised2.md#deep-learning-in-a-nutshell-core-concepts-optional)
    + [Core Concepts](04-Supervised2.md#core-concepts)
    + [Fundamental Concepts](04-Supervised2.md#fundamental-concepts)
    + [Convolutional Deep Learning](04-Supervised2.md#convolutional-deep-learning)
+ [Assisting Pathologists in Detecting Cancer with Deep Learning (optional)](./04-Supervised2.md#assisting-pathologists-in-detecting-cancer-with-deep-learning-optional)
+ [Data Leakage](./04-Supervised2.md#data-leakage)
+ [The Treachery of Leakage (optional)](./04-Supervised2.md#the-treachery-of-leakage-optional)
    + [When is a data leaky?](04-Supervised2.md#when-is-a-data-leaky)
    + [Freedom of Information](04-Supervised2.md#freedom-of-information)
    + [There’s no such thing as a time machine](04-Supervised2.md#theres-no-such-thing-as-a-time-machine)
    + [What can you do?](04-Supervised2.md#what-can-you-do)
+ [Leakage in Data Mining: Formulation, Detection, and Avoidance (optional)](./04-Supervised2.md#leakage-in-data-mining-formulation-detection-and-avoidance-optional)
    + [Introduction](p3-Leakage.md#introduction)
    + [Leakage in the KDD Literature](p3-Leakage.md#leakage-in-the-kdd-literature)
    + [Formulation](p3-Leakage.md#formulation)
        + [Preliminaries and Legitimacy](p3-Leakage.md#preliminaries-and-legitimacy)
        + [Leaking Feature](p3-Leakage.md#leaking-feeature)
        + [Leakage in Training Examples](p3-Leakage.md#leakage-in-training-examples)
        + [Discussion](p3-Leakage.md#discussion)
    + [Avoidance](p3-Leakage.md#avoidance)
        + [Methodology](p3-Leakage.md#methodology)
        + [External Leakage in Comparisons](p3-Leakage.md#external-leakage-in-comparisons)
    + [Detection](p3-Leakage.md#detection)
    + [(Not) Fixing Leakage](p3-Leakage.md#not-fixing-leakaging)
    + [Conclusion](p3-Leakage.md#conclusion)
+ [Data Leakage Example: The ICML 2013 Whale Challenge (optional)](./04-Supervised2.md#data-leakage-example-the-icml-2013-whale-challenge-optional)
+ [Rules of Machine Learning: Best Practices for ML Engineering (optional)](./04-Supervised2.md#rules-of-machine-learning-best-practices-for-ml-engineering-optional)
    + [Terminology](p4-MLRules.md#terminology)
    + [Overview](p4-MLRules.md#overview)
    + [Before Machine Learning](p4-MLRules.md#before-machine-learning)
        + [Rule #01](p4-MLRules.md#rule-01): Don’t be afraid to launch a product without machine learning.
        + [Rule #02](p4-MLRules.md#rule-02): Make metrics design and implementation a priority.
        + [Rule #03](p4-MLRules.md#rule-03): Choose machine learning over a complex heuristic.
    + [ML Phase I: Your First Pipeline](p4-MLRules.md#ml-phase-i-your-first-pipeline)
        + [Rule #04](p4-MLRules.md#rule-04): Keep the first model simple and get the infrastructure right.
        + [Rule #05](p4-MLRules.md#rule-05): Test the infrastructure independently from the machine learning.
        + [Rule #06](p4-MLRules.md#rule-06): Be careful about dropped data when copying pipelines.
        + [Rule #07](p4-MLRules.md#rule-07): Turn heuristics into features, or handle them externally. 
    + [Monitoring](p4-MLRules.md#monitoring)
        + [Rule #08](p4-MLRules.md#rule-08): Know the freshness requirements of your system.
        + [Rule #09](p4-MLRules.md#rule-09): Detect problems before exporting models.
        + [Rule #10](p4-MLRules.md#rule-10): Watch for silent failures.
        + [Rule #11](p4-MLRules.mdrule-11): Give feature sets owners and documentation.
    + [Your First Objective](p4-MLRules.md#your-first-objective)
        + [Rule #12](p4-MLRules.md#rule-12): Don’t overthink which objective you choose to directly optimize.
        + [Rule #13](p4-MLRules.md#rule-13): Choose a simple, observable and attributable metric for your first objective.
        + [Rule #14](p4-MLRules.md#rule-14): Starting with an interpretable model makes debugging easier.
        + [Rule #15](p4-MLRules.md#rule-15): Separate Spam Filtering and Quality Ranking in a Policy Layer.
    + [ML Phase II: Feature Engineering](p4-MLRules.md#ml-phase-ii-feature-engineering)
        + [Rule #16](p4-MLRules.md#rule-16): Plan to launch and iterate.
        + [Rule #17](p4-MLRules.md#rule-17): Start with directly observed and reported features as opposed to learned features.
        + [Rule #18](p4-MLRules.md#rule-18): Explore with features of content that generalize across contexts.
        + [Rule #19](p4-MLRules.md#rule-19): Use very specific features when you can.
        + [Rule #20](p4-MLRules.md#rule-20): Combine and modify existing features to create new features in humanunderstandable ways.
        + [Rule #21](p4-MLRules.md#rule-21): The number of feature weights you can learn in a linear model is roughly proportional to the amount of data you have.
        + [Rule #22](p4-MLRules.md#rule-): Clean up features you are no longer using.
    + [Human Analysis of the System](p4-MLRules.md#human-analysis-of-the-system)
        + [Rule #23](p4-MLRules.md#rule-23): You are not a typical end user.
        + [Rule #24](p4-MLRules.md#rule-24): Measure the delta between models.
        + [Rule #25](p4-MLRules.md#rule-25): When choosing models, utilitarian performance trumps predictive power.
        + [Rule #26](p4-MLRules.md#rule-26): Look for patterns in the measured errors, and create new features.
        + [Rule #27](p4-MLRules.md#rule-27): Try to quantify observed undesirable behavior.
        + [Rule #28](p4-MLRules.md#rule-28): Be aware that identical shortterm behavior does not imply identical longterm behavior.
    + [Training-Serving Skew](p4-MLRules.md#training---serving-skew)
        + [Rule #29](p4-MLRules.md#rule-29): The best way to make sure that you train like you serve is to save the set of features used at serving time, and then pipe those features to a log to use them at training time.
        + [Rule #30](p4-MLRules.md#rule-30): Importance weight sampled data, don’t arbitrarily drop it!
        + [Rule #31](p4-MLRules.md#rule-31): Beware that if you join data from a table at training and serving time, the data in the table may change.
        + [Rule #32](p4-MLRules.md#rule-32): Reuse code between your training pipeline and your serving pipeline whenever possible.
        + [Rule #33](p4-MLRules.md#rule-33): If you produce a model based on the data until January 5th, test the model on the data from January 6th and after.
        + [Rule #34](p4-MLRules.md#rule-34): In binary classification for filtering (such as spam detection or determining interesting emails), make small shortterm sacrifices in performance for very clean data.
        + [Rule #35](p4-MLRules.md#rule-35): Beware of the inherent skew in ranking problems.
        + [Rule #36](p4-MLRules.md#rule-36): Avoid feedback loops with positional features.
        + [Rule #37](p4-MLRules.md#rule-37): Measure Training/Serving Skew.
    + [ML Phase III: Slowed Growth, Optimization Refinement, and Complex Models](p4-MLRules.md#ml-phase-iii-slowed-growth-optimization-refinement-and-complex-models)
        + [Rule #38](p4-MLRules.md#rule-38): Don’t waste time on new features if unaligned objectives have become the issue.
        + [Rule #39](p4-MLRules.md#rule-39): Launch decisions will depend upon more than one metric.
        + [Rule #40](p4-MLRules.md#rule-40): Keep ensembles simple.
        + [Rule #41](p4-MLRules.md#rule-41): When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.
        + [Rule #42](p4-MLRules.md#rule-42): Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.
        + [Rule #43](p4-MLRules.md#rule-43): Your friends tend to be the same across different products. Your interests tend not to be.
+ [Quiz: Module 4 Quiz](./04-Supervised2.md#quiz-module-4-quiz)


## [Assignment 4](./asgn04.md)

Notebooks
Useful Links for Assignment
No AUC output in the feedback from AG? Check this!
Assignment 4 FAQs
Assignment 4 AUC Competition(also a good start point)
How to read train.csv via pd.read_csv
Wikipedia: Missing Data
How to Treat Missing Values in Your Data
Introduction
Techniques
Illustration



## Optional: [Unsupervised Machine Learning](./05-Unsupervised.md)

+ [Unsupervised Learning Notebook](./05-Unsupervised.md#)
+ [Introduction](./05-Unsupervised.md#)
+ [Dimensionality Reduction and Manifold Learning](./05-Unsupervised.md#)
+ [Clustering](./05-Unsupervised.md#)
+ [How to Use t-SNE Effectively](./05-Unsupervised.md#)
+ [How Machines Make Sense of Big Data: an Introduction to Clustering Algorithms](./05-Unsupervised.md#)




