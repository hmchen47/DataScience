# Module 1: Working with Text in Python

## Course Syllabus

### Prerequisites

In order to be successful in this course, you will need to know how to program in Python. The expectation is that you have completed the first three courses in this Applied Data Science with Python series, specifically Course 1 on [Introduction to Data Science in Python](https://www.coursera.org/learn/python-data-analysis) and Course 3 on [Applied Machine Learning in Python](https://www.coursera.org/learn/python-machine-learning), so that you are familiar with the numpy and pandas Python libraries for data manipulation, and scikit-learn toolkit for machine learning algorithms.

### Week by week

In this course, you will build your skill sets in text mining through lectures on core topics, code walk-throughs on tasks related to weekly assignments, reading material, quizzes, and programming assignments.

In __Module One__, you will be introduced to basic text mining tasks, and will be able to interpret text in terms of its building blocks – i.e. words and sentences, and reading in text files, processing text, and addressing common issues with unstructured text. You will also learn how to write regular expressions to find and extract words and concepts that follow specific textual patterns. You will be introduced to UTF-8 encoding and how multi-byte characters are handled in Python. This week’s assignment will focus on identifying dates using regular expressions and normalize them.

In __Module Two__, you will delve into NLTK, a very popular toolkit for processing text in Python. Through NLTK, you will be introduced to common natural language processing tasks and how to extract semantic meaning from text. For this week’s assignment, you’ll get a hands-on experience with NLTK to process and derive meaningful features and statistics from text.

In __Module Three__, you will engage with two of the most standard text classification approaches, viz. naïve Bayes and support vector machine classification. Building on some of the topics you might have encountered in Course 3 of this specialization, you will learn about deriving features from text and using NLTK and scikit-learn toolkits for supervised text classification. You will also be introduced to another natural language challenge of analyzing sentiment from text reviews. For this week’s assignment, you will train a classifier to detect spam messages from non-spam (“ham”) messages. Through this assignment, you will also get a hands-on experience with cross-validation and training and testing phases of supervised classification tasks.

In __Module Four__, you will be introduced to more advanced text mining approaches of topic modeling and semantic text similarity. You will also explore advanced information extraction topics, such as named entity recognition, building on concepts you have seen through Module One and Module Three of this course. The final assignment lets you explore semantic similarity of text snippets and building topic models using the gensim package. You will also experience the practical challenge of making sense of topic models in real life.

### Enrollment Options

Coursera has made the decision to make Specializations available by monthly subscription. This means you can choose to pay a monthly fee to access all of the courses in a specific Specialization.

Coursera’s switch to monthly subscriptions comes with another change -- for those learners who choose the “Audit Only” enrollment, you will no longer be able to submit assignments for grades nor see answers for those assignments. You will still have access to all the course materials but you will not be graded on your work, nor see answers to graded assignments.

For further information on the different enrollment options for Coursera courses, please visit the [Enrollment Options Help](https://learner.coursera.help/hc/en-us/articles/209818613-Enrollment-options) page. If you have feedback about the enrollment options shared on the Enrollment Options page, you can share your thoughts with Coursera in this [survey](https://www.surveymonkey.com/r/65DPLHG).

### Grading and Assignments

The lectures will provide you with some guidance for completing assignments, but you will need to take initiative and look beyond assignment instructions in order to be successful. You'll need to know how to ask questions in the discussion forums of your peers, and seek out new information through web searches and [Stack Overflow](http://stackoverflow.com/questions/tagged/matplotlib). Be sure to also check out the [Additional Resources](https://www.coursera.org/learn/python-text-mining/resources/aCvfj).

If you are not sure what kind of output is required, or think there is a need for more clarity, please head to the course discussion forums. Note that some assignments and in video quizzes may not be mobile friendly.

Some assignments allow you to download and view your fellow learner’s code and/or data. If you want to look at the learner's code, we recommend that you open it through the Jupyter notebook system on the Coursera platform as that will be more secure. Please ensure that all data you share is publicly available, since you will be sharing these data with other learners.

| Course Item | Percentage of Final Grade | Passing Threshold |
|-------------|---------------------------|-------------------|
| Week 1 Quiz | 5% | 80% |
| Week 1 Jupyter Notebook Assignment | 20% |
| Week 2 Quiz | 5% | 80% |
| Week 2 Jupyter Notebook Assignment | 20% |
| Week 3 Quiz | 5% | 80% |
| Week 3 Jupyter Notebook Assignment | 20% |
| Week 4 Quiz | 5% | 80% |
| Week 4 Jupyter Notebook Assignment | 20% |

### Code of Conduct

Visit Coursera’s Code of Conduct and to abide by guidelines there. It is important when giving feedback to your peers to be polite and to be sensitive to the diversity of cultures and backgrounds of learners in your course.

### Working Offline

While the Coursera platform has an integrated Jupyter Notebook system, you can work offline on your own computer by installing Python 3.5+ and the Jupyter software packages. For more details, consult the Jupyter Notebook FAQ.

Note that this course uses the following packages: (a) sklearn-0.18.1, (b) nltk-3.2.4, (c) gensim-2.1.0, and (d) pandas-0.20.1.

### Accessibility

We strive to develop fully accessible courses. Occasionally, some of our content does not fully meet our accessibility goals. Please use this form to inform us of any accessibility issues you are experiencing in this course.

### Help!

If you're having problems, here are a couple of great places to go for help:

If the problem is with the Coursera platform such as verification on assignments, in video quiz problems, or the Jupyter Notebooks, please check out the [Coursera Learner Support Forums](https://learner.coursera.help/hc/en-us/requests).

If the problem deals with understanding the assignment or how to use the Jupyter Notebooks, please read our [Jupyter Notebook FAQ page](https://www.coursera.org/learn/python-data-analysis/resources/0dhYG) in the course resources

If you have questions with the content of the course, or questions about programming in python or with the toolkits described, you can contact your peers and the course instructors in the discussion forums, or go to [Stack Overflow](http://stackoverflow.com/questions/tagged/python).

Having trouble accessing your previously submitted assignments? If your session has ended, you can access these again by selecting the "Switch Session" option. Details for how to select this can be found in this learner help center article. If you still have issues accessing your materials after switching sessions, please reach out to Coursera learner support via our online chat forums in the Learner Help Center.

### In-Video Questions (IVQs)

In this course, in-video questions or IVQs may appear during lectures to help you learn as well as assess your understanding of the content. IVQs are optional and do not count towards your overall course grade.

### Types of in-video questions

Many of the lectures contain in-video questions (IVQs). These questions are presented in a variety of formats. Some will ask you to write or think about a concept from the video. Others will ask for a short answer. Still others may ask you to choose from a multiple-choice list of answers. If an IVQ is a survey or a poll, you will see a summary of responses from other learners after you respond. You can look at the question again later to see new summary data as more of your peers answer.

Some IVQs also contain runnable code blocks. These IVQs allow you to practice the coding concepts during the lecture. In this course, these types of IVQs will usually be directly followed with the solution code.


## Additional Resources

+ Dr Chuck Severance's Coursera Specialization, [Python for Everybody](https://www.coursera.org/specializations/python)
+ [Python Docs](https://docs.python.org/3/) (for general Python documentation)
+ [Python Classes Docs](https://docs.python.org/3.5/tutorial/classes.html)
+ [Scipy](http://scipy.org/) (for [IPython](http://ipython.org/), [Numpy](http://www.numpy.org/), [Pandas](http://pandas.pydata.org/), and [Matplotlib](http://matplotlib.org/))
+ [scitkit-learn Docs](http://scikit-learn.org/stable/documentation.html)
+ [scikit-learn Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)
+ [NLTK](http://www.nltk.org/)
+ [NLTK Book](http://www.nltk.org/book/)
+ [Gensim](https://radimrehurek.com/gensim/intro.html)
+ Don't forget to check [Stack Overflow](https://stackoverflow.com/questions/tagged/scikit-learn) and [Cross Validated](https://stats.stackexchange.com/)!


## Introduction to Text Mining

### Lecture Notes

+ Text is Everywhere!
+ Text data is growing fast!
    + Data continues to grow exponentially
        + Estimated to be 2.5 Exabytes (2.5 million TB) a day
        + Grow to 40 Zettabytes (40 billion TB) by 2020 (50-times that of 2010)`: + Approximately 80% of all data is estimated to be unstructured, text-rich data
        + $> 40$ million articles (5 million in English) in Wikipedia
        + $> 4.5$ billion Web pages
        + $> 500$ million tweets a day, 200 billion a year
        + $> 1.5$ trillion queries / searches on Google a year
+ Data hidden in plain sight
    <a href="https://www.coursera.org/learn/python-text-mining/lecture/y5C24/introduction-to-text-mining"> <br/>
        <img src="images/p1-01.png" alt="text" title= "caption" height="300">
    </a>
+ So, what can be done with text?
    + Parse text
    + Find / Identify / Extract relevant information from text
    + Classify text documents
    + Search for relevant text documents
    + Sentiment analysis
    + Topic modeling
    + ...


### Lecture Video

<a href="https://d3c33hcgiwev3.cloudfront.net/h8qk-2bDEeeSBw5DxGzUwg.processed/full/360p/index.mp4?Expires=1542326400&Signature=jKMebI0NimtJMotyL0Dy2Z4uIubfG937KDeKOlGVxcViFjL8MWoN~xRF8faOYkf4PMvxAH-sxg8~i0a-xkkPBIRjh~EfIVyy6Lj5Opyp4WXgeT885s-SPOYsDlzUZ4zcbzVkHlK9FgBD3aifObtNCBHwrSYr17mabvPjm6JnRRA_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" alt="Introduction to Text Mining" target="_blank">
    <img src="http://files.softicons.com/download/system-icons/windows-8-metro-invert-icons-by-dakirby309/png/64x64/Folders%20&%20OS/My%20Videos.png" alt="Video" width="40px"> 
</a>


## Handling Text in Python

### Lecture Notes

+ Primitive constructs in Text
    + Sentences / input strings
    + Words or Tokens
    + Characters
    + Document, larger files
    + And their properties …

+ Let’s try it out!
    ```python
    >>> text1 = "Ethics are built right into the ideals and objectives of the United Nations "
    >>> len(text1)`
    76
    >>> text2 = text1.split(' ')`
    >>> len(text2)`
    13
    >>> text2
    ['Ethics', 'are', 'built', 'right', 'into', 'the', 'ideals', 'and',
    'objectives', 'of', 'the', 'United', 'Nations', '']
    ```

+ Finding specific words
    + Long words: Words that are more than 3 letters long
        ```python
        >>> [w for w in text2 if len(w) > 3]
        ['Ethics', 'built', 'right', 'into', 'ideals', 'objectives', 'United', 'Nations']
        ```
    + Capitalized words
        ```python
        >>> [w for w in text2 if w.istitle()]
        ['Ethics', 'United', 'Nations']
        ```
    + Words that end with `s`
        ```python
        >>> [w for w in text2 if w.endswith('s')]
        ['Ethics', 'ideals', 'objectives', 'Nations']
        ```

+ Finding unique words: using set()`
    ```python
    >>> text3 = 'To be or not to be'
    >>> text4 = text3.split(' ')`
    >>> len(text4)`
    6
    >>> len(set(text4))`
    5
    >>> set(text4)`
    Set(['not', 'To', 'or', 'to', 'be'])
    >>> len(set([w.lower() for w in text4]))
    4
    >>> set([w.lower() for w in text4])
    Set(['not', 'to', 'or', 'be']
    ```

+ Some word comparison functions …
    + `s.startswith(t)`
    + `s.endswith(t)`
    + `t in s`
    + `s.isupper()`; `s.islower()`; `s.istitle()`
    + `s.isalpha()`; `s.isdigit()`; `s.isalnum()`

+ String Operations
    + `s.lower()`; `s.upper()`; `s.titlecase()`
    + `s.split(t)`
    + `s.splitlines()`
    + `s.join(t)`
    + `s.strip()`; `s.rstrip()`
    + `s.find(t)`; `s.rfind(t)`
    + `s.replace(u, v)`

+ From words to characters
    ```python
    >>> text5 = 'ouagadougou'
    >>> text6 = text5.split('ou')`
    >>> text6
    ['', 'agad', 'g', '']
    >>> 'ou'.join(text6)`
    'ouagadougou’
    >>> text5.split('')`
    Traceback (most recent call last):
    File "<stdin>", line 1, in
    <module>
    ValueError: empty separator
    >>> list(text5)`
    ['o', 'u', 'a', 'g', 'a', 'd', 'o', 'u', 'g', 'o', 'u']
    >>> [c for c in text5]
    ['o', 'u', 'a', 'g', 'a', 'd', 'o', 'u', 'g', 'o', 'u']
    ```

+ Cleaning Text
    ```python
    >>> text8 = ' A quick brown fox jumped over the lazy dog. '
    >>> text8.split(' ')`
    ['', '', '\t', 'A', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog.', '']
    >>> text9 = text8.strip()`: >>> text9.split(' ')`
    ['A', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog.']
    ```

+ Changing Text
    + Find and replace
        ```python
        >>> text9
        'A quick brown fox jumped over the lazy dog.'
        >>> text9.find('o')`
        10
        >>> text9.rfind('o')`
        40
        >>> text9.replace('o', 'O')`
        'A quick brOwn fOx jumped Over the lazy dOg.'
        ```

+ Handling Larger Texts
    + Reading files line by line
        ```python
        >>> f = open('UNDHR.txt', 'r')`
        >>> f.readline()`
        'Universal Declaration of Human Rights\n’
        ```
    + Reading the full file
        ```python
        >>> f.seek(0)`
        >>> text12 = f.read()`
        >>> len(text12)`
        10891
        >>> text13 = text12.splitlines()`:
        >>> len(text13)`
        158
        >>> text13[0]
        'Universal Declaration of Human Rights'
        ```
+ File Operations
    + `f = open(filename, mode)`
    + `f.readline()`; `f.read()`; `f.read(n)`
    + `for line in f: doSomething(line)`
    + `f.seek(n)`
    + `f.write(message)`
    + `f.close()`
    + `f.closed`

+ Issues with reading text files
    ```python
    >>> f = open('UNDHR.txt', 'r')`
    >>> text14 = f.readline()`
    'Universal Declaration of Human Rights\n'
    ```
    + How do you remove the last newline character?
        ```python
        >>> text14.rstrip()`
        'Universal Declaration of Human Rights’
        ```
        + Works also for DOS newlines (`^M`) that shows up as `'\r'` or `'\r\n'`

+ Take Home Concepts
    + Handling text sentences
    + Splitting sentences into words, words into characters
    + Finding unique words
    + Handling text from documents

+ `str` class
    + Signature: `str(object='')` -> str; `str(bytes_or_buffer[, encoding[, errors]])` -> str
    + Docstring: Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of `object.__str__()` (if defined)  or repr(object). encoding defaults to `sys.getdefaultencoding()`. errors defaults to `'strict'`.
    + Methods defined here:
        + `__add__(self, value, /)`: Return self+value.
        + `__contains__(self, key, /)`: Return key in self.
        + `__eq__(self, value, /)`: Return `self==value`.
        + `__format__(...)`, `S.__format__(format_spec)` -> str: Return a formatted version of S as described by format_spec.
        + `__ge__(self, value, /)`: Return `self>=value`.
        + `__getattribute__(self, name, /)`: Return `getattr(self, name)`.
        + `__getitem__(self, key, /)`: Return `self[key].
        + `__getnewargs__(...)`
        + `__gt__(self, value, /)`: Return `self>value`.
        + `__hash__(self, /)`: Return `hash(self)`.
        + `__iter__(self, /)`: Implement `iter(self)`.
        + `__le__(self, value, /)`: Return `self<=value`.
        + `__len__(self, /`: Return `len(self)`.
        + `__lt__(self, value, /)`: Return `self<value`.
        + `__mod__(self, value, /)`: Return `self%value`.
        + `__mul__(self, value, /)`: Return `self*value`.
        + `__ne__(self, value, /)`: Return `self!=value`.
        + `__new__(*args, **kwargs)` from `builtins.type`: Create and return a new object.  See help(type) for accurate signature.
        + `__repr__(self, /)`: Return `repr(self)`.
        + `__rmod__(self, value, /)`: Return `value%self`.
        + `__rmul__(self, value, /)`: Return `value*self`.
        + `__sizeof__(...)`, `S.__sizeof__()` -> size of S in memory, in bytes
        + `__str__(self, /)`: Return `str(self)`.
        + `capitalize(...)`, `S.capitalize()` -> str: Return a capitalized version of `S`, i.e. make the first character have upper case and the rest lower case.
        + `casefold(...)`, `S.casefold()` -> str: Return a version of `S` suitable for caseless comparisons.
        + `center(...)`, `S.center(width[, fillchar])` -> str: Return S centered in a string of length width. Padding is done using the specified fill character (default is a space)`:     + `count(...)`, `S.count(sub[, start[, end]])` -> int: Return the number of non-overlapping occurrences of substring sub in string `S[start:end]`.  Optional arguments start and end are interpreted as in slice notation.
        + `encode(...)`, `S.encode(encoding='utf-8', errors='strict')` -> bytes: Encode `S` using the codec registered for encoding. Default encoding is 'utf-8'. errors may be given to set a different error handling scheme. Default is 'strict' meaning that encoding errors raise a UnicodeEncodeError. Other possible values are 'ignore', 'replace' and 'xmlcharrefreplace' as well as any other name registered with `codecs.register_error` that can handle UnicodeEncodeErrors.
        + `endswith(...)`, `S.endswith(suffix[, start[, end]])` -> bool: Return True if `S` ends with the specified suffix, False otherwise. With optional start, test `S` beginning at that position. With optional end, `stop` comparing S at that position. suffix can also be a tuple of strings to try.
        + `expandtabs(...)`, `S.expandtabs(tabsize=8)` -> str: Return a copy of `S` where all tab characters are expanded using spaces. If tabsize is not given, a tab size of 8 characters is assumed.
        + `find(...)`, `S.find(sub[, start[, end]])` -> int: Return the lowest index in S where substring sub is found, such that sub is contained within `S[start:end]`.  Optional arguments start and end are interpreted as in slice notation. Return `-1` on failure.
        + `format(...)`, `S.format(*args, **kwargs)` -> str: Return a formatted version of S, using substitutions from `args` and `kwargs`. The substitutions are identified by braces ('{' and '}').
        + `format_map(...)`, `S.format_map(mapping)` -> str: Return a formatted version of `S`, using substitutions from mapping. The substitutions are identified by braces ('{' and '}').
        + `index(...)`, `S.index(sub[, start[, end]])` -> int: Return the lowest index in `S` where substring `sub` is found, such that `sub` is contained within `S[start:end]`.  Optional arguments start and end are interpreted as in slice notation. Raises ValueError when the substring is not found.
        + `isalnum(...)`, `S.isalnum()` -> bool: Return True if all characters in `S` are alphanumeric and there is at least one character in `S`, False otherwise.
        + `isalpha(...)`, `S.isalpha()` -> bool: Return True if all characters in `S` are alphabetic and there is at least one character in `S`, False otherwise.
        + `isdecimal(...)`, `S.isdecimal()` -> bool: Return True if there are only decimal characters in `S`, False otherwise.
        + `isdigit(...)`, `S.isdigit()` -> bool: Return True if all characters in `S` are digits and there is at least one character in `S`, False otherwise.
        + `isidentifier(...)`, `S.isidentifier()` -> bool: Return True if S is a valid identifier according to the language definition. Use `keyword.iskeyword()` to test for reserved identifiers such as "def" and "class".
        + `islower(...)`, `S.islower()` -> bool: Return True if all cased characters in `S` are lowercase and there is at least one cased character in `S`, False otherwise.
        + `isnumeric(...)`, `S.isnumeric()` -> bool: Return True if there are only numeric characters in `S`, False otherwise.
        + `isprintable(...)`, `S.isprintable()` -> bool: Return True if all characters in `S` are considered printable in `repr()` or `S` is empty, False otherwise.
        + `isspace(...)`: given, only the first count occurrences are replaced.
        + `istitle(...)`, `S.istitle()` -> bool: Return True if S is a titlecased string and there is at least one character in `S`, i.e. upper- and titlecase characters may only follow uncased characters and lowercase characters only cased ones. Return False otherwise.
        + `isupper(...)`, `S.isupper()` -> bool: Return True if all cased characters in `S` are uppercase and there is at least one cased character in `S`, False otherwise.
        + `join(...)`, `S.join(iterable)` -> str: Return a string which is the concatenation of the strings in the iterable.  The separator between elements is `S`.
        + `ljust(...)`, `S.ljust(width[, fillchar])` -> str: Return `S` left-justified in a Unicode string of length width. Padding is done using the specified fill character (default is a space).
        + `lower(...)`, `S.lower()` -> str: Return a copy of the string `S` converted to lowercase.
        + `lstrip(...)`, `S.lstrip([chars])` -> str: Return a copy of the string `S` with leading whitespace removed. If chars is given and not None, remove characters in chars instead.
        + `partition(...)`, `S.partition(sep)` -> (head, sep, tail): Search for the separator `sep` in `S`, and return the part before it, the separator itself, and the part after it.  If the separator is not found, return S and two empty strings.
        + `replace(...)`, `S.replace(old, new[, count])` -> str: Return a copy of `S` with all occurrences of substring old replaced by new.  If the optional argument count is given, only the first count occurrences are replaced.
        + `rfind(...)`, `S.rfind(sub[, start[, end]])` -> int: Return the highest index in `S` where substring `sub` is found, such that `sub` is contained within `S[start:end]`.  Optional arguments `start` and `end` are interpreted as in slice notation. Return `-1` on failure.
        + `rindex(...)`, `S.rindex(sub[, start[, end]])` -> int: Return the highest index in S where substring sub is found, such that `sub` is contained within `S[start:end]`.  Optional arguments `start` and `end` are interpreted as in slice notation. Raises ValueError when the substring is not found.
        + `rjust(...)`, `S.rjust(width[, fillchar])` -> str: Return `S` right-justified in a string of length width. Padding is done using the specified fill character (default is a space).
        + `rpartition(...)`, `S.rpartition(sep)` -> (head, sep, tail): Search for the separator `sep` in `S`, starting at the end of `S`, and return the part before it, the separator itself, and the part after it.  If the separator is not found, return two empty strings and `S`.
        + `rsplit(...)`, `S.rsplit(sep=None, maxsplit=-1)` -> list of strings: Return a list of the words in `S`, using `sep` as the delimiter string, starting at the end of the string and working to the front.  If `maxsplit` is given, at most `maxsplit` splits are done. If sep is not specified, any whitespace string is a separator.
        + `rstrip(...)`, `S.rstrip([chars])` -> str: Return a copy of the string `S` with trailing whitespace removed. If `chars` is given and not None, remove characters in chars instead.
        + `split(...)`, `S.split(sep=None, maxsplit=-1)` -> list of strings: Return a list of the words in `S`, using `sep` as the delimiter string.  If `maxsplit` is given, at most `maxsplit` splits are done. If `sep` is not specified or is None, any given, only the first count occurrences are replaced.
        + `splitlines(...)`, `S.splitlines([keepends])` -> list of strings: Return a list of the lines in `S`, breaking at line boundaries. Line breaks are not included in the resulting list unless keepends is given and true.
        + `startswith(...)`, `S.startswith(prefix[, start[, end]])` -> bool: Return True if `S` starts with the specified prefix, False otherwise. With optional `start`, test `S` beginning at that position. With optional `end`, stop comparing `S` at that position. prefix can also be a tuple of strings to try.
        + `strip(...)`, `S.strip([chars])` -> str: Return a copy of the string S with leading and trailing whitespace removed. If chars is given and not None, remove characters in chars instead.
        + `swapcase(...)`, `S.swapcase()` -> str: Return a copy of `S` with uppercase characters converted to lowercase and vice versa.
        + `title(...)`, `S.title()` -> str: Return a titlecased version of `S`, i.e. words start with title case characters, all remaining cased characters have lower case.
        + `translate(...)`, `S.translate(table)` -> str: Return a copy of the string `S` in which each character has been mapped through the given translation table. The table must implement lookup/indexing via `__getitem__`, for instance a dictionary or list, mapping Unicode ordinals to Unicode ordinals, strings, or None. If this operation raises LookupError, the character is left untouched. Characters mapped to None are deleted.
        + `upper(...)`, `S.upper()` -> str: Return a copy of `S` converted to uppercase.
        + `zfill(...)`, `S.zfill(width)` -> str: Pad a numeric string `S` with zeros on the left, to fill a field of the specified width. The string `S` is never truncated.
        + `maketrans(x, y=None, z=None, /)`: Return a translation table usable for `str.translate()`. If there is only one argument, it must be a dictionary mapping Unicode ordinals (integers) or characters to Unicode ordinals, strings or None. Character keys will be then converted to ordinals. If there are two arguments, they must be strings of equal length, and in the resulting dictionary, each character in `x` will be mapped to the character at the same position in `y`. If there is a third argument, it must be a string, whose characters will be mapped to None in the result.

+ `open` function
    + Signature: `open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)`
    + Docstring: Open file and return a stream.  Raise `IOErro`r upon failure.
    + Parametres:
        + `file` (text or byte string): the name (and the path if the file isn't in the current working directory) of the file to be opened or an integer file descriptor of the file to be wrapped. (If a file descriptor is given, it is closed when the returned I/O object is closed, unless closefd is set to False.)
        + `mode` (string - optional): specify the mode in which the file is opened. <br/>
            It defaults to 'r' which means open for reading in text mode.  Other common values are 'w' for writing (truncating the file if it already exists), 'x' for creating and writing to a new file, and 'a' for appending (which on some Unix systems, means that all writes append to the end of the file regardless of the current seek position). In text mode, if encoding is not specified the encoding used is platform dependent: locale.getpreferredencoding(False) is called to get the current locale encoding. (For reading and writing raw bytes use binary mode and leave encoding unspecified.) The available modes are:

            | mode | Description |
            |------|-------------|
            | 'r'  | open for reading (default) |
            | 'w'  | open for writing, truncating the file first |
            | 'x'  | create a new file and open it for writing |
            | 'a'  | open for writing, appending to the end of the file if it exists |
            | 'b'  | binary mode |
            | 't'  | text mode (default) |
            | '+'  | open a disk file for updating (reading and writing) |
            | 'U'  | universal newline mode (deprecated) |

            The default mode is 'rt' (open for reading text). For binary random access, the mode 'w+b' opens and truncates the file to 0 bytes, while 'r+b' opens the file without truncation. The 'x' mode implies 'w' and raises an `FileExistsError` if the file already exists.

            Python distinguishes between files opened in binary and text modes, even when the underlying operating system doesn't. Files opened in binary mode (appending 'b' to the mode argument) return contents a bytes objects without any decoding. In text mode (the default, or when 't' is appended to the mode argument), the contents of the file are returned as strings, the bytes having been first decoded using a platform-dependent encoding or using the specified encoding if given.

            'U' mode is deprecated and will raise an exception in future versions of Python.  It has no effect in Python 3.  Use newline to control universal newlines mode.
        + `buffering` (integer, optional): used to set the buffering policy. Pass `0` to switch buffering off (only allowed in binary mode), `1` to select line buffering (only usable in text mode), and an $integer > 1$ to indicate the size of a fixed-size chunk buffer.  When no buffering argument is given, the default buffering policy works as follows:
            + Binary files are buffered in fixed-size chunks; the size of the buffer is chosen using a heuristic trying to determine the underlying device's "block size" and falling back on `io.DEFAULT_BUFFER_SIZE`. On many systems, the buffer will typically be 4096 or 8192 bytes long.
            + "Interactive" text files (files for which isatty() returns True) use line buffering.  Other text files use the policy described above for binary files.
        + `encoding`: the name of the encoding used to decode or encode the file. This should only be used in text mode. The default encoding is platform dependent, but any encoding supported by Python can be passed.  See the codecs module for the list of supported encodings.
        + `errors` (string, optional): specify how encoding errors are to be handled---this argument should not be used in binary mode. Pass 'strict' to raise a ValueError exception if there is an encoding error (the default of None has the same effect), or pass 'ignore' to ignore errors. (Note that ignoring encoding errors can lead to data loss.) See the documentation for codecs.register or run 'help(codecs.Codec)' for a list of the permitted encoding error strings.
        + `newline`: control how universal newlines works (it only applies to text mode). It can be None, '', '\n', '\r', and '\r\n'.  It works as follows:
            + On input, if newline is None, universal newlines mode is enabled. Lines in the input can end in '\n', '\r', or '\r\n', and these are translated into '\n' before being returned to the caller. If it is '', universal newline mode is enabled, but line endings are returned to the caller untranslated. If it has any of the other legal values, input lines are only terminated by the given string, and the line ending is returned to the caller untranslated.
            + On output, if newline is None, any '\n' characters written are translated to the system default line separator, os.linesep. If newline is '' or '\n', no translation takes place. If newline is any of the other legal values, any '\n' characters written are translated to the given string.
        + `close`: 
            + If closefd is False, the underlying file descriptor will be kept open when the file is closed. This does not work when a file name is given and must be True in that case.
            + A custom opener can be used by passing a callable as *opener*. The underlying file descriptor for the file object is then obtained by calling *opener* with (*file*, *flags*). *opener* must return an open file descriptor (passing os.open as *opener* results in functionality similar to passing None).
    + Return:
        + `open()` returns a file object whose type depends on the mode, and through which the standard file operations such as reading and writing are performed. When open() is used to open a file in a text mode ('w', 'r', 'wt', 'rt', etc.), it returns a TextIOWrapper. When used to open a file in a binary mode, the returned class varies: in read binary mode, it returns a BufferedReader; in write binary and append binary modes, it returns a BufferedWriter, and in read/write mode, it returns a BufferedRandom.
        + It is also possible to use a string or bytearray as a file for both reading and writing. For strings StringIO can be used like a file opened in a text mode, and for bytes a BytesIO can be used like a file opened in a binary mode.


+ `TextIOWrapper(_TextIOBase)`
    + Docstring: Character and line based layer over a `BufferedIOBase` object, `buffer`.
        + encoding gives the name of the encoding that the stream will be decoded or encoded with. It defaults to locale.getpreferredencoding(False).
        + errors determines the strictness of encoding and decoding (see help(codecs.Codec) or the documentation for codecs.register) and defaults to "strict".
        + newline controls how line endings are handled. It can be None, '', '\n', '\r', and '\r\n'.  It works as follows:
            + On input, if newline is None, universal newlines mode is enabled. Lines in the input can end in '\n', '\r', or '\r\n', and these are translated into '\n' before being returned to the caller. If it is '', universal newline mode is enabled, but line endings are returned to the caller untranslated. If it has any of the other legal values, input lines are only terminated by the given string, and the line ending is returned to the caller untranslated.
            + On output, if newline is None, any '\n' characters written are translated to the system default line separator, os.linesep. If newline is '' or '\n', no translation takes place. If newline is any of the other legal values, any '\n' characters written are translated to the given string.
            + If line_buffering is True, a call to flush is implied when a call to write contains a newline character.
    + Method resolution order:
        + `TextIOWrapper`
        + `_TextIOBase`
        + `_IOBase`
        + `builtins.object`
    + Methods
        + `__getstate__(...)`
        + `__init__(self, /, *args, **kwargs)`: Initialize self.  See help(type(self)) for accurate signature.
        + `__new__(*args, **kwargs)` from `builtins.type`: Create and return a new object.  See help(type) for accurate signature.
        + `__next__(self, /)`: Implement `next(self)`.
        + `__repr__(self, /)`: Return `repr(self)`.
        + `close(self, /)`: Flush and close the IO object. This method has no effect if the file is already closed.
        + `detach(self, /)`: Separate the underlying buffer from the `TextIOBase` and return it. After the underlying buffer has been detached, the `TextIO` is in an unusable state.
        + `fileno(self, /)`: Returns underlying file descriptor if one exists. `OSError` is raised if the IO object does not use a file descriptor.
        + `flush(self, /)`: Flush write buffers, if applicable. This is not implemented for read-only and non-blocking streams.
        + `isatty(self, /)`: Return whether this is an 'interactive' stream. Return False if it can't be determined.
        + `read(self, size=-1, /)`: Read at most n characters from stream. Read from underlying buffer until we have n characters or we hit EOF. If n is negative or omitted, read until EOF.
        + `readable(self, /)`: Return whether object was opened for reading. If False, read() will raise OSError.
        + `readline(self, size=-1, /)`: Read until newline or EOF. Returns an empty string if EOF is hit immediately.
        + `seek(self, cookie, whence=0, /)`: Change stream position. Change the stream position to the given byte offset. The offset is interpreted relative to the position indicated by whence.  Values for whence are:
            + `0` -- start of stream (the default); offset should be zero or positive
            + `1` -- current stream position; offset may be negative
            + `2` -- end of stream; offset is usually negative

            Return the new absolute position.
        + `seekable(self, /)`: Return whether object supports random access. If False, `seek()`, `tell()` and `truncate()` will raise `OSError`. This method may need to do a test `seek()`.
        + `tell(self, /)`: Return current stream position.
        + `truncate(self, pos=None, /)`: Truncate file to size bytes. File pointer is left unchanged.  Size defaults to the current IO position as reported by `tell()`.  Returns the new size.
        + `writable(self, /)`: Return whether object was opened for writing. If False, `write()` will raise `OSError`.
        + `write(self, text, /)`: Write string to stream. Returns the number of characters written (which is always equal to the length of the string).
    + Data descriptors defined here:
        + `buffer`
        + `closed`
        + `encoding`: Encoding of the text stream. Subclasses should override.
        + `errors`: The error setting of the decoder or encoder. Subclasses should override.
        + `line_buffering`
        + `name`
        + `newlines`: Line endings translated so far. Only line endings translated during reading are considered. Subclasses should override.
    + Methods inherited from `_IOBase`:
        + `__del__(...)`
        + `__enter__(...)`
        + `__exit__(...)`
        + `__iter__(self, /)`: Implement iter(self).
        + `readlines(self, hint=-1, /)`: Return a list of lines from the stream. hint can be specified to control the number of lines read: no more lines will be read if the total size (in bytes/characters) of all lines so far exceeds hint.
        + `writelines(self, lines, /)`
    + Data descriptors inherited from _IOBase:
        + `__dict_`



### Lecture Video

<a href="https://d3c33hcgiwev3.cloudfront.net/6k5l6He9EeewexKhHrUb5g.processed/full/360p/index.mp4?Expires=1542499200&Signature=E-vaE7tQ7CEe7~KuhxTrllT7smB2K5YtQTIA6vh0vH8B4BH7gtIV6eu2MzDq6rXVg9HA06w5DHxgThIgtXy3BvxJwGDE2fHNRLPbGkNmeREJ6A8AFJjb7jcxNrJ-88QSxeL661G~EFCrUdQIU86in0ZfkW3rBnxC7sjpQwn2hsk_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" alt="Handling Text in Python" target="_blank">
    <img src="http://files.softicons.com/download/system-icons/windows-8-metro-invert-icons-by-dakirby309/png/64x64/Folders%20&%20OS/My%20Videos.png" alt="Video" width="40px"> 
</a>


## Notice for Auditing Learners: Assignment Submission

Please note: only verified learners can submit assignments. If you are auditing this course, you will be able to go through the quizzes or assignments, but you will not be able to submit your assignment for a grade. If you wish to have your assignments graded and receive a course certificate, we encourage you to upgrade to the Certified Learner track for this course. Coursera has provided [information about purchasing a certificate](https://learner.coursera.help/hc/en-us/articles/208280146-Pay-for-a-course-or-Specialization), and you can also get help from the [Coursera Help Center](https://learner.coursera.help/hc/en-us).


## Notebook: Working with Text

+ [Launching Web Page](https://www.coursera.org/learn/python-text-mining/notebook/0hunw/working-with-text)
+ [Notebook Web Page](https://hub.coursera-notebooks.org/user/dfxbyieeexzfjsmxjreyig/notebooks/Working%20With%20Text.ipynb)
+ [Local Notebook](notebooks/01-Working+With+Text.ipynb)
+ [Local Python Code](notebooks/01-Working+With+Text.py)


## Regular Expressions

### Lecture Notes

+ Processing Free-text
    ```python
    >>> text10 = '"Ethics are built right into the ideals and objectives of the United Nations" #UNSG @ NY Society for Ethical Culture bit.ly/2guVelr @UN @UN_Women'
    >>> text11 = text10.split(' ')
    >>> text11
    ['"Ethics', 'are', 'built', 'right', 'into', 'the', 'ideals', 'and', 'objectives', 'of', 'the', 'United', 'Nations"', '#UNSG', '@', 'NY', 'Society', 'for', 'Ethical', 'Culture', 'bit.ly/2guVelr', '@UN', '@UN_Women']
    ```
    + How do you find all Hashtags? Callouts?
    + Quiz: Write code that would extract hashtags from the following tweet:
        ```python
        tweet = "@nltk Text analysis is awesome! #regex #pandas #python"
        words = tweet.split(' ')
        print([w for w in words if w.startswith("#")])
        # ['#regex', '#pandas', '#python']
        ```

+ Finding Specific Words
    + Hashtags
        ```python
        >>> [w for w in text11 if w.startswith('#')]
        ['#UNSG']
        ```
    + Callouts
        ```python
        >>> [w for w in text11 if w.startswith('@')]
        ['@', '@UN', '@UN_Women']
        ```

+ Finding patterns with regular expressions
    + Callouts are more than just tokens beginning with '@':  `@UN_Spokesperson @katyperry @coursera`
    + Match _something_ after '@': `@[A-Za-z0-9_]+`
        + Alphabets
        + Numbers
        + Special symbols like '_'

+ Let’s try it out!
    ```python
    >>> text10 = '"Ethics are built right into the ideals and objectives of the United Nations" #UNSG @ NY Society for Ethical Culture bit.ly/2guVelr @UN @UN_Women'
    >>> text11 = text10.split(' ')
    >>> [w for w in text11 if w.startswith('@')]
    ['@', '@UN', '@UN_Women']
    ```
    + Import regular expressions first!
        ```python
        >>> import re
        >>> [w for w in text11 if re.search('@[A-Za-z0-9_]+', w)]
        ['@UN', '@UN_Women']
        ```

+ Parsing the callout regular expression: `@[A-Za-z0-9_]+`
    + starts with @
    + followed by any alphabet (upper or lower case), digit, or underscore
    + that repeats at least once, but any number of times

+ Meta-characters: Character matches
    + `.`: wildcard, matches a single character
    + `^`: start of a string
    + `$`: end of a string
    + `[]`: matches one of the set of characters within []
    + `[a-z]`: matches one of the range of characters a, b, …, z
    + `[^abc]`: matches a character that is not a, b, or, c
    + `a|b`: matches either a or b, where a and b are strings
    + `()`: Scoping for operators
    + `\`: Escape character for special characters (\t, \n, \b)
    + `(?:)`: Non-capturing group, the group is matched but is not captured for back-referencing

+ Meta-characters: Character symbols
    + `\b`: Matches word boundary
    + `\d`: Any digit, equivalent to `[0-9]`
    + `\D`: Any non-digit, equivalent to `[^0-9]`
    + `\s`: Any whitespace, equivalent to `[ \t\n\r\f\v]`
    + `\S`: Any non-whitespace, equivalent to `[^ \t\n\r\f\v]`
    + `\w`: Alphanumeric character, equivalent to `[a-zA-Z0-9_]`
    + `\W`: Non-alphanumeric, equivalent to `[^a-zA-Z0-9_]`

+ Meta-characters: Repetitions
    + `*`: matches zero or more occurrences
    + `+`: matches one or more occurrences
    + `?`: matches zero or one occurrences
    + `{n}`: exactly $n$ repetitions, $n ≥ 0$
    + `{n,}`: at least $n$ repetitions
    + `{,n}`: at most $n$ repetitions
    + `{m,n}`: at least $m$ and at most $n$ repetitions

+ Recall the callout regular expression
    ```python
    >>> text10 = '"Ethics are built right into the ideals and objectives of the United Nations" #UNSG @ NY Society for Ethical Culture bit.ly/2guVelr @UN @UN_Women'
    >>> text11 = text10.split(' ')
    >>> [w for w in text11 if re.search('@[A-Za-z0-9_]+', w)]
    ['@UN', '@UN_Women']
    >>> [w for w in text11 if re.search('@\w+', w)]
    ['@UN', '@UN_Women']
    ```

+ Let’s look at some more examples! - Finding specific characters
    ```python
    >>> text12 = ‘ouagadougou’
    >>> re.findall(r'[aeiou]', text12)
    ['o', 'u', 'a', 'a', 'o', 'u', 'o', 'u']
    >>> re.findall(r'[^aeiou]', text12)
    ['g', 'd', 'g’]
    ```

+ Case study: Regular expression for Dates: 
    + Date variations for 23rd October 2002: `\d{2}[/-]\d{2}[/-]\d{4}`
        + 23-10-2002
        + 23/10/2002
        + 23/10/02
        + 10/23/2002
        + 23 Oct 2002
        + 23 October 2002
        + Oct 23, 2002
        + October 23, 2002
    + Demo
        ```python
        >>> dateStr = '23-10-2002\n23/10/2002\n23/10/02\n10/23/2002\n23 Oct 2002\n23 October 2002\nOct 23, 2002\nOctober 23, 2002\n'

        >>> re.findall(r'\d{2}[\/-]\d{2}[\/-]\d{4}', dateStr)
        ['23-10-2002', '23/10/2002', '10/23/2002']
        >>> re.findall(r'\d{2}[\/-]\d{2}[\/-]\d{2,4}', dateStr)
        ['23-10-2002', '23/10/2002', '23/10/02', '10/23/2002']
        >>> re.findall(r'\d{1,2}[\/-]\d{1,2}[\/-]\d{2,4}', dateStr)
        ['23-10-2002', '23/10/2002', '23/10/02', '10/23/2002']

        >>> re.findall(r'\d{2} (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \d{4}', dateStr)
        ['Oct']
        >>> re.findall(r'\d{2} (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \d{4}', dateStr)
        ['23 Oct 2002']
        >>> re.findall(r'\d{2} (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[az]* \d{4}', dateStr)
        ['23 Oct 2002']
        >>> re.findall(r'(?:\d{2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* (?:\d{2}, )?\d{4}', dateStr)
        ['23 Oct 2002', '23 October 2002', 'Oct 23, 2002', 'October 23, 2002']
        >>> re.findall(r'(?:\d{1,2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* (?:\d{1,2}, )?\d{4}', dateStr)
        ['23 Oct 2002', '23 October 2002', 'Oct 23, 2002', 'October 23, 2002']
        ```

+ Take Home Concepts
    + What are regular expressions?
    + Regular expression meta-characters
    + Building a regular expression to identify dates
    + [Regular Expressions 101](https://regex101.com/)

+ `findall` method
    + Signature: `re.findall(pattern, string, flags=0)`
    + Docstring: Return a list of all non-overlapping matches in the string. <br/>
        If one or more capturing groups are present in the pattern, return a list of groups; this will be a list of tuples if the pattern has more than one group. <br/>
        Empty matches are included in the result.
    + Parameters:
        + `pattern`: The regular expression pattern

+ Regular Expression special characters:
    + `.`: Matches any character except a newline.
    + `^`: Matches the start of the string.
    + `$`: Matches the end of the string or just before the newline at the end of the string.
    + `*`: Matches 0 or more (greedy) repetitions of the preceding RE. Greedy means that it will match as many repetitions as possible.
    + `+`: Matches 1 or more (greedy) repetitions of the preceding RE.
    + `?`: Matches 0 or 1 (greedy) of the preceding RE.
    + `*?`,`+?`, `??`: Non-greedy versions of the previous three special characters.
    + `{m,n}`: Matches from m to n repetitions of the preceding RE.
    + `{m,n}?`: Non-greedy version of the above.
    + `\\`: Either escapes special characters or signals a special sequence.
    + `[]`: Indicates a set of characters. A "^" as the first character indicates a complementing set.
    + `|`: A|B, creates an RE that will match either A or B.
    + `(...)`: Matches the RE inside the parentheses. The contents can be retrieved or matched later in the string.
    + `(?aiLmsux)`: Set the A, I, L, M, S, U, or X flag for the RE (see below).
    + `(?:...)`: Non-grouping version of regular parentheses.
    + `(?P<name>...)`: The substring matched by the group is accessible by name.
    + `(?P=name)`: Matches the text matched earlier by the group named name.
    + `(?#...)`: A comment; ignored.
    + `(?=...)`: Matches if ... matches next, but doesn't consume the string.
    + `(?!...)`: Matches if ... doesn't match next.
    + `(?<=...)`: Matches if preceded by ... (must be fixed length).
    + `(?<!...)`: Matches if not preceded by ... (must be fixed length).
    + `(?(id/name)yes|no)`: Matches yes pattern if the group with id/name matched, the (optional) no pattern otherwise.

+ Regrlar Expression character symbol
    + `\number`: Matches the contents of the group of the same number.
    + `\A`: Matches only at the start of the string.
    + `\Z`: Matches only at the end of the string.
    + `\b`: Matches the empty string, but only at the start or end of a word.
    + `\B`: Matches the empty string, but not at the start or end of a word.
    + `\d`: Matches any decimal digit; equivalent to the set [0-9] in bytes patterns or string patterns with the ASCII flag. In string patterns without the ASCII flag, it will match the whole range of Unicode digits.
    + `\D`: Matches any non-digit character; equivalent to [^\d].
    + `\s`: Matches any whitespace character; equivalent to [ \t\n\r\f\v] in bytes patterns or string patterns with the ASCII flag. In string patterns without the ASCII flag, it will match the whole range of Unicode whitespace characters.
    + `\S`: Matches any non-whitespace character; equivalent to [^\s].
    + `\w`: Matches any alphanumeric character; equivalent to [a-zA-Z0-9_] in bytes patterns or string patterns with the ASCII flag. In string patterns without the ASCII flag, it will match the range of Unicode alphanumeric characters (letters plus digits plus underscore). With LOCALE, it will match the set [0-9_] plus characters defined as letters for the current locale.
    + `\W`: Matches the complement of \w.
    + `\\`: Matches a literal backslash.

+ Regular Expression Functions
    + `compile(pattern, flags=0)`: Compile a regular expression pattern, returning a pattern object.
    + `escape(pattern)`: Escape all the characters in pattern except ASCII letters, numbers and '_'.
    + `findall(pattern, string, flags=0)`: Return a list of all non-overlapping matches in the string. <br/> If one or more capturing groups are present in the pattern, return a list of groups; this will be a list of tuples if the pattern has more than one group. <br/> Empty matches are included in the result.
    + `finditer(pattern, string, flags=0)`: Return an iterator over all non-overlapping matches in the string.  For each match, the iterator returns a match object. <br/> Empty matches are included in the result.
    + `fullmatch(pattern, string, flags=0)`: Try to apply the pattern to all of the string, returning a match object, or None if no match was found.
    + `match(pattern, string, flags=0)`: Try to apply the pattern at the start of the string, returning a match object, or None if no match was found.
    + `purge()`: Clear the regular expression caches
    + `search(pattern, string, flags=0)`: Scan through string looking for a match to the pattern, returning a match object, or None if no match was found.
    + `split(pattern, string, maxsplit=0, flags=0)`: Split the source string by the occurrences of the pattern, returning a list containing the resulting substrings.  If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list.  If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list.
    + `sub(pattern, repl, string, count=0, flags=0)`: Return the string obtained by replacing the leftmost non-overlapping occurrences of the pattern in string by the replacement repl.  repl can be either a string or a callable; if a string, backslash escapes in it are processed.  If it is a callable, it's passed the match object and must return a replacement string to be used.
    + `subn(pattern, repl, string, count=0, flags=0)`: Return a 2-tuple containing (new_string, number). new_string is the string obtained by replacing the leftmost non-overlapping occurrences of the pattern in the source string by the replacement repl.  number is the number of substitutions that were made. repl can be either a string or a callable; if a string, backslash escapes in it are processed. If it is a callable, it's passed the match object and must return a replacement string to be used.
    + `template(pattern, flags=0)`: Compile a template pattern, returning a pattern object


### Lecture Video

<a href="https://d3c33hcgiwev3.cloudfront.net/B4K_Xne-Eee1BBJ2zgI9PA.processed/full/360p/index.mp4?Expires=1542672000&Signature=OlHh17RwmHPkICk7WIACrlCeKjhXQ3FuAm-72CS8EEHxHwg6ejYOaOiw6lFxnwTUl102uK--iMpeU4JbLUR0W6WwzvEX1oKn-6fbwu8Lkma9F3y6h2p3wRkgeAj3tvcQOvLrB7q~yi-m0ji95SFiVSwjtjmijoahL6sFuqNg1wo_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" alt="Regular Expressions" target="_blank">
    <img src="http://files.softicons.com/download/system-icons/windows-8-metro-invert-icons-by-dakirby309/png/64x64/Folders%20&%20OS/My%20Videos.png" alt="Video" width="40px"> 
</a>


## Notebook: Regex with Pandas and Named Groups

+ [Launching Web Page](https://www.coursera.org/learn/python-text-mining/notebook/CtwPh/regex-with-pandas-and-named-groups)
+ [Notebook Web Page](https://hub.coursera-notebooks.org/user/dfxbyieeexzfjsmxjreyig/notebooks/Regex%20with%20Pandas%20and%20Named%20Groups.ipynb)
+ [Local Notebook](notebooks/01-Regex+with+Pandas+and+Named+Groups.ipynb)
+ [Local Python Code](notebooks/01-Regex+with+Pandas+and+Named+Groups.py)


## Demonstration: Regex with Pandas and Named Groups

### Lecture Notes

+ Demo
    ```Python
    import pandas as pd

    time_sentences = ["Monday: The doctor's appointment is at 2:45pm.",
                    "Tuesday: The dentist's appointment is at 11:30 am.",
                    "Wednesday: At 7:00pm, there is a basketball game!",
                    "Thursday: Be back home by 11:15 pm at the latest.",
                    "Friday: Take the train at 08:10 am, arrive at 09:00am."]

    df = pd.DataFrame(time_sentences, columns=['text'])
    #                                                   text
    # 0        Monday: The doctor's appointment is at 2:45pm.
    # 1        Tuesday: The dentist's appointment is at 11:30...
    # 2        Wednesday: At 7:00pm, there is a basketball game!
    # 3        Thursday: Be back home by 11:15 pm at the latest.
    # 4        Friday: Take the train at 08:10 am, arrive at ...

    # find the number of characters for each string in df['text']
    df['text'].str.len()
    # 0    46
    # 1    50
    # 2    49
    # 3    49
    # 4    54
    # Name: text, dtype: int64

    # find the number of tokens for each string in df['text']
    df['text'].str.split().str.len()
    # 0     7
    # 1     8
    # 2     8
    # 3    10
    # 4    10
    # Name: text, dtype: int64

    # find which entries contain the word 'appointment'
    df['text'].str.contains('appointment')
    # 0     True
    # 1     True
    # 2    False
    # 3    False
    # 4    False
    # Name: text, dtype: bool

    # find how many times a digit occurs in each string
    df['text'].str.count(r'\d')
    # 0    3
    # 1    4
    # 2    3
    # 3    4
    # 4    8
    # Name: text, dtype: int64

    # find all occurances of the digits
    df['text'].str.findall(r'\d')
    # 0                   [2, 4, 5]
    # 1                [1, 1, 3, 0]
    # 2                   [7, 0, 0]
    # 3                [1, 1, 1, 5]
    # 4    [0, 8, 1, 0, 0, 9, 0, 0]
    # Name: text, dtype: object

    # group and find the hours and minutes
    df['text'].str.findall(r'(\d?\d):(\d\d)')
    # 0               [(2, 45)]
    # 1              [(11, 30)]
    # 2               [(7, 00)]
    # 3              [(11, 15)]
    # 4    [(08, 10), (09, 00)]
    # Name: text, dtype: object

    # replace weekdays with '???'
    df['text'].str.replace(r'\w+day\b', '???')
    # 0          ???: The doctor's appointment is at 2:45pm.
    # 1       ???: The dentist's appointment is at 11:30 am.
    # 2          ???: At 7:00pm, there is a basketball game!
    # 3         ???: Be back home by 11:15 pm at the latest.
    # 4    ???: Take the train at 08:10 am, arrive at 09:...
    # Name: text, dtype: object

    # replace weekdays with 3 letter abbrevations
    df['text'].str.replace(r'(\w+day\b)', lambda x: x.groups()[0][:3])
    # 0          Mon: The doctor's appointment is at 2:45pm.
    # 1       Tue: The dentist's appointment is at 11:30 am.
    # 2          Wed: At 7:00pm, there is a basketball game!
    # 3         Thu: Be back home by 11:15 pm at the latest.
    # 4    Fri: Take the train at 08:10 am, arrive at 09:...
    # Name: text, dtype: object

    # create new columns from first match of extracted groups
    df['text'].str.extract(r'(\d?\d):(\d\d)')
    #       0     1
    # 0     2    45
    # 1    11    30
    # 2     7    00
    # 3    11    15
    # 4    08    10

    # extract the entire time, the hours, the minutes, and the period
    df['text'].str.extractall(r'((\d?\d):(\d\d) ?([ap]m))')
    #              0         1     2     3
    #  match
    # 0    0     2:45 pm     2    45    pm
    # 1    0    11:30 am    11    30    am
    # 2    0     7:00 pm     7    00    pm
    # 3    0    11:15 pm    11    15    pm
    # 4    0    08:10 am    08    10    am
    #      1    09:00 am    09    00    am

    # extract the entire time, the hours, the minutes, and the period with group names
    df['text'].str.extractall(r'(?P<time>(?P<hour>\d?\d):(?P<minute>\d\d) ?(?P<period>[ap]m))')
    #              time    hour minute  period
    #  match
    # 0    0     2:45 pm     2    45    pm
    # 1    0    11:30 am    11    30    am
    # 2    0     7:00 pm     7    00    pm
    # 3    0    11:15 pm    11    15    pm
    # 4    0    08:10 am    08    10    am
    #      1    09:00 am    09    00    am
    ```

+ `str.len` method
    + Signature: `len()` method of `pandas.core.strings.StringMethods` instance
    + Docstring: Compute length of each string in the Series/Index.
    + Returns: `lengths` (Series/Index of integer values)

+ `str.split` method
    + Signature: `split(pat=None, n=-1, expand=False)` method of `pandas.core.strings.StringMethods` instance
    + Docstring: Split each string (a la re.split) in the Series/Index by given pattern, propagating NA values. Equivalent to `str.split`.
    + Parameters
        + `pat` (string, default None): String or regular expression to split on. If None, splits on whitespace
        + `n` (int, default -1 (all)): None, 0 and -1 will be interpreted as return all splits
        + `expand` (bool, default False): 
            + If True, return DataFrame/MultiIndex expanding dimensionality.
            + If False, return Series/Index.
    + Returns: `splait` (Series/Index or DataFrame/MultiIndex of objects)

+ `str.contsina` method
    + Signature: `contains(pat, case=True, flags=0, na=nan, regex=True)` method of `pandas.core.strings.StringMethods` instance
    + Dcostring: Return boolean Series/array whether given pattern/regex is contained in each string in the Series/Index.
    + Parameters
        + `pat` (string): Character sequence or regular expression
        + `case` (boolean, default True): If True, case sensitive
        + `flags` (int, default 0 (no flags)): re module flags, e.g. re.IGNORECASE
        + `na`: default NaN, fill value for missing values.
        + `regex` (bool, default True): If True use `re.search`, otherwise use Python in operator
    + Returns: `contained` (Series/array of boolean values)

+ `str.count` method
    + Signature: `str_count(pat, flags=0, **kwargs)` method of `pandas.core.strings.StringMethods` instance
    + Docstring: Count occurrences of pattern in each string of the Series/Index.
    + Parameters
        + `pat` (string): valid regular expression
        + `flags` (int, default 0 (no flags)): re module flags, e.g. re.IGNORECASE
    + Returns: `counts` (Series/Index of integer values)

+ `str.findall` method
    + Signature: `str_findall(pat, flags=0, **kwargs)` method of pandas.core.strings.StringMethods instance
    + Docstring: Find all occurrences of pattern or regular expression in the Series/Index. Equivalent to `re.findall`.
    + Parameters
        + `pat` (string): Pattern or regular expression
        + `flags` (int, default 0 (no flags)): re module flags, e.g. re.IGNORECASE
    + Returns: `matches` (Series/Index of lists)

+ `str.replace` method
    + Signature: `replace(pat, repl, n=-1, case=None, flags=0)` method of `pandas.core.strings.StringMethods` instance
    + Docstring: Replace occurrences of pattern/regex in the Series/Index with some other string. Equivalent to `str.replace` or `re.sub`.
    + Parameters
        + `pat` (string or compiled regex): String can be a character sequence or regular expression.
        + `repl` (string or callable): Replacement string or a callable. The callable is passed the regex match object and must return a replacement string to be used. See `re.sub`.
        + `n` (int, default -1 (all)): Number of replacements to make from start
        + `case` (boolean, default None): 
            + If True, case sensitive (the default if `pat` is a string)
            + Set to False for case insensitive
            + Cannot be set if `pat` is a compiled regex
        + `flags` (int, default 0 (no flags)): 
            + re module flags, e.g. re.IGNORECASE
            + Cannot be set if `pat` is a compiled regex
    + Returns: `replaced` (Series/Index of objects)
    + Notes: When `pat` is a compiled regex, all flags should be included in the compiled regex. Use of `case` or `flags` with a compiled regex will raise an error.

+ `str.extract` method
    + Signature: `extract(pat, flags=0, expand=None)` method of `pandas.core.strings.StringMethods` instance
    + Docstring: For each subject string in the Series, extract groups from the first match of regular expression `pat`.
    + Parameters
        + `pat` (string): Regular expression pattern with capturing groups
        + `flags` (int, default 0 (no flags)): re module flags, e.g. re.IGNORECASE
        + `expand` (bool, default False): 
            + If True, return DataFrame.
            + If False, return Series/Index/DataFrame.
    + Returns: DataFrame with one row for each subject string, and one column for each group. Any capture group names in regular expression `pat` will be used for column names; otherwise capture group numbers will be used. The dtype of each result column is always object, even when no match is found. If `expand=False` and `pat` has only one capture group, then return a Series (if subject is a Series) or Index (if subject is an Index).
    + Example: 
        ```python
        s = Series(['a1', 'b2', 'c3'])
        s.str.extract('([ab])(\d)')
        #      0    1
        # 0    a    1
        # 1    b    2
        # 2  NaN  NaN
        s.str.extract('(?P<letter>[ab])(?P<digit>\d)')
        #   letter digit
        # 0      a     1
        # 1      b     2
        # 2    NaN   NaN
        ```

### Lecture Video

<a href="https://d3c33hcgiwev3.cloudfront.net/XgLPxFiAEee7Ng519iSOCg.processed/full/360p/index.mp4?Expires=1542758400&Signature=ajsS0QXCGSS04BRx3QIjgxwrh5O8qmvm~WTeyUd2AuGt5luzRG-vzO6EZBAPk2fSIxDqxNNuEa9FFZ5kjRqpRBMUTH9NJlCqCzWG1ympH6PApdD~Ji5gHcmyJ8uGJoIh-~pMAnTNhN46aZMyAzsXH3MMSZ0-EK-N~y4LEAk0zAY_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" alt="Demonstration: Regex with Pandas and Named Groups" target="_blank">
    <img src="http://files.softicons.com/download/system-icons/windows-8-metro-invert-icons-by-dakirby309/png/64x64/Folders%20&%20OS/My%20Videos.png" alt="Video" width="40px"> 
</a>


## Practice Quiz

1. Which of these options correspond to matching a pattern at least once?

    a. `*`
    b. `?`
    c. `+`
    d. `{2}`
    e. `{2, 2}`
    f. `{2,3}`
    g. `{1,3}`
    h. `{3,}`
    i. `{,3}`
    j. `{2,}`

    Ans: c, xg


2. Which of these options correspond to matching a pattern zero or more times?

    a. `*`
    b. `?`
    c. `+`
    d. `{2}`
    e. `{2, 2}`
    f. `{2,3}`
    g. `{1,3}`
    h. `{3,}`
    i. `{,3}`
    j. `{2,}`

    Ans: a


3. Which of these options correspond to matching xyz at the start of the string?

    a. `[^xyz]`
    b. `^xyz`
    c. `$xyz`
    d. `\\xyz`
    e. `.xyz`
    f. `[]xyz`
    g. `[xyz^]`
    h. `xyz^`
    i. `xyz$`
    j. `xyz\\`
    k. `xyz.`
    l. `xyz[]`
    m. `(xyz)`
    n. `[xyz]`
    o. `xyz`
    p. `"xyz"`

    Ans: b



4. Which of these options correspond to matching xyz at the end of the string?

    a. `[^xyz]`
    b. `^xyz`
    c. `$xyz`
    d. `\\xyz`
    e. `.xyz`
    f. `[]xyz`
    g. `[xyz^]`
    h. `xyz^`
    i. `xyz$`
    j. `xyz\\`
    k. `xyz.`
    l. `xyz[]`
    m. `(xyz)`
    n. `[xyz]`
    o. `xyz`
    p. `"xyz"`

    Ans: i, xc


## Internationalization and Issues with Non-ASCII Characters

### Lecture Notes

+ World of Languages

+ ASCII and English
    + ASCII: American Standard Code for Information Interchange
    + 7-bit character encoding standard: 128 valid codes
    + Range: 0x00 – 0x7F [$(0000 0000)_2$ to $(0111 1111)_2$]
    + Includes alphabets (upper and lower cases), digits, punctuations, common symbols, control characters
    + Worked (relatively) well for English typewriting

+ Resume vs. Résumé
    + Diacritics
        + résumé :: resume
        + naïve :: naive
        + café :: cafe
        + Québec
        + Zürich
        + Fédération Internationale de Football Association (FIFA)
    + International languages
        + 基本上 सहायक ασπασθ универсальной
        + ♪ ♬ ♩
        + ☺ ☹

+ Written Scripts
    + Latin: 36% (2.6B people)
    + Chinese: 18% (1.3B)
    + Devanagari: 14% (1B)
    + Arabic: 14% (1B)
    + Cyrillic: 4% (0.3B)
    + Dravidian: 3.5% (0.25B)
    <a href="https://www.worldstandards.eu/alphabets/"> <br/>
        <img src="https://www.worldstandards.eu/WordPress/wp-content/uploads/alphabets-spread-around-the-world-1024x645.jpg" alt="A quick calculation shows that about 2.6 billion people (36% of the world population) use the Latin alphabet, about 1.3 billion people (18%) use the Chinese script, about 1 billion people (14%) use the Devanagari script (India), about 1 billion people (14%) use the Arabic alphabet, about 0.3 billion people (4%) use the Cyrillic alphabet and about 0.25 billion people (3.5%) use the Dravidian script (South India)." title= "The distribution of the world’s most important scripts that are currently used" height="300">
    </a>

+ Other Character Encodings
    + IBM EBCDIC
    + Latin-1
    + JIS: Japanese Industrial Standards
    + CCCII: Chinese Character Code for Information Interchange
    + EUC: Extended Unix Code
    + Numerous other national standards
    + __Unicode__ and __UTF-8__

+ Share of Web pages with different encodings
    <a href="https://en.wikipedia.org/wiki/File:Utf8webgrowth.svg"> <br/>
        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Utf8webgrowth.svg/903px-Utf8webgrowth.svg.png" alt="Note that the ASCII only figure reflects web pages with any declared header if they only include ASCII characters. Since UTF-8 is the default character set from HTML5, its current figure is probably much higher. The trend is confirmed by more recent published statistics which aren't compatible. West European includes ISO-8859-1 and Windows 1252. Others includes Chinese, Korean, Cyrillic, Eastern European, Arabic etc." title= "Shows the usage of the main encodings on the web from 2001 to 2014 as recorded by Google" height="200">
    </a>

+ Unicode
    + Industry standard for encoding and representing text
    + Over 128,000 characters from 130+ scripts and symbol sets
    + Can be implemented by different character endings
        + UTF-8: One byte to up to four bytes
        + UTF-16: One or two 16-bit code units
        + UTF-32: One 32-bit code unit

+ UTF-8
    + Unicode Transformational Format – 8-bits
    + Variable length encoding: One to four bytes
    + Backward compatible with ASCII
        + One byte codes same as ASCII
    + Dominant character encoding for the Web
    + How to handle in Python?
        + Default in Python 3
        + In Python 2: `# -*- coding: utf-8 -*-`

+ Example: Résumé
    + Python 3
        ```python
        >>> text1="Résumé"
        >>> len(text1)
        6
        >>> text1
        'Résumé'
        >>> [c for c in text1]
        ['R', 'é', 's', 'u', 'm', 'é']
        ```
    + Python 2
        ```python
        >>> text1="Résumé"
        >>> len(text1)
        8
        >>> text1
        'R\xc3\xa9sum\xc3\xa9'  # 0xc3a9 -> 2 bytes for é
        >>> [c for c in text1]
        ['R', '\xc3', '\xa9', 's', 'u', 'm', '\xc3', '\xa9']
        >>> text2=u'Résumé'
        >>> len(text2)
        6
        >>> text2
        u'R\xe9sum\xe9'
        >>> [c for c in text2]
        [u'R', u'\xe9', u's', u'u', u'm', u'\xe9']
        ```

+ Take Home Concepts
    + Diversity in Text
    + ASCII and other character encodings
    + Handling text in UTF-8

+ Lecture Exercise:
    + Even though we have so many languages around the world, the computer systems are still not necessarily well-equipped to accept inputs in all languages. Are you able to type in words in your native language in its native script? <br/>
        + Type the name of your first language in the native script (when possible)
        + Type the name of your first language in English
        + Run the code and see if python can print the text
    + Demo
        ```python
        native_name = "中文"
        english_name = "Mandarin"
        print(native_name, '\n', english_name)
        ```

### Lecture Video

<a href="https://d3c33hcgiwev3.cloudfront.net/76wXRGbGEeenaA4bzqx_CA.processed/full/360p/index.mp4?Expires=1542758400&Signature=Wjud85jXbofeuexxsi7sPQHvHP6HbIRVowBGKvkqOzXby14bpXagtMQfwSZcOe0h6KCHSH5cPCp-g8PGEhLot4o2Fy8Yxq~q9q3DWK~XABcxgk5fiAqcZOVj0nNDKFTeaSt84w1OdPyJr2C2~28EFxQvHnw1m92IHe0hNbZNTgU_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" alt="Internationalization and Issues with Non-ASCII Characters" target="_blank">
    <img src="http://files.softicons.com/download/system-icons/windows-8-metro-invert-icons-by-dakirby309/png/64x64/Folders%20&%20OS/My%20Videos.png" alt="Video" width="40px"> 
</a>


## Resources: Common issues with free text

+ Regular Expressions
    + [Regular expressions documentation in Python 3](https://docs.python.org/3/library/re.html)

+ Tips and tricks of the trade for cleaning text in Python
    + https://stanford.edu/~rjweiss/public_html/IRiSS2013/text2/notebooks/cleaningtext.html
    + https://www.analyticsvidhya.com/blog/2014/11/text-data-cleaning-steps-python/
    + http://ieva.rocks/2016/08/07/cleaning-text-for-nlp/
    + https://chrisalbon.com/python/cleaning_text.html


## Module 1 Quiz





